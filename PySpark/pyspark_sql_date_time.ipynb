{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# PySpark SparkSQL Date/Time\n",
    "\n",
    "## Date/Time Format Patterns\n",
    "\n",
    "* [A Comprehensive Look at Dates and Timestamps in Apache Spark™ 3.0](https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html)(MUST)\n",
    "\n",
    "* [Datetime Patterns for Formatting and Parsing](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n",
    "\n",
    "> Spark uses pattern letters in the following table for date and timestamp parsing and formatting:\n",
    "\n",
    "* [SparkSQL - Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "* [Migration Guide: SQL, Datasets and DataFrame](https://spark.apache.org/docs/latest/sql-migration-guide.html)\n",
    "\n",
    "* [Deep Dive into Apache Spark DateTime Functions](https://medium.com/expedia-group-tech/deep-dive-into-apache-spark-datetime-functions-b66de737950a)\n",
    "\n",
    "> Catalog of DateTime functions in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fc02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from datetime import (\n",
    "    datetime,\n",
    "    date\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ec8dc",
   "metadata": {},
   "source": [
    "## Python packages\n",
    "\n",
    "Execute after the PYTHONPATH setup.\n",
    "\n",
    "### pyspark.sql.funtions\n",
    "\n",
    "See [pyspark.sql.functions module](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions) for available function you can import. Spark Documentation [Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html#day) has functions such as ```day```, ```month``` but they cannot be imported and [pyspark.sql.functions module](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions) does not have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    avg,\n",
    "    stddev,\n",
    "    isnan,\n",
    "    to_date,\n",
    "    to_timestamp,\n",
    "    date_format,\n",
    "    year,\n",
    "    month,\n",
    "    hour,\n",
    "    min,\n",
    "    second,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 19:40:53,883 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-20 19:40:56,300 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-02-20 19:40:58,938 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc80e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "NUM_PARTITIONS = 3\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226dc8b",
   "metadata": {},
   "source": [
    "# Date/Time Format String\n",
    "\n",
    "* [Datetime Patterns for Formatting and Parsing](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n",
    "\n",
    "\n",
    "| Symbol | Meaning                      | Presentation | Examples                                       |\n",
    "|--------|------------------------------|--------------|------------------------------------------------|\n",
    "| G      | era                          | text         | AD; Anno Domini                                |\n",
    "| y      | year                         | year         | 2020; 20                                       |\n",
    "| D      | day-of-year                  | number(3)    | 189                                            |\n",
    "| M/L    | month-of-year                | month        | 7; 07; Jul; July                               |\n",
    "| d      | day-of-month                 | number(3)    | 28                                             |\n",
    "| Q/q    | quarter-of-year              | number/text  | 3; 03; Q3; 3rd quarter                         |\n",
    "| E      | day-of-week                  | text         | Tue; Tuesday                                   |\n",
    "| F      | aligned day of week in month | number(1)    | 3                                              |\n",
    "| a      | am-pm-of-day                 | am-pm        | PM                                             |\n",
    "| h      | clock-hour-of-am-pm (1-12)   | number(2)    | 12                                             |\n",
    "| K      | hour-of-am-pm (0-11)         | number(2)    | 0                                              |\n",
    "| k      | clock-hour-of-day (1-24)     | number(2)    | 0                                              |\n",
    "| H      | hour-of-day (0-23)           | number(2)    | 0                                              |\n",
    "| m      | minute-of-hour               | number(2)    | 30                                             |\n",
    "| s      | second-of-minute             | number(2)    | 55                                             |\n",
    "| S      | fraction-of-second           | fraction     | 978                                            |\n",
    "| V      | time-zone ID                 | zone-id      | America/Los_Angeles; Z; -08:30                 |\n",
    "| z      | time-zone name               | zone-name    | Pacific Standard Time; PST                     |\n",
    "| O      | localized zone-offset        | offset-O     | GMT+8; GMT+08:00; UTC-08:00;                   |\n",
    "| X      | zone-offset ‘Z’ for zero     | offset-X     | Z; -08; -0830; -08:30; -083015; -08:30:15;     |\n",
    "| x      | zone-offset                  | offset-x     | +0000; -08; -0830; -08:30; -083015; -08:30:15; |\n",
    "| Z      | zone-offset                  | offset-Z     | +0000; -0800; -08:00;                          |\n",
    "| ‘      | escape for text              | delimiter    |                                                |\n",
    "| ’‘     | single quote                 | literal      | ’                                              |\n",
    "| [      | optional section start       |              |                                                |\n",
    "| ]      | optional section end         |              |                                                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15861bdb",
   "metadata": {},
   "source": [
    "# Data Types\n",
    "\n",
    "\n",
    "* [Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html#data-types)\n",
    "\n",
    "```from pyspark.sql.types import *```\n",
    "\n",
    "| Data type | Value type in Python | API to access or create a data type |  |\n",
    "|:---|:---|:---|:--|\n",
    "|ByteType | int or long Note: Numbers will be converted to 1-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -128 to 127. | ByteType() |  |\n",
    "| ShortType | int or long Note: Numbers will be converted to 2-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -32768 to 32767. | ShortType() |  |\n",
    "| IntegerType | int or long | IntegerType() |  |\n",
    "| LongType | long Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807.Otherwise, please convert data to decimal.Decimal and use DecimalType. | LongType() |  |\n",
    "| FloatType | float Note: Numbers will be converted to 4-byte single-precision floating point numbers at runtime. | FloatType() |  |\n",
    "| DoubleType | float | DoubleType() |  |\n",
    "| DecimalType | decimal.Decimal | DecimalType() |  |\n",
    "| StringType | string | StringType() |  |\n",
    "| BinaryType | bytearray | BinaryType() |  |\n",
    "| BooleanType | bool | BooleanType() |  |\n",
    "| TimestampType | datetime.datetime | TimestampType() |  |\n",
    "| DateType | datetime.date | DateType() |  |\n",
    "| ArrayType | list, tuple, or array | ArrayType(elementType, [containsNull]) Note:The default value of containsNull is True. |  |\n",
    "| MapType | dict | MapType(keyType, valueType, [valueContainsNull]) Note:The default value of valueContainsNull is True. |  |\n",
    "| StructType | list or tuple | StructType(fields) Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed. |  |\n",
    "| StructField | The value type in Python of the data type of this field (For example, Int for a StructField with the data type IntegerType) | StructField(name, dataType, [nullable]) Note: The default value of nullable is True. |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb7433",
   "metadata": {},
   "source": [
    "# Date/Timestamp Literals\n",
    "\n",
    "* [Datetime Literal](https://spark.apache.org/docs/latest/sql-ref-literals.html#datetime-literal)\n",
    "\n",
    "### Date Literal\n",
    "\n",
    "> A datetime literal is used to specify a date or timestamp value.\n",
    "> ```\n",
    "> DATE { 'yyyy' |\n",
    ">        'yyyy-[m]m' |\n",
    ">        'yyyy-[m]m-[d]d' |\n",
    ">        'yyyy-[m]m-[d]d[T]'  }\n",
    "> ```\n",
    "\n",
    "Example: ```DATE '2011-11-11'``` is the Date literal which SparlSQL engine interprets proprietary manner.\n",
    "```\n",
    "SELECT DATE '2011-11-11' AS col;\n",
    "+----------+\n",
    "|       col|\n",
    "+----------+\n",
    "|2011-11-11|\n",
    "+----------+\n",
    "```\n",
    "\n",
    "### Timestamp Literal\n",
    "\n",
    "> ```\n",
    "> TIMESTAMP { 'yyyy' |\n",
    ">             'yyyy-[m]m' |\n",
    ">             'yyyy-[m]m-[d]d' |\n",
    ">             'yyyy-[m]m-[d]d ' |\n",
    ">             'yyyy-[m]m-[d]d[T][h]h[:]' |\n",
    ">             'yyyy-[m]m-[d]d[T][h]h:[m]m[:]' |\n",
    ">             'yyyy-[m]m-[d]d[T][h]h:[m]m:[s]s[.]' |\n",
    ">             'yyyy-[m]m-[d]d[T][h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]' }\n",
    "> ```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "SELECT TIMESTAMP '1997-01-31 09:26:56.66666666UTC+08:00' AS col;\n",
    "+--------------------------+\n",
    "|                      col |\n",
    "+--------------------------+\n",
    "|1997-01-30 17:26:56.666666|\n",
    "+--------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378d2f9",
   "metadata": {},
   "source": [
    "---\n",
    "# 2 digit-year handling\n",
    "\n",
    "```to_date``` can convert 2 digit year e.g. ```31-DEC-98``` into ```2098-12-31```.\n",
    "\n",
    "* [spark to_date function - how to convert 31-DEC-98 to 1998-12-31 not 2098-12-31](https://stackoverflow.com/questions/71182230)\n",
    "\n",
    "> On Spark 3.0, a new dates parser was introduced, with a changed behavior for dealing with 2 digits year.\n",
    "You could find a reference for the change under Upgrading from Spark SQL 2.4 to 3.0.\n",
    "> ```spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')``` will give you the original behavior with the required results\n",
    "\n",
    "```\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')\n",
    "\n",
    "(spark.createDataFrame([('31-DEC-98',)], 'my_date string')\n",
    " .select(F.to_date('my_date','dd-MMM-yy')\n",
    " .alias('my_new_date')).show()\n",
    ")\n",
    "\n",
    "+-----------+\n",
    "|my_new_date|\n",
    "+-----------+\n",
    "| 1998-12-31|\n",
    "+-----------+\n",
    "```\n",
    "\n",
    "* [spark - where is spark.sql.legacy.timeParserPolicy documented](https://stackoverflow.com/questions/71190476/spark-where-is-spark-sql-legacy-timeparserpolicy-documented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ff992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4201800",
   "metadata": {},
   "source": [
    "---\n",
    "# Extract date/time element\n",
    "\n",
    "\n",
    "* [Spark documentation - date_format][1]\n",
    "\n",
    "> date_format(timestamp, fmt) - Converts timestamp to a value of string in the format specified by the date format fmt.\n",
    "> * timestamp - A date/timestamp or string to be converted to the given format.\n",
    "> * fmt - Date/time format pattern to follow. See Datetime Patterns for valid date and time format patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23f8be",
   "metadata": {},
   "source": [
    "## Year\n",
    "\n",
    "* [Datetime Patterns for Formatting and Parsing](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n",
    "\n",
    "> Year: The count of letters determines the minimum field width below which padding is used. **If the count of letters is two, then a reduced two digit form is used**. For printing, this outputs the rightmost two digits. For parsing, this will parse using the base value of 2000, resulting in a year within the range 2000 to 2099 inclusive.\n",
    "> \n",
    "> **If the count of letters is less than four (but not two)**, then the sign is only output for negative years. Otherwise, the sign is output if the pad width is exceeded when ‘G’ is not present. 7 or more letters will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa35e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|year_number|\n",
      "+-----------+\n",
      "|       2007|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13T09:00', 'y') AS year_number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9f47c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|year_number|\n",
      "+-----------+\n",
      "|         07|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13T09:00', 'yy') AS year_number\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6da82e",
   "metadata": {},
   "source": [
    "## Month\n",
    "\n",
    "* [Datetime Patterns for Formatting and Parsing](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n",
    "\n",
    "> Month: It follows the rule of Number/Text. The text form is depend on letters - ‘M’ denotes the ‘standard’ form, and ‘L’ is for ‘stand-alone’ form. These two forms are different only in some certain languages. For example, in Russian, ‘Июль’ is the stand-alone form of July, and ‘Июля’ is the standard form. Here are examples for all supported pattern letters:\n",
    "\n",
    "```\n",
    "select date_format(date '1970-01-01', \"M\")\n",
    "1\n",
    "```\n",
    "\n",
    "```\n",
    "select date_format(date '1970-09-01', \"MM\")\n",
    "09\n",
    "```\n",
    "\n",
    "```\n",
    "select date_format(date '1970-01-01', \"d MMM\")\n",
    "1 Jan\n",
    "```\n",
    "\n",
    "```\n",
    "select date_format(date '1970-01-01', \"d MMMM\")\n",
    "1 January\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "185f1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|month_text|\n",
      "+----------+\n",
      "|       Nov|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13T09:00', 'MMM') AS month_text\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd1b03",
   "metadata": {},
   "source": [
    "## Week in month (?)\n",
    "\n",
    "To be verified.\n",
    "\n",
    "* [spark - what is F string for the date/time format?](https://stackoverflow.com/questions/71190684/spark-what-is-f-string-for-the-date-time-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b5fc9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|day_in_week_text|\n",
      "+----------------+\n",
      "|               2|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-10', 'F') AS day_in_week_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4c6d29d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|day_in_week_text|\n",
      "+----------------+\n",
      "|               3|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-17', 'F') AS day_in_week_text\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570534b3",
   "metadata": {},
   "source": [
    "## Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff6dce",
   "metadata": {},
   "source": [
    "### Day in the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1389249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|day_in_month_number|\n",
      "+-------------------+\n",
      "|                 13|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13T09:00', 'd') AS day_in_month_number\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65962ab",
   "metadata": {},
   "source": [
    "### Day in the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50f4ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|day_in_year_number|\n",
      "+------------------+\n",
      "|               317|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13T09:00', 'D') AS day_in_year_number\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a82d3a5",
   "metadata": {},
   "source": [
    "## Day in the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7414dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|day_in_week_text|\n",
      "+----------------+\n",
      "|             Tue|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13T09:00', 'E') AS day_in_week_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98167f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|day_in_week_text|day_in_week_num|\n",
      "+----------------+---------------+\n",
      "|             Tue|              3|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    date_format(to_date('2007-11-13', 'yyyy-MM-dd'), 'E') AS day_in_week_text,\n",
    "    dayofweek(to_date('2007-11-13', 'yyyy-MM-dd')) AS day_in_week_num\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423feb6",
   "metadata": {},
   "source": [
    "## Hour in the day (0-23)\n",
    "\n",
    "* [spark - how to extract hour from timestamp?](https://stackoverflow.com/questions/71190604/spark-how-to-extract-hour-from-timestamp)\n",
    "\n",
    "> why the date_format does not extract 08:15 for 8:15am\n",
    "> ```\n",
    "> spark.sql(\"select date_format(date '1994-11-05T08:15:30-05:00', 'HH:mm') AS hour_in_day_number\").show()\n",
    ">\n",
    ">+------------------+\n",
    ">|hour_in_day_number|\n",
    ">+------------------+\n",
    ">|             00:00|\n",
    ">+------------------+\n",
    ">```\n",
    "\n",
    "> You used date, which only keep year, month and day.\n",
    "> ```date '1994-11-05T08:15:30-05:00'```\n",
    "> You can try use tiemstamp as below:\n",
    "> ```timestamp '1994-11-05T08:15:30-05:00'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4245017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|hour_in_day_number|\n",
      "+------------------+\n",
      "|             12:15|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(timestamp '1994-11-05T08:15:30-05:00', 'hh:mm') AS hour_in_day_number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a84e29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|hour_in_day_number|\n",
      "+------------------+\n",
      "|             00:15|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(timestamp '1994-11-05T08:15:30-05:00', 'HH:mm') AS hour_in_day_number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea1f2a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|hour_in_day_number|\n",
      "+------------------+\n",
      "|             24:00|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '1994-11-05T08:15:30-05:00', 'kk:mm') AS hour_in_day_number\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4815dee9",
   "metadata": {},
   "source": [
    "## AM/PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4cbcdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|AMPM|\n",
      "+----+\n",
      "|  AM|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13T09:00', 'aa') AS AMPM\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ed5af05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|month_text|\n",
      "+----------+\n",
      "|       Nov|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date_format(date '2007-11-13', 'MMM') AS month_text\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa65c9d",
   "metadata": {},
   "source": [
    "---\n",
    "# Date/Time Comparison\n",
    "\n",
    "```to_date```, ```date_format``` **expects a column** as its parameter. Use ```lit()``` to convert a valie to a column of valie.\n",
    "\n",
    "* [pyspark.sql.functions.lit(col)](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.lit.html)\n",
    "\n",
    "> df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9866e",
   "metadata": {},
   "source": [
    "Note that ```date_format```, ```to_date``` returns Column object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc572c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'date_format(TIMESTAMP '2000-12-11 10:20:20', yyyy-MM-dd)'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_format(date=lit(datetime(year=2000,month=12,day=11,hour=10,minute=20,second=20)),format=\"yyyy-MM-dd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efdb29",
   "metadata": {},
   "source": [
    "## Date comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e182641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------+\n",
      "| id|name|number|      date|boolean|\n",
      "+---+----+------+----------+-------+\n",
      "|  1|tako|3.1415|2000-01-01|   true|\n",
      "|  2| ika| 1.618|1999-12-31|  false|\n",
      "+---+----+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"number\", DoubleType(), False),\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"boolean\", BooleanType(), False),\n",
    "])\n",
    "data = [\n",
    "    (1, \"tako\", 3.1415, date(year=2000,month=1,day=1), True),\n",
    "    (2, \"ika\", 1.6180, date(year=1999,month=12,day=31), False)\n",
    "]\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91b38c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|day_text|day_num|\n",
      "+--------+-------+\n",
      "|     Sat|      7|\n",
      "|     Fri|      6|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    date_format(date, \"EEE\") AS day_text,\n",
    "    dayofweek(date) AS day_num\n",
    "FROM\n",
    "    df\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cea59df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------+\n",
      "| id|name|number|      date|boolean|\n",
      "+---+----+------+----------+-------+\n",
      "|  1|tako|3.1415|2000-01-01|   true|\n",
      "+---+----+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"date\")==date(year=2000,month=1,day=1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd2825a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------+\n",
      "| id|name|number|      date|boolean|\n",
      "+---+----+------+----------+-------+\n",
      "|  1|tako|3.1415|2000-01-01|   true|\n",
      "+---+----+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"date\")==to_date(lit(\"2000-01-01\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dd9b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+----------+-------+\n",
      "| id|name|number|      date|boolean|\n",
      "+---+----+------+----------+-------+\n",
      "|  2| ika| 1.618|1999-12-31|  false|\n",
      "+---+----+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"date\")==date_format(lit(date(year=1999,month=12,day=31)), \"yyyy-MM-dd\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c72b566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2000-01-01|\n",
      "|1999-12-31|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98658e",
   "metadata": {},
   "source": [
    "## Timestamp comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0de910c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------------+-------+\n",
      "| id|name|number|           datetime|boolean|\n",
      "+---+----+------+-------------------+-------+\n",
      "|  1|tako|3.1415|2000-01-01 10:20:30|   true|\n",
      "|  2| ika| 1.618|1999-12-31 20:50:34|  false|\n",
      "+---+----+------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"number\", DoubleType(), False),\n",
    "    StructField(\"datetime\", TimestampType(), False),\n",
    "    StructField(\"boolean\", BooleanType(), False),\n",
    "])\n",
    "data = [\n",
    "    (1, \"tako\", 3.1415, datetime(year=2000,month=1,day=1,hour=10,minute=20, second=30), True),\n",
    "    (2, \"ika\", 1.6180, datetime(year=1999,month=12,day=31,hour=20,minute=50, second=34), False)\n",
    "]\n",
    "timestamp_df = spark.createDataFrame(data=data, schema=schema)\n",
    "timestamp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "619a4df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------------+-------+\n",
      "| id|name|number|           datetime|boolean|\n",
      "+---+----+------+-------------------+-------+\n",
      "|  2| ika| 1.618|1999-12-31 20:50:34|  false|\n",
      "+---+----+------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp_df.where(hour(col(\"datetime\")) == 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b5753d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+\n",
      "|           datetime|(hour(datetime) = 20)|\n",
      "+-------------------+---------------------+\n",
      "|2000-01-01 10:20:30|                false|\n",
      "|1999-12-31 20:50:34|                 true|\n",
      "+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp_df.select(\"datetime\", hour(col(\"datetime\")) == 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee281a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------------------+-------+\n",
      "| id|name|number|           datetime|boolean|\n",
      "+---+----+------+-------------------+-------+\n",
      "|  2| ika| 1.618|1999-12-31 20:50:34|  false|\n",
      "+---+----+------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp_df.where(\n",
    "    date_format(col(\"datetime\"), 'yyyy-MM-dd:HH') == \"1999-12-31:20\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
