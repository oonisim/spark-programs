{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# Create PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b9883",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "* [The UC Irvine Machine Learning Repository  - Record Linkage Comparison Patterns Data Set](https://archive.ics.uci.edu/ml/datasets/Record+Linkage+Comparison+Patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1a5c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   163  100   163    0     0    529      0 --:--:-- --:--:-- --:--:--   529\n",
      "100 53.8M  100 53.8M    0     0  7635k      0  0:00:07  0:00:07 --:--:-- 10.3M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  donation.zip\n",
      "Archive:  block_9.zip\n",
      "\n",
      "Archive:  block_4.zip\n",
      "\n",
      "Archive:  block_10.zip\n",
      "\n",
      "Archive:  block_1.zip\n",
      "\n",
      "Archive:  block_6.zip\n",
      "\n",
      "Archive:  block_5.zip\n",
      "\n",
      "Archive:  block_2.zip\n",
      "\n",
      "Archive:  block_8.zip\n",
      "\n",
      "Archive:  block_3.zip\n",
      "\n",
      "Archive:  block_7.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10 archives were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p data/linkage\n",
    "cd data/linkage/\n",
    "curl -L -o donation.zip https://bit.ly/1Aoywaq\n",
    "\n",
    "unzip -f donation.zip\n",
    "unzip -f 'block_*.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b58ed336",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/linkage/\n",
    "hdfs dfs -mkdir -p linkage\n",
    "hdfs dfs -put -f block_*.csv linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 07:31:33,602 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-19 07:31:36,147 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201eac20",
   "metadata": {},
   "source": [
    "# Read CSV\n",
    "\n",
    "* [SparkSQL CSV Files](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)\n",
    "\n",
    "> Spark SQL provides spark.read().csv(\"file_name\") to read a file or directory of files in CSV format into Spark DataFrame, and dataframe.write().csv(\"path\") to write to a CSV file. Function option() can be used to customize the behavior of reading or writing.\n",
    "\n",
    "[SparkSession.read()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.html#read--) returns [DataFrameReader](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html) instance which has [option](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-boolean-) method by which we can specify CSV options.\n",
    "\n",
    "The options are listed in [Data Source Option](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30caa51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev = spark.read.csv(\"linkage\")\n",
    "prev.printSchema()\n",
    "del prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1622305",
   "metadata": {},
   "source": [
    "## CSV Options\n",
    "\n",
    "* Schema Inference \n",
    "* Null value replacement\n",
    "* Header handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8eecc4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"?\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"linkage\")\n",
    "\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47ba9be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993e994",
   "metadata": {},
   "source": [
    "# DataFrame Structure\n",
    "\n",
    "## Compasison with HTML\n",
    "\n",
    "| HTML       | HTML Description             | Spark       | Spark Description                                                                                                                                                                                                                                                                                                                       |   |\n",
    "|------------|-------------------------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|\n",
    "| ```<table>```    | List of ```<tr>```            | DataFrame   | List of Row, and DataFrame is an alias of type DataSet[Row].                                                                                                                                                                                                                                                                      |   |\n",
    "| ```<tr>```       | Table row. List of ```<td> ```| Row         | List of typed values.  ```Row``` has the schema of type **StructType** |   |\n",
    "|            |                         | StructType  | Defines the type of a Row and a list of StructField that defines a field of a Row.                                                                                                                                                                                                                                                |   |\n",
    "|```<td>```Table  | Table field             | StructField | Specify the DataType of a field of a row.                                                                                                                                                                                                                                                                                         |   |\n",
    "|            |                         | DataType    |                                                                                                                                                                                                                                                                                                                                   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd05ad",
   "metadata": {},
   "source": [
    "## DataFrame Schema Definition\n",
    "\n",
    "Instead of using **Schema Inference**, you can provide the **Schema Definition** for  ```SparkSession.read.schema(schema).csv(file)``` to use when reading the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e182641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField(id_1,IntegerType,false)\n",
      "StructField(id_2,StringType,false)\n",
      "StructField(cmp_fname_c1,DoubleType,false)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"id_1\", IntegerType(), False),\n",
    "    StructField(\"id_2\", StringType(), False),\n",
    "    StructField(\"cmp_fname_c1\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "for element in schema:\n",
    "    print(element)\n",
    "    \n",
    "# spark.read.schema(schema).csv(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a51f7",
   "metadata": {},
   "source": [
    "## Row\n",
    "\n",
    "Each row of the DataFrame is an instance of ```pyspark.sql.Row```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f186dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id_1=3148, id_2=8326, cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql \n",
    "\n",
    "row: pyspark.sql.Row = parsed.first()\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276809f",
   "metadata": {},
   "source": [
    "## Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb2190db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_1 type:<class 'int'> value: 3148\n"
     ]
    }
   ],
   "source": [
    "id_1 = row['id_1']\n",
    "print(f\"id_1 type:{type(id_1)} value: {id_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751eebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345fd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "673c320f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d5b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
