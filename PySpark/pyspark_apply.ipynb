{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# PySpark - Apply Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13fc02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d12c0842",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = !whoami\n",
    "USER = USER[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b9755",
   "metadata": {},
   "source": [
    "## PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "830da51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = f\"/home/{USER}/venv/ml/bin/python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318e32e",
   "metadata": {},
   "source": [
    "## PySpark packages\n",
    "\n",
    "Execute after the PYTHONPATH setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bc37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    isnan,\n",
    "    lower,\n",
    "    concat,\n",
    "    udf,\n",
    "    array\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 11:33:58,531 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "#    .config('spark.yarn.appMasterEnv.PYSPARK_PYTHON', f\"/home/{USER}/venv/ml/bin/python3\")\\\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .config('spark.yarn.executorEnv.PYSPARK_PYTHON', f\"/home/{USER}/venv/ml/bin/python3\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc80e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "NUM_PARTITIONS = 3\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d96701",
   "metadata": {},
   "source": [
    "# Apply functon on all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f114fd7",
   "metadata": {},
   "source": [
    "## upper/lower function\n",
    "\n",
    "Example to apply function on all the columns using functools.reduce().\n",
    "\n",
    "1. Go through all the columns one by one.\n",
    "2. Apply function ```pyspark.sql.functions.lower``` on each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e704c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|eye_color|\n",
      "+----+---------+\n",
      "|Jose|     BLUE|\n",
      "|  lI|    BrOwN|\n",
      "+----+---------+\n",
      "\n",
      "+----+---------+\n",
      "|name|eye_color|\n",
      "+----+---------+\n",
      "|jose|     blue|\n",
      "|  li|    brown|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "def f(df: DataFrame,  column: str) -> DataFrame:\n",
    "    return df.withColumn(column, pyspark.sql.functions.lower(col(column)))\n",
    "\n",
    "\n",
    "source_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"Jose\", \"BLUE\"),\n",
    "        (\"lI\", \"BrOwN\")\n",
    "    ],\n",
    "    schema=[\"name\", \"eye_color\"]\n",
    ")\n",
    "source_df.show()\n",
    "\n",
    "applied = (reduce(\n",
    "    f,\n",
    "    source_df.columns,\n",
    "    source_df\n",
    "))\n",
    "applied.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6c0f290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+\n",
      "|name|eye_color|concatenated|\n",
      "+----+---------+------------+\n",
      "|Jose|     BLUE|   Jose BLUE|\n",
      "|  lI|    BrOwN|    lI BrOwN|\n",
      "+----+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df.withColumn(\"concatenated\", concat(col(\"name\"), lit(\" \"), col(\"eye_color\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa05220",
   "metadata": {},
   "source": [
    "## Concatenate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0d68183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|       _1|\n",
      "+---------+\n",
      "|Jose BLUE|\n",
      "| lI BrOwN|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def g(row):\n",
    "    return (\" \".join([row['name'], row[\"eye_color\"]]),)\n",
    "              \n",
    "source_df.rdd.map(g).toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f155ffe",
   "metadata": {},
   "source": [
    "---\n",
    "# UDF\n",
    "\n",
    "* [How to Turn Python Functions into PySpark Functions (UDF)](https://changhsinlee.com/pyspark-udf/)\n",
    "\n",
    "> Spark UDF doesn’t convert integers to floats, unlike Python function which works for both integers and floats, a Spark UDF will return a column of NULLs if the input data type doesn’t match the output data type\n",
    "\n",
    "```\n",
    "## Force the output to be float\n",
    "square_udf_float2 = udf(lambda z: float(z**2), FloatType())\n",
    "```\n",
    "\n",
    "OR \n",
    "```\n",
    "@udf(\"float\")\n",
    "def square_udf_float(x):\n",
    "    return float(x**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08742ce",
   "metadata": {},
   "source": [
    "## Apply numpy functions on columns\n",
    "\n",
    "* [apply udf to multiple columns and use numpy operations](https://stackoverflow.com/a/58179373/4281353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f769efb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "|count| df|docs|\n",
      "+-----+---+----+\n",
      "|  138|  5|  10|\n",
      "|  128|  4|  10|\n",
      "|  112|  3|  10|\n",
      "|  120|  3|  10|\n",
      "|  189|  1|  10|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    data = [(138,5,10), (128,4,10), (112,3,10), (120,3,10), (189,1,10)], \n",
    "    schema=[\"count\",\"df\",\"docs\"]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92cf09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"float\")\n",
    "def newFunction(count, df, docs):\n",
    "    import numpy as np\n",
    "    returnValue = (1 + np.log(count)) * np.log(docs/df)\n",
    "    return returnValue.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4efcbf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+-------------------+\n",
      "|count| df|docs|new_function_result|\n",
      "+-----+---+----+-------------------+\n",
      "|  138|  5|  10|           4.108459|\n",
      "|  128|  4|  10|           5.362161|\n",
      "|  112|  3|  10|          6.8849173|\n",
      "|  120|  3|  10|           6.967983|\n",
      "|  189|  1|  10|          14.372153|\n",
      "+-----+---+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_function_result\", newFunction(\"count\",\"df\",\"docs\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf8cee",
   "metadata": {},
   "source": [
    "## Sqare the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "551bee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"float\")\n",
    "def square_udf_float(x):\n",
    "    return float(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a390a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+-------+\n",
      "|count| df|docs| square|\n",
      "+-----+---+----+-------+\n",
      "|  138|  5|  10|19044.0|\n",
      "|  128|  4|  10|16384.0|\n",
      "|  112|  3|  10|12544.0|\n",
      "|  120|  3|  10|14400.0|\n",
      "|  189|  1|  10|35721.0|\n",
      "+-----+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"square\", square_udf_float(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
