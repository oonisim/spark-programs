{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fc02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# PySpark - Manage Python Dependencies\n",
    "\n",
    "There are multiple ways to manage Python dependencies so that PySpark executor can import the depdencies **in the worker nodes** as well as the driver node. Make clear awareness if you are addressing the driver node or worker nodes. If executor cannot find the dependenceis, you will see the error ```ModuleNotFoundError: No module named ...```.\n",
    "\n",
    "\n",
    "\n",
    "## PYSPARK_PYTHON Enironment variable\n",
    "\n",
    "Use the environment varilable to tell Spark runtime where is the Python interpreter of the Python environment that has the dependencies installed.\n",
    "\n",
    "### Worker Nodes\n",
    "\n",
    "Setup ```PYSPARK_PYTHON``` in the nodes to point to the python interpreter path in the Python enviornment installed in the worker nodes (virtual environment or system). The Python environment needs to have the required package installed with the package management tool (pip, anaaconda, etc).\n",
    "\n",
    "There are a few ways:\n",
    "\n",
    "Using ```$SPARK_HOME/conf/spark-env.sh``` in the worker nodes to point to the Python interpreter.\n",
    "```\n",
    "export PYSPARK_PYTHON=<Python interpreter path>\n",
    "```\n",
    "\n",
    "OR\n",
    "\n",
    "Using ```spark.yarn.executorEnv.PYSPARK_PYTHON``` at the SparkSession creation.\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .config('spark.yarn.executorEnv.PYSPARK_PYTHON', \"/home/oonisim/venv/ml/bin/python3\")\\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Driver Node\n",
    "\n",
    "Depends on the cluster mode. ```PYSPARK_DRIVER_PYTHON``` has to be **unset** in Kubernetes or YARN cluster modes.\n",
    "\n",
    "#### YARN client mode\n",
    "\n",
    "Using the ```$SPARK_HOME/conf/spark-env.sh`` in the driver node, or export from the command line.\n",
    "\n",
    "```\n",
    "export PYSPARK_DRIVER_PYTHON=<Python interpreter path> # Do not set in cluster modes.\n",
    "```\n",
    "\n",
    "#### YARN cluster mode\n",
    "\n",
    "Using the ```spark.yarn.appMasterEnv.PYSPARK_PYTHON```.\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .config('spark.yarn.appMasterEnv.PYSPARK_PYTHON', \"/home/oonisim/venv/ml/bin/python3\")\\\n",
    "    .config('spark.yarn.executorEnv.PYSPARK_PYTHON', \"/home/oonisim/venv/ml/bin/python3\")\\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "\n",
    "## Virtual Environment Archive\n",
    "\n",
    "Tell Spark runtime the location of the virtual environment archive in which the dependencies are installed using the ```--archive``` or ```spark-archives```. The archive can be placed in the driver node or in the HDFS.\n",
    "\n",
    "1. Create a virtual environment (venv or conda).\n",
    "2. Install the dependencies in the virtual environment.\n",
    "3. Archieve the virtual environment directory into a ```tgz``` file.\n",
    "4. Point to the archieve path via ```--archive``` at spark-submit or ```spark.archives``` in PySpark shell/Notebook.\n",
    "\n",
    "* [Spark User Guide - Python Package Management](https://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html)\n",
    "\n",
    "```\n",
    "export PYSPARK_DRIVER_PYTHON=python # Do not set in cluster modes.\n",
    "export PYSPARK_PYTHON=./environment/bin/python\n",
    "spark-submit --archives pyspark_venv.tar.gz#environment app.py\n",
    "```\n",
    "\n",
    "OR \n",
    "\n",
    "```\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from app import main\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\n",
    "        \"spark.archives\",  # <----- 'spark.yarn.dist.archives' in YARN.\n",
    "        \"pyspark_venv.tar.gz#environment\"\n",
    "    )\\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "## Spark setups Virtual Environment\n",
    "\n",
    "Hortonworks approach. This feature is currently only supported in yarn mode.\n",
    "\n",
    "* [Cloudera Community - Using VirtualEnv with PySpark](https://community.cloudera.com/t5/Community-Articles/Using-VirtualEnv-with-PySpark/ta-p/245905)\n",
    "* [SPARK-13587](https://issues.apache.org/jira/browse/SPARK-13587)\n",
    "\n",
    "> This method is trying to create virtualenv before python worker start, and this virtualenv is application scope, after the spark application job finish, the virtualenv will be cleanup. A\n",
    "\n",
    "Spark setups the virutal environment in the Spark nodes.\n",
    "\n",
    "\n",
    "* Each node must have internet access (for downloading packages).\n",
    "* Python 2.7 or Python 3.x must be installed (pip is also installed).\n",
    "\n",
    "```\n",
    "spark-submit --master yarn-client \\\n",
    "    --conf spark.pyspark.virtualenv.enabled=true \n",
    "    --conf spark.pyspark.virtualenv.type=native\n",
    "    --conf spark.pyspark.virtualenv.requirements=/Users/jzhang/github/spark/requirements.txt\n",
    "    --conf spark.pyspark.virtualenv.bin.path=/Users/jzhang/anaconda/bin/virtualenv \n",
    "    --conf spark.pyspark.python=/usr/local/bin/python3 \\\n",
    "spark_virtualenv.py\n",
    "```\n",
    "\n",
    "### Configuration Parameters \n",
    "\n",
    "| Property | Description |\n",
    "|:---|:---|\n",
    "| spark.pyspark.virtualenv.enabled | Property flag to enable virtualenv |\n",
    "| spark.pyspark.virtualenv.type | Type of virtualenv. Valid values are “native”, “conda” |\n",
    "| spark.pyspark.virtualenv.requirements | Requirements file (optional, not required for interactive mode) |\n",
    "| spark.pyspark.virtualenv.bin.path | The location of virtualenv executable file for type native or conda executable file for type conda |\n",
    "| spark.pyspark.virtualenv.python_version | Python version for conda. (optional, only required when you use conda in interactive mode) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318e32e",
   "metadata": {},
   "source": [
    "## PySpark packages\n",
    "\n",
    "Execute after the PYTHONPATH setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    udf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session without PYSPARK_PYTHON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 10:15:17,981 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-23 10:15:19,811 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-02-23 10:15:21,862 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c405b3",
   "metadata": {},
   "source": [
    "## Use numpy in the worker\n",
    "\n",
    "Will see ```ModuleNotFoundError: No module named 'numpy'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3809ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "|count| df|docs|\n",
      "+-----+---+----+\n",
      "|  138|  5|  10|\n",
      "|  128|  4|  10|\n",
      "|  112|  3|  10|\n",
      "|  120|  3|  10|\n",
      "|  189|  1|  10|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.createDataFrame(\n",
    "    data = [(138,5,10), (128,4,10), (112,3,10), (120,3,10), (189,1,10)], \n",
    "    schema=[\"count\",\"df\",\"docs\"]\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749797fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"float\")\n",
    "def newFunction(count, df, docs):\n",
    "    import numpy as np\n",
    "    returnValue = (1 + np.log(count)) * np.log(docs/df)\n",
    "    return returnValue.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f69225f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 10:16:02,185 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (ubuntu executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_40170/3270350464.py\", line 3, in newFunction\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-02-23 10:16:02,399 ERROR scheduler.TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_40170/3270350464.py\", line 3, in newFunction\nModuleNotFoundError: No module named 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m result\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_function_result\u001b[39m\u001b[38;5;124m\"\u001b[39m, newFunction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/spark-3.1.2/python/lib/pyspark.zip/pyspark/sql/dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/spark-3.1.2/python/lib/pyspark.zip/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0012/container_1645514515228_0012_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_40170/3270350464.py\", line 3, in newFunction\nModuleNotFoundError: No module named 'numpy'\n"
     ]
    }
   ],
   "source": [
    "result=result.withColumn(\"new_function_result\", newFunction(\"count\",\"df\",\"docs\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14077d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark\n",
    "spark.stop()\n",
    "del spark\n",
    "gc.collect()\n",
    "\n",
    "# Need to restart the Jupyter kernel otherwise the error \"Spark stopped\"\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457fa1d",
   "metadata": {},
   "source": [
    "# Spark Session with PYSPARK_PYTHON\n",
    "\n",
    "Providing the PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f97a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ad8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768314c",
   "metadata": {},
   "source": [
    "## PYSPARK_PYTHON Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"/home/oonisim/venv/ml/bin/python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e03e74",
   "metadata": {},
   "source": [
    "## PySpark packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a49e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    udf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c52d9f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 10:19:49,450 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-23 10:19:52,605 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "#    .config('spark.yarn.appMasterEnv.PYSPARK_PYTHON', \"/home/oonisim/venv/ml/bin/python3\")\\\n",
    "#    .config('spark.yarn.executorEnv.PYSPARK_PYTHON', \"/home/oonisim/venv/ml/bin/python3\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97672fb2",
   "metadata": {},
   "source": [
    "## Use numpy in the worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21435683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "|count| df|docs|\n",
      "+-----+---+----+\n",
      "|  138|  5|  10|\n",
      "|  128|  4|  10|\n",
      "|  112|  3|  10|\n",
      "|  120|  3|  10|\n",
      "|  189|  1|  10|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.createDataFrame(\n",
    "    data = [(138,5,10), (128,4,10), (112,3,10), (120,3,10), (189,1,10)], \n",
    "    schema=[\"count\",\"df\",\"docs\"]\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c64860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 10:33:00,182 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 5.0 (TID 8) (ubuntu executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000003/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_41389/1210656308.py\", line 3, in newFunction\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-02-23 10:33:00,306 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 5.0 (TID 9) (ubuntu executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_41389/1210656308.py\", line 3, in newFunction\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 10:33:00,417 ERROR scheduler.TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_41389/1210656308.py\", line 3, in newFunction\nModuleNotFoundError: No module named 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m returnValue\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      7\u001b[0m result\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_function_result\u001b[39m\u001b[38;5;124m\"\u001b[39m, newFunction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/spark-3.1.2/python/lib/pyspark.zip/pyspark/sql/dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/spark-3.1.2/python/lib/pyspark.zip/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/oonisim/appcache/application_1645514515228_0014/container_1645514515228_0014_01_000002/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_41389/1210656308.py\", line 3, in newFunction\nModuleNotFoundError: No module named 'numpy'\n"
     ]
    }
   ],
   "source": [
    "@udf(\"float\")\n",
    "def newFunction(count, df, docs):\n",
    "    import numpy as np\n",
    "    returnValue = (1 + np.log(count)) * np.log(docs/df)\n",
    "    return returnValue.item()\n",
    "\n",
    "result=result.withColumn(\"new_function_result\", newFunction(\"count\",\"df\",\"docs\"))\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1202"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bbf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
