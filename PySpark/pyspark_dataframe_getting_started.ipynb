{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# PySpark DataFrame Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13fc02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc972bc5",
   "metadata": {},
   "source": [
    "## PySpark package imports\n",
    "\n",
    "Execute after the PYTHONPATH setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7bc37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    avg,\n",
    "    stddev,\n",
    "    isnan\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b9883",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "* [The UC Irvine Machine Learning Repository  - Record Linkage Comparison Patterns Data Set](https://archive.ics.uci.edu/ml/datasets/Record+Linkage+Comparison+Patterns)\n",
    "\n",
    "The data are pairs of patient records to identify the two records refer to the same patient or not (na-yose in Japanse).It is from the record linkage study performed at a hospital in 2010 analyzing pairs of patient records that were matched according to several different criteria, such as the patient’s name (first and last), address, and birthday. \n",
    "\n",
    "Each matching field was assigned a numerical score from 0.0 to 1.0 based on how similar the strings were, and the data was then hand-labeled to identify which pairs represented the same person and which did not. \n",
    "\n",
    "\n",
    "| feature | description  |\n",
    "|:---------|:--------------|\n",
    "| is_match| if the pair is a match or not (1: match)          |\n",
    "| cmp_sex | if the gender of the pair is a match (1:match)             |\n",
    "|         |              |\n",
    "|         |              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a5c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   163  100   163    0     0    545      0 --:--:-- --:--:-- --:--:--   543\n",
      "100 53.8M  100 53.8M    0     0  7688k      0  0:00:07  0:00:07 --:--:-- 10.3M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  donation.zip\n",
      " extracting: block_10.zip            \n",
      " extracting: block_1.zip             \n",
      " extracting: block_2.zip             \n",
      " extracting: block_3.zip             \n",
      " extracting: block_4.zip             \n",
      " extracting: block_5.zip             \n",
      " extracting: block_6.zip             \n",
      " extracting: block_7.zip             \n",
      " extracting: block_8.zip             \n",
      " extracting: block_9.zip             \n",
      "  inflating: documentation           \n",
      "  inflating: frequencies.csv         \n",
      "Archive:  block_9.zip\n",
      "  inflating: block_9.csv             \n",
      "\n",
      "Archive:  block_4.zip\n",
      "  inflating: block_4.csv             \n",
      "\n",
      "Archive:  block_10.zip\n",
      "  inflating: block_10.csv            \n",
      "\n",
      "Archive:  block_1.zip\n",
      "  inflating: block_1.csv             \n",
      "\n",
      "Archive:  block_6.zip\n",
      "  inflating: block_6.csv             \n",
      "\n",
      "Archive:  block_5.zip\n",
      "  inflating: block_5.csv             \n",
      "\n",
      "Archive:  block_2.zip\n",
      "  inflating: block_2.csv             \n",
      "\n",
      "Archive:  block_8.zip\n",
      "  inflating: block_8.csv             \n",
      "\n",
      "Archive:  block_3.zip\n",
      "  inflating: block_3.csv             \n",
      "\n",
      "Archive:  block_7.zip\n",
      "  inflating: block_7.csv             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10 archives were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p ./data/linkage\n",
    "cd ./data/linkage/\n",
    "curl -L -o donation.zip https://bit.ly/1Aoywaq\n",
    "unzip -o donation.zip\n",
    "unzip -o 'block_*.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b58ed336",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/linkage/\n",
    "hdfs dfs -mkdir -p linkage\n",
    "hdfs dfs -put -f block_*.csv linkage\n",
    "\n",
    "rm -rf block_* documentation frequencies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 09:21:59,289 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-19 09:22:03,324 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc80e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "NUM_PARTITIONS = 3\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201eac20",
   "metadata": {},
   "source": [
    "# Read CSV\n",
    "\n",
    "* [SparkSQL CSV Files](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)\n",
    "\n",
    "> Spark SQL provides spark.read().csv(\"file_name\") to read a file or directory of files in CSV format into Spark DataFrame, and dataframe.write().csv(\"path\") to write to a CSV file. Function option() can be used to customize the behavior of reading or writing.\n",
    "\n",
    "[SparkSession.read()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.html#read--) returns [DataFrameReader](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html) instance which has [option](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-boolean-) method by which we can specify CSV options.\n",
    "\n",
    "The options are listed in [Data Source Option](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30caa51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev = spark.read.csv(\"linkage\")\n",
    "prev.printSchema()\n",
    "del prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1622305",
   "metadata": {},
   "source": [
    "## CSV Options\n",
    "\n",
    "* Schema Inference \n",
    "* Null value replacement\n",
    "* Header handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eecc4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"?\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"linkage\")\n",
    "\n",
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0e5b6",
   "metadata": {},
   "source": [
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47ba9be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba059fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfceeb4",
   "metadata": {},
   "source": [
    "## Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d886bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 186:>                                                        (0 + 2) / 3]\r",
      "\r",
      "[Stage 186:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "|summary|              id_1|              id_2|      cmp_fname_c1|      cmp_fname_c2|       cmp_lname_c1|       cmp_lname_c2|            cmp_sex|             cmp_bd|             cmp_bm|            cmp_by|            cmp_plz|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "|  count|           5749132|           5749132|           5748125|            103698|            5749132|               2464|            5749132|            5748337|            5748337|           5748337|            5736289|\n",
      "|   mean| 33324.48559643438| 66587.43558331935|0.7129024704425707| 0.900017671890335|0.31562781930763056|0.31841283153174366|  0.955001381078048|0.22446526708507172|0.48885529849763504|0.2227485966810923|0.00552866147434343|\n",
      "| stddev|23659.859374487987|23620.487613269706|0.3887583596162788|0.2713176105782331| 0.3342336339615803| 0.3685670662006655|0.20730111116897443|0.41722972238461925| 0.4998758236779003|0.4160909629831711|0.07414914925420013|\n",
      "|    min|                 1|                 6|               0.0|               0.0|                0.0|                0.0|                  0|                  0|                  0|                 0|                  0|\n",
      "|    max|             99980|            100000|               1.0|               1.0|                1.0|                1.0|                  1|                  1|                  1|                 1|                  1|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary = parsed.describe()\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b133a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+--------------------+-------------------+------------------+\n",
      "|summary|              id_1|              id_2|       cmp_fname_c1|       cmp_fname_c2|       cmp_lname_c1|      cmp_lname_c2|            cmp_sex|             cmp_bd|              cmp_bm|             cmp_by|           cmp_plz|\n",
      "+-------+------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+--------------------+-------------------+------------------+\n",
      "|  count|             20931|             20931|              20922|               1333|              20931|               475|              20931|              20925|               20925|              20925|             20902|\n",
      "|   mean| 34575.72117911232| 51259.95939037791| 0.9973163859635039| 0.9898900320318176| 0.9970152595958813| 0.969370167843852|  0.987291577086618| 0.9970848267622461|  0.9979450418160095| 0.9961290322580645|0.9584250310975027|\n",
      "| stddev|21950.312851969127|24345.733453775203|0.03650667584833678|0.08251973727615236|0.04311880753394509|0.1534528074038892|0.11201570591216439|0.05391487659807982|0.045286127452170685|0.06209804856731052|0.1996206334593192|\n",
      "|    min|                 5|                 6|                0.0|                0.0|                0.0|               0.0|                  0|                  0|                   0|                  0|                 0|\n",
      "|    max|             99946|             99996|                1.0|                1.0|                1.0|               1.0|                  1|                  1|                   1|                  1|                 1|\n",
      "+-------+------------------+------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matched_summary = parsed.where(col(\"is_match\") == \"true\").describe()\n",
    "matched_summary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d0fa5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 214:>                                                        (0 + 2) / 3]\r",
      "\r",
      "[Stage 214:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+-------------------+--------------------+\n",
      "|summary|              id_1|             id_2|      cmp_fname_c1|      cmp_fname_c2|      cmp_lname_c1|       cmp_lname_c2|            cmp_sex|            cmp_bd|            cmp_bm|             cmp_by|             cmp_plz|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+-------------------+--------------------+\n",
      "|  count|           5728201|          5728201|           5727203|            102365|           5728201|               1989|            5728201|           5727412|           5727412|            5727412|             5715387|\n",
      "|   mean|33319.913548075565|66643.44259218557|0.7118634802163704| 0.898847351409032|0.3131380113360652|0.16295544855122573| 0.9548833918362851|0.2216425149788421| 0.486995347986141| 0.2199230647280133|0.002043781112285135|\n",
      "| stddev| 23665.76013033079|23599.55172824128| 0.389080600969852|0.2727209029401019|0.3322812130572728|0.19302366635287022|0.20755988859217647|0.4153518275558802|0.4998308940493894|0.41419432671429895| 0.04516197989362574|\n",
      "|    min|                 1|               30|               0.0|               0.0|               0.0|                0.0|                  0|                 0|                 0|                  0|                   0|\n",
      "|    max|             99980|           100000|               1.0|               1.0|               1.0|                1.0|                  1|                 1|                 1|                  1|                   1|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+-------------------+-------------------+------------------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unmatched_summary = parsed.where(col(\"is_match\") == \"false\").describe()\n",
    "unmatched_summary.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b1819",
   "metadata": {},
   "source": [
    "## Rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6caa0434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|17186|64804|        null|        null|         0.5|        null|      1|     0|     0|     1|      0|   false|\n",
      "|58872|80686|        null|        null|       0.125|        null|      0|     1|     1|     1|      0|   false|\n",
      "|19093|75754|        null|        null|         1.0|        null|      1|     0|     0|     0|      0|   false|\n",
      "|51568|69136|        null|        null|       0.625|        null|      1|     0|     0|     0|      0|   false|\n",
      "|36952|63401|        null|        null|         0.0|        null|      1|     1|     1|     1|      0|   false|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.where(\n",
    "    col(\"cmp_fname_c1\").isNull() &\n",
    "    col(\"cmp_fname_c2\").isNull() & \n",
    "    (col(\"is_match\") == \"false\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3eed0c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5645434"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.where(\n",
    "    col(\"cmp_fname_c2\").isNull() \n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4993e994",
   "metadata": {},
   "source": [
    "---\n",
    "# DataFrame Structure\n",
    "\n",
    "## Comparison with HTML\n",
    "\n",
    "| HTML       | HTML Description             | Spark       | Spark Description                                                                                                                                                                                                                                                                                                                       |   |\n",
    "|------------|-------------------------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|\n",
    "| ```<table>```    | List of ```<tr>```            | DataFrame   | List of Row, and DataFrame is an alias of type DataSet[Row].                                                                                                                                                                                                                                                                      |   |\n",
    "| ```<tr>```       | Table row. List of ```<td> ```| Row         | List of typed values.  ```Row``` has the schema of type **StructType** |   |\n",
    "|            |                         | StructType  | Defines the type of a Row and a list of StructField that defines a field of a Row.                                                                                                                                                                                                                                                |   |\n",
    "|```<td>```Table  | Table field             | StructField | Specify the DataType of a field of a row.                                                                                                                                                                                                                                                                                         |   |\n",
    "|            |                         | DataType    |                                                                                                                                                                                                                                                                                                                                   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd05ad",
   "metadata": {},
   "source": [
    "## DataFrame Schema Definition\n",
    "\n",
    "Instead of using **Schema Inference**, you can provide the **Schema Definition** for  ```SparkSession.read.schema(schema).csv(file)``` to use when reading the CSV file.\n",
    "\n",
    "\n",
    "* [Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html#data-types)\n",
    "\n",
    "```from pyspark.sql.types import *```\n",
    "\n",
    "| Data type | Value type in Python | API to access or create a data type |  |\n",
    "|:---|:---|:---|:--|\n",
    "|ByteType | int or long Note: Numbers will be converted to 1-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -128 to 127. | ByteType() |  |\n",
    "| ShortType | int or long Note: Numbers will be converted to 2-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -32768 to 32767. | ShortType() |  |\n",
    "| IntegerType | int or long | IntegerType() |  |\n",
    "| LongType | long Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807.Otherwise, please convert data to decimal.Decimal and use DecimalType. | LongType() |  |\n",
    "| FloatType | float Note: Numbers will be converted to 4-byte single-precision floating point numbers at runtime. | FloatType() |  |\n",
    "| DoubleType | float | DoubleType() |  |\n",
    "| DecimalType | decimal.Decimal | DecimalType() |  |\n",
    "| StringType | string | StringType() |  |\n",
    "| BinaryType | bytearray | BinaryType() |  |\n",
    "| BooleanType | bool | BooleanType() |  |\n",
    "| TimestampType | datetime.datetime | TimestampType() |  |\n",
    "| DateType | datetime.date | DateType() |  |\n",
    "| ArrayType | list, tuple, or array | ArrayType(elementType, [containsNull]) Note:The default value of containsNull is True. |  |\n",
    "| MapType | dict | MapType(keyType, valueType, [valueContainsNull]) Note:The default value of valueContainsNull is True. |  |\n",
    "| StructType | list or tuple | StructType(fields) Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed. |  |\n",
    "| StructField | The value type in Python of the data type of this field (For example, Int for a StructField with the data type IntegerType) | StructField(name, dataType, [nullable]) Note: The default value of nullable is True. |  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e182641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField(id_1,IntegerType,false)\n",
      "StructField(id_2,StringType,false)\n",
      "StructField(cmp_fname_c1,DoubleType,false)\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id_1\", IntegerType(), False),\n",
    "    StructField(\"id_2\", StringType(), False),\n",
    "    StructField(\"cmp_fname_c1\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "for element in schema:\n",
    "    print(element)\n",
    "    \n",
    "# spark.read.schema(schema).csv(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a51f7",
   "metadata": {},
   "source": [
    "## Row\n",
    "\n",
    "Each row of the DataFrame is an instance of ```pyspark.sql.Row```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f186dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id_1=3148, id_2=8326, cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row: pyspark.sql.Row = parsed.first()\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276809f",
   "metadata": {},
   "source": [
    "## Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb2190db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_1 type:<class 'int'> value: 3148\n"
     ]
    }
   ],
   "source": [
    "id_1 = row['id_1']\n",
    "print(f\"id_1 type:{type(id_1)} value: {id_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b4e10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e79d36",
   "metadata": {},
   "source": [
    "# Caching the dataframe\n",
    "\n",
    "The call to ```cache``` indicates that the contents of the DataFrame should be stored in memory the next time it’s computed. Spark defines a few different mechanisms, or StorageLevel values, for persisting data. cache() is shorthand for [DataFrame.persist](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.persist.html)(StorageLevel.MEMORY), which stores the rows as unserialized Java objects (NOT Python objects).\n",
    "\n",
    "* [class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html)\n",
    "\n",
    "## [RDD Persistence](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)\n",
    "\n",
    "> One of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n",
    "> \n",
    "> You can mark an RDD to be persisted using the persist() or cache() methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.\n",
    "> \n",
    "> In addition, each persisted RDD can be stored using a different **storage level**, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a StorageLevel object (Scala, Java, Python) to persist(). The ```cache()``` method is a shorthand for using the default storage level, which is **StorageLevel.MEMORY_ONLY** (store deserialized objects in memory). The full set of storage levels is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a175624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_1: int, id_2: int, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74057827",
   "metadata": {},
   "source": [
    "---\n",
    "# Cast column type\n",
    "\n",
    "Estimates of the data has string columns which should be numeric. Convert them to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8c870c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "62aa48c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "|summary|              id_1|              id_2|      cmp_fname_c1|      cmp_fname_c2|       cmp_lname_c1|       cmp_lname_c2|            cmp_sex|             cmp_bd|             cmp_bm|            cmp_by|            cmp_plz|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "|  count|           5749132|           5749132|           5748125|            103698|            5749132|               2464|            5749132|            5748337|            5748337|           5748337|            5736289|\n",
      "|   mean| 33324.48559643438| 66587.43558331935|0.7129024704425707| 0.900017671890335|0.31562781930763056|0.31841283153174366|  0.955001381078048|0.22446526708507172|0.48885529849763504|0.2227485966810923|0.00552866147434343|\n",
      "| stddev|23659.859374487987|23620.487613269706|0.3887583596162788|0.2713176105782331| 0.3342336339615803| 0.3685670662006655|0.20730111116897443|0.41722972238461925| 0.4998758236779003|0.4160909629831711|0.07414914925420013|\n",
      "|    min|                 1|                 6|               0.0|               0.0|                0.0|                0.0|                  0|                  0|                  0|                 0|                  0|\n",
      "|    max|             99980|            100000|               1.0|               1.0|                1.0|                1.0|                  1|                  1|                  1|                 1|                  1|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1b028843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- id_1: double (nullable = true)\n",
      " |-- id_2: double (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: double (nullable = true)\n",
      " |-- cmp_bd: double (nullable = true)\n",
      " |-- cmp_bm: double (nullable = true)\n",
      " |-- cmp_by: double (nullable = true)\n",
      " |-- cmp_plz: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in summary.columns[1:]:\n",
    "    summary = summary.withColumn(column, summary[column].cast(DoubleType()))\n",
    "\n",
    "summary.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e470f2",
   "metadata": {},
   "source": [
    "# Transpose Dataframe\n",
    "\n",
    "Spark DataFrame does not have transpose method. When the data size is small, convert first to Pandas to transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "30167906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 206:>                                                        (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+-------------------+---+--------+\n",
      "|       field|    count|               mean|             stddev|min|     max|\n",
      "+------------+---------+-------------------+-------------------+---+--------+\n",
      "|        id_1|5749132.0|  33324.48559643438| 23659.859374487987|1.0| 99980.0|\n",
      "|        id_2|5749132.0|  66587.43558331935| 23620.487613269706|6.0|100000.0|\n",
      "|cmp_fname_c1|5748125.0| 0.7129024704425707| 0.3887583596162788|0.0|     1.0|\n",
      "|cmp_fname_c2| 103698.0|  0.900017671890335| 0.2713176105782331|0.0|     1.0|\n",
      "|cmp_lname_c1|5749132.0|0.31562781930763056| 0.3342336339615803|0.0|     1.0|\n",
      "|cmp_lname_c2|   2464.0|0.31841283153174366| 0.3685670662006655|0.0|     1.0|\n",
      "|     cmp_sex|5749132.0|  0.955001381078048|0.20730111116897443|0.0|     1.0|\n",
      "|      cmp_bd|5748337.0|0.22446526708507172|0.41722972238461925|0.0|     1.0|\n",
      "|      cmp_bm|5748337.0|0.48885529849763504| 0.4998758236779003|0.0|     1.0|\n",
      "|      cmp_by|5748337.0| 0.2227485966810923| 0.4160909629831711|0.0|     1.0|\n",
      "|     cmp_plz|5736289.0|0.00552866147434343|0.07414914925420013|0.0|     1.0|\n",
      "+------------+---------+-------------------+-------------------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transpose(df: pyspark.sql.DataFrame):\n",
    "    assert df.count() < 1000\n",
    "    pdf = df.toPandas()\n",
    "    pdf = pdf.set_index(df.columns[0]).transpose().reset_index()\n",
    "    pdf = pdf.rename(columns={\"index\":\"field\"})\n",
    "    pdf = pdf.rename_axis(None, axis=1)\n",
    "    \n",
    "    transposed: pyspark.sql.Dataframe = spark.createDataFrame(pdf)\n",
    "    del pdf\n",
    "    return transposed\n",
    "   \n",
    "\n",
    "transpose(summary).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcab4b3",
   "metadata": {},
   "source": [
    "---\n",
    "# Group By and Aggregation\n",
    "\n",
    "## Column name reference\n",
    "Two ways we can reference the names of the columns in the DataFrame: either as literal strings, like in groupBy(\"is_match\"), or as Column objects by using the \"col()\" function. Need to use the ```col ```function to call the ```desc``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "573e1260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.groupby('is_match').count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d35356c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:============================================>         (164 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|is_match|        avg(cmp_plz)|stddev_samp(cmp_sex)|\n",
      "+--------+--------------------+--------------------+\n",
      "|    true|  0.9584250310975027|  0.1120157059121644|\n",
      "|   false|0.002043781112285135| 0.20755988859217647|\n",
      "+--------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:==================================================>   (186 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed.groupby('is_match').agg(\n",
    "    avg(\"cmp_plz\"),\n",
    "    stddev(\"cmp_sex\")\n",
    ").orderBy(col(\"is_match\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76f5d6",
   "metadata": {},
   "source": [
    "---\n",
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2fc8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9721a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 152:====================================================> (97 + 2) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------------------------+\n",
      "|    cnt|avg_plz|std(CAST(cmp_sex AS DOUBLE))|\n",
      "+-------+-------+----------------------------+\n",
      "|  20931|0.95843|          0.1120157059121644|\n",
      "|5728201|0.00204|         0.20755988859217644|\n",
      "+-------+-------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(is_match) AS cnt,\n",
    "    ROUND(AVG(cmp_plz),5) AS avg_plz,\n",
    "    STD(cmp_sex)\n",
    "FROM linkage\n",
    "GROUP BY is_match\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b92ebc",
   "metadata": {},
   "source": [
    "## Join\n",
    "\n",
    "Calculate the diffence of mean values of the fields between matched records and unmatched records to identify the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cf612056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------------------+-------------------+---+-----+\n",
      "|       field|count|              mean|             stddev|min|  max|\n",
      "+------------+-----+------------------+-------------------+---+-----+\n",
      "|        id_1|20931| 34575.72117911232| 21950.312851969127|  5|99946|\n",
      "|        id_2|20931| 51259.95939037791| 24345.733453775203|  6|99996|\n",
      "|cmp_fname_c1|20922|0.9973163859635039|0.03650667584833678|0.0|  1.0|\n",
      "+------------+-----+------------------+-------------------+---+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------+-------+------------------+-----------------+---+------+\n",
      "|       field|  count|              mean|           stddev|min|   max|\n",
      "+------------+-------+------------------+-----------------+---+------+\n",
      "|        id_1|5728201|33319.913548075565|23665.76013033079|  1| 99980|\n",
      "|        id_2|5728201| 66643.44259218557|23599.55172824128| 30|100000|\n",
      "|cmp_fname_c1|5727203|0.7118634802163704|0.389080600969852|0.0|   1.0|\n",
      "+------------+-------+------------------+-----------------+---+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matched_summary_transposed = transpose(matched_summary)\n",
    "matched_summary_transposed.show(3)\n",
    "\n",
    "unmatched_summary_transposed = transpose(unmatched_summary)\n",
    "unmatched_summary_transposed.show(3)\n",
    "\n",
    "matched_summary_transposed.createOrReplaceTempView(\"matched\")\n",
    "unmatched_summary_transposed.createOrReplaceTempView(\"unmatched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eccf9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+---------+----------+\n",
      "|matched_count|unmatch_count|    total|mean_delta|\n",
      "+-------------+-------------+---------+----------+\n",
      "|        20902|      5715387|5736289.0|     0.956|\n",
      "|          475|         1989|   2464.0|     0.806|\n",
      "|        20925|      5727412|5748337.0|     0.776|\n",
      "|        20925|      5727412|5748337.0|     0.775|\n",
      "|        20931|      5728201|5749132.0|     0.684|\n",
      "|        20925|      5727412|5748337.0|     0.511|\n",
      "|        20922|      5727203|5748125.0|     0.285|\n",
      "|         1333|       102365| 103698.0|     0.091|\n",
      "|        20931|      5728201|5749132.0|     0.032|\n",
      "+-------------+-------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.count AS matched_count,\n",
    "    u.count AS unmatch_count,\n",
    "    m.count + u.count as total,\n",
    "    ROUND(m.mean - u.mean, 3) as mean_delta\n",
    "FROM\n",
    "    matched AS m \n",
    "    INNER JOIN unmatched u ON m.field = u.field\n",
    "WHERE\n",
    "    m.field NOT IN ('id_1', 'id_2')\n",
    "ORDER BY \n",
    "    mean_delta desc\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2181"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d5b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
