{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# How to use external jars\n",
    "\n",
    "* [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)\n",
    "\n",
    "> * spark.jars  \n",
    "> Comma-separated list of jars to include on the **driver and executor classpaths**. Globs are allowed.\n",
    "\n",
    "> * spark.driver.extraClassPath  \n",
    "> Extra classpath entries to prepend to the classpath of the driver.\n",
    "\n",
    "> * spark.**executor**.extraClassPath  \n",
    "> This exists primarily for **backwards-compatibility** with older versions of Spark. Users typically **should NOT need to set this option**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f86f7",
   "metadata": {},
   "source": [
    "# Example (Hadoop GCP/BQ Connector)\n",
    "\n",
    "Demonstrate how to use the external [Apache Spark SQL connector for Google BigQuery](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) jar using the example from [Use the BigQuery connector with Spark](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example#pyspark).\n",
    "\n",
    "MUST follow the instruction [Installing the connector](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md).\n",
    "\n",
    "> ### Configureing Spark\n",
    "> ```\n",
    "> spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\n",
    "> spark.hadoop.google.cloud.auth.service.account.enable=true\n",
    "> spark.hadoop.google.cloud.auth.service.account.json.keyfile=<path/to/keyfile.json>\n",
    "> ```\n",
    "\n",
    "## References\n",
    "\n",
    "* [https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md is not up-to-date #618](https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/618)\n",
    "* [Clarification about installation with spark #188](https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/188) (MUST)\n",
    "* [When writing to BQ run into this error No FileSystem for scheme: gs #206](https://github.com/GoogleCloudDataproc/spark-bigquery-connector/issues/206)\n",
    "* [Accessing GCS from Spark/Hadoop outside Google Cloud #52](https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/52)\n",
    "* [Configuration properties](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274df33",
   "metadata": {},
   "source": [
    "## GCP Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c65e6",
   "metadata": {},
   "source": [
    "### Application Default Credentials\n",
    "\n",
    "* [Installing the connector](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md).\n",
    "\n",
    "> ### Ensure authenticated Cloud Storage access\n",
    "> Depending on where the machines which comprise your cluster are located, you must do one of the following:  \n",
    "> * non-Google Cloud Platform - Obtain an [OAuth 2.0 private key](https://cloud.google.com/storage/docs/authentication#generating-a-private-key). \n",
    "\n",
    "Spark needs to authenticate itself with GCP via the credential file pointed to by ```GOOGLE_APPLICATION_CREDENTIALS``` environment variable. \n",
    "\n",
    "* [Authentication on GCP with Docker: Application Default Credentials](https://medium.com/datamindedbe/application-default-credentials-477879e31cb5)\n",
    "* [Accessing GCS from Spark/Hadoop outside Google Cloud #52](https://github.com/GoogleCloudDataproc/hadoop-connectors/issues/52)\n",
    "* [Authentication overview](https://cloud.google.com/docs/authentication)\n",
    "* [Authenticating as a service account](https://cloud.google.com/docs/authentication/production)\n",
    "\n",
    "\n",
    "The credential file is created by executing the command:\n",
    "\n",
    "```\n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "Set the Spark properties to enable the GCP authentication and refer to the key file.\n",
    "\n",
    "```\n",
    "spark.hadoop.google.cloud.auth.service.account.enable=true\n",
    "spark.hadoop.google.cloud.auth.service.account.json.keyfile=<path/to/keyfile.json>\n",
    "```\n",
    "\n",
    "Note that both the Spark job submit account and Spark executor account need the permission to read the file, otherwise Permission Denied error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f10a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_APPLICATION_CREDENTIALS = '/home/spark/.config/gcloud/application_default_credentials.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff4e91",
   "metadata": {},
   "source": [
    "### BigQuery Connector jar\n",
    "\n",
    "Need two jar files. As far as I know, it is not clearly documented.\n",
    "\n",
    "1. spark-bigquery-latest_2.12.jar\n",
    "2. gcs-connector-hadoop3-2.2.2-shaded.jar\n",
    "\n",
    "* [Apache Spark SQL connector for Google BigQuery (Beta)](https://github.com/GoogleCloudDataproc/spark-bigquery-connector#downloading-the-connector)\n",
    "\n",
    "| version | Link |\n",
    "| --- | --- |\n",
    "| Scala 2.11 | `gs://spark-lib/bigquery/spark-bigquery-latest_2.11.jar` ([HTTP link](https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-latest_2.11.jar)) |\n",
    "| Scala 2.12 | `gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar` ([HTTP link](https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-latest_2.12.jar)) |\n",
    "\n",
    "* [Getting the connector](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters)\n",
    "\n",
    "> specific version from Apache Maven repository (you should download a shaded jar that has -shaded suffix in the name):  \n",
    "> * Cloud Storage connector for [Hadoop 3.x](https://search.maven.org/search?q=g:com.google.cloud.bigdataoss%20AND%20a:gcs-connector%20AND%20v:hadoop3-*)\n",
    "\n",
    "\n",
    "\n",
    "Download the jars for the Spark version to a local location and setup the Spark property (comma separated) to the  jar files.\n",
    "\n",
    "\n",
    "```\n",
    "spark.jar=/path/to/jar1,/path/to/jar2,/path/to/jar3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575f57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.realpath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc970a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar_spark_bigquery = \"spark-bigquery-latest_2.12.jar\"\n",
    "source = f\"gs://spark-lib/bigquery/{jar_spark_bigquery}\"\n",
    "classpath = f\"{DIR}/{jar_spark_bigquery}\"\n",
    "\n",
    "!gsutil -q cp {source} {classpath}\n",
    "# !ls {classpath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c650ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_version = \"3-2.2.2\"\n",
    "jar_hadoop_bigquery = f\"gcs-connector-hadoop{hadoop_version}-shaded.jar\"\n",
    "url=f\"https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.2/{jar_hadoop_bigquery}\"\n",
    "\n",
    "!wget -q -O {DIR}/{jar_hadoop_bigquery} {url}\n",
    "classpath = f\"{classpath},{DIR}/{jar_hadoop_bigquery}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ac813",
   "metadata": {},
   "source": [
    "### GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8a4cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive-theme-323611'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=!(gcloud config get-value project)\n",
    "project=result[0]\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ddf966",
   "metadata": {},
   "source": [
    "\n",
    "### BigQuery Dataset\n",
    "\n",
    "* [Running the code](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example#pyspark)\n",
    "> Before running this example, create a dataset named \"wordcount_dataset\" or change the output dataset in the code to an existing BigQuery dataset in your Google Cloud project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be0af4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating BiGQuery dataset positive-theme-323611:wordcount_example\n",
      "Dataset 'positive-theme-323611:wordcount_example' successfully created.\n",
      "Checking BiGQuery dataset positive-theme-323611:wordcount_example\n",
      "      datasetId      \n",
      " ------------------- \n",
      "  gcpbook_ch5        \n",
      "  wordcount_example  \n"
     ]
    }
   ],
   "source": [
    "dataset = f\"{project}:wordcount_example\"\n",
    "print(f\"Recreating BiGQuery dataset {dataset}\")\n",
    "\n",
    "!bq --location=us rm --force=true --dataset=true {dataset} || echo \"\"\n",
    "!bq --location=us mk --dataset {dataset}\n",
    "\n",
    "print(f\"Checking BiGQuery dataset {dataset}\")\n",
    "!bq --location=us ls --datasets=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372dbc18",
   "metadata": {},
   "source": [
    "### Cloud Storage Bucket\n",
    "\n",
    "> Create a Cloud Storage bucket, which will be used to export to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb0dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating positive-theme-323611-wordcount-example\n",
      "Removing gs://positive-theme-323611-wordcount-example/...\n",
      "Creating gs://positive-theme-323611-wordcount-example/...\n",
      "Checking positive-theme-323611-wordcount-example\n",
      "gs://positive-theme-323611-wordcount-example/ :\n",
      "\tStorage class:\t\t\tSTANDARD\n",
      "\tLocation type:\t\t\tmulti-region\n",
      "\tLocation constraint:\t\tUS\n",
      "\tVersioning enabled:\t\tNone\n",
      "\tLogging configuration:\t\tNone\n",
      "\tWebsite configuration:\t\tNone\n",
      "\tCORS configuration: \t\tNone\n",
      "\tLifecycle configuration:\tNone\n",
      "\tRequester Pays enabled:\t\tNone\n",
      "\tLabels:\t\t\t\tNone\n",
      "\tDefault KMS key:\t\tNone\n",
      "\tTime created:\t\t\tFri, 17 Sep 2021 00:22:11 GMT\n",
      "\tTime updated:\t\t\tFri, 17 Sep 2021 00:22:11 GMT\n",
      "\tMetageneration:\t\t\t1\n",
      "\tBucket Policy Only enabled:\tFalse\n",
      "\tPublic access prevention:\tunspecified\n",
      "\tACL:\t\t\t\t\n",
      "\t  [\n",
      "\t    {\n",
      "\t      \"entity\": \"project-owners-412177242019\",\n",
      "\t      \"projectTeam\": {\n",
      "\t        \"projectNumber\": \"412177242019\",\n",
      "\t        \"team\": \"owners\"\n",
      "\t      },\n",
      "\t      \"role\": \"OWNER\"\n",
      "\t    },\n",
      "\t    {\n",
      "\t      \"entity\": \"project-editors-412177242019\",\n",
      "\t      \"projectTeam\": {\n",
      "\t        \"projectNumber\": \"412177242019\",\n",
      "\t        \"team\": \"editors\"\n",
      "\t      },\n",
      "\t      \"role\": \"OWNER\"\n",
      "\t    },\n",
      "\t    {\n",
      "\t      \"entity\": \"project-viewers-412177242019\",\n",
      "\t      \"projectTeam\": {\n",
      "\t        \"projectNumber\": \"412177242019\",\n",
      "\t        \"team\": \"viewers\"\n",
      "\t      },\n",
      "\t      \"role\": \"READER\"\n",
      "\t    }\n",
      "\t  ]\n",
      "\tDefault ACL:\t\t\t\n",
      "\t  [\n",
      "\t    {\n",
      "\t      \"entity\": \"project-owners-412177242019\",\n",
      "\t      \"projectTeam\": {\n",
      "\t        \"projectNumber\": \"412177242019\",\n",
      "\t        \"team\": \"owners\"\n",
      "\t      },\n",
      "\t      \"role\": \"OWNER\"\n",
      "\t    },\n",
      "\t    {\n",
      "\t      \"entity\": \"project-editors-412177242019\",\n",
      "\t      \"projectTeam\": {\n",
      "\t        \"projectNumber\": \"412177242019\",\n",
      "\t        \"team\": \"editors\"\n",
      "\t      },\n",
      "\t      \"role\": \"OWNER\"\n",
      "\t    },\n",
      "\t    {\n",
      "\t      \"entity\": \"project-viewers-412177242019\",\n",
      "\t      \"projectTeam\": {\n",
      "\t        \"projectNumber\": \"412177242019\",\n",
      "\t        \"team\": \"viewers\"\n",
      "\t      },\n",
      "\t      \"role\": \"READER\"\n",
      "\t    }\n",
      "\t  ]\n"
     ]
    }
   ],
   "source": [
    "bucket = \"positive-theme-323611-wordcount-example\"\n",
    "\n",
    "print(f\"Recreating {bucket}\")\n",
    "!gsutil rm -r gs://{bucket}\n",
    "!gsutil mb gs://{bucket}\n",
    "\n",
    "print(f\"Checking {bucket}\")\n",
    "!gsutil ls -b -L gs://{bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bed51e",
   "metadata": {},
   "source": [
    "---\n",
    "# PySpark Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345d990",
   "metadata": {},
   "source": [
    "---\n",
    "# Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d50fc",
   "metadata": {},
   "source": [
    "## HADOOP_CONF_DIR\n",
    "\n",
    "Copy the **HADOOP_CONF_DIR** from the Hadoop/YARN master node and set the ```HADOOP_CONF_DIR``` environment variable locally to point to the directory.\n",
    "\n",
    "* [Launching Spark on YARN\n",
    "](http://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn)\n",
    "\n",
    "> Ensure that **HADOOP_CONF_DIR** or **YARN_CONF_DIR** points to the directory which contains the (client side) configuration files for the Hadoop cluster. These configs are used to write to HDFS and connect to the YARN ResourceManager. The configuration contained in this directory will be distributed to the YARN cluster so that all containers used by the application use the same configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6159b3",
   "metadata": {},
   "source": [
    "## PYSPARK_SUBMIT_ARGS\n",
    "\n",
    "Specify the [spark-submit](https://spark.apache.org/docs/3.1.2/submitting-applications.html#launching-applications-with-spark-submit) parameters.\n",
    "\n",
    "```\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```\n",
    "\n",
    "The ```conf``` paramters are [Spark properties](https://spark.apache.org/docs/latest/configuration.html#available-properties) e.g. ```spark.executor.memory```\n",
    "\n",
    "Alternatively, use [SparkSession.builder](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession).\n",
    "\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "./bin/spark-submit \\\n",
    "  --class org.apache.spark.examples.SparkPi \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --supervise \\\n",
    "  --executor-memory 20G \\\n",
    "  --total-executor-cores 100 \\\n",
    "  /path/to/examples.jar \\\n",
    "  1000\n",
    "```\n",
    "\n",
    "### Environment variable\n",
    "\n",
    "```\n",
    "export PYSPARK_SUBMIT_ARGS='--master yarn --executor-memory 20G --total-executor-cores 100 --num-executors 5 --driver-memory 2g --executor-memory 2g pyspark-submit'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n",
    "\n",
    "Set the Spark properties for GCP authentication.\n",
    "```\n",
    ".config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    ".config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", GOOGLE_APPLICATION_CREDENTIALS) \\\n",
    "```\n",
    "\n",
    "Note that ```.config(\"spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS\", GOOGLE_APPLICATION_CREDENTIALS)``` is not required if the Spark properties have been setup as per [Installing the connector](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md):\n",
    "\n",
    "Set the Spark property for external jar files.\n",
    "```\n",
    ".config(\"spark.jars\", classpath) \\\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/17 10:22:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/09/17 10:22:21 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName('spark-bigquery-demo') \\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", GOOGLE_APPLICATION_CREDENTIALS) \\\n",
    "    .config(\"spark.jars\", classpath) \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e4112",
   "metadata": {},
   "source": [
    "## Access BigQuery using the BigQuery Connector jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0010dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/17 10:23:09 WARN DefaultCredentialsProvider: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     word|word_count|\n",
      "+---------+----------+\n",
      "|     XVII|         2|\n",
      "|    spoil|        28|\n",
      "|    Drink|         7|\n",
      "|forgetful|         5|\n",
      "|   Cannot|        46|\n",
      "|    cures|        10|\n",
      "|   harder|        13|\n",
      "|  tresses|         3|\n",
      "|      few|        62|\n",
      "|  steel'd|         5|\n",
      "| tripping|         7|\n",
      "|   travel|        35|\n",
      "|   ransom|        55|\n",
      "|     hope|       366|\n",
      "|       By|       816|\n",
      "|     some|      1169|\n",
      "|    those|       508|\n",
      "|    still|       567|\n",
      "|      art|       893|\n",
      "|    feign|        10|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- word: string (nullable = false)\n",
      " |-- word_count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Load data from BigQuery.\n",
    "words = spark.read.format('bigquery') \\\n",
    "    .option('table', 'bigquery-public-data:samples.shakespeare') \\\n",
    "    .load()\n",
    "words.createOrReplaceTempView('words')\n",
    "\n",
    "# Perform word count.\n",
    "word_count = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    word, \n",
    "    SUM(word_count) AS word_count \n",
    "FROM words \n",
    "GROUP BY word\n",
    "\"\"\")\n",
    "\n",
    "word_count.show()\n",
    "word_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1426a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Saving the data to BigQuery.\n",
    "# To avoid \"No FileSystem for scheme: gs\", make sure the Spark properties \"spark.hadoop.fs.gs.impl\"\n",
    "word_count.write.format('bigquery') \\\n",
    "    .option('table', f'{dataset}.wordcount_output') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416582dc",
   "metadata": {},
   "source": [
    "## BigQuery Connector Write Result\n",
    "\n",
    "<img src=\"image/spark_gcp_bq_connector_wordcount_example_output.png\" align=\"left\" width=750/>\n",
    "<img src=\"image/spark_gcp_bq_connector_wordcount_example_output_preview.png\" align=\"left\" width=550/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()\n",
    "\n",
    "del spark\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
