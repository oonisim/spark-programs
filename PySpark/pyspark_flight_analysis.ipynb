{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# Flight data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fc02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from datetime import (\n",
    "    datetime,\n",
    "    date\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc972bc5",
   "metadata": {},
   "source": [
    "## PySpark package imports\n",
    "\n",
    "Execute after the PYTHONPATH setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    lit,\n",
    "    avg,\n",
    "    stddev,\n",
    "    isnan,\n",
    "    date_format,\n",
    "    to_date,\n",
    "    months_between,\n",
    "    add_months,\n",
    "    lower,\n",
    "    upper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 07:49:52,678 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-24 07:49:57,214 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc80e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "NUM_PARTITIONS = 3\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c757d",
   "metadata": {},
   "source": [
    "# DataFrame from Python data\n",
    "\n",
    "* [SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd05ad",
   "metadata": {},
   "source": [
    "# Schema Definition\n",
    "\n",
    "* [Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html#data-types)\n",
    "\n",
    "```from pyspark.sql.types import *```\n",
    "\n",
    "| Data type | Value type in Python | API to access or create a data type |  |\n",
    "|:---|:---|:---|:--|\n",
    "|ByteType | int or long Note: Numbers will be converted to 1-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -128 to 127. | ByteType() |  |\n",
    "| ShortType | int or long Note: Numbers will be converted to 2-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -32768 to 32767. | ShortType() |  |\n",
    "| IntegerType | int or long | IntegerType() |  |\n",
    "| LongType | long Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807.Otherwise, please convert data to decimal.Decimal and use DecimalType. | LongType() |  |\n",
    "| FloatType | float Note: Numbers will be converted to 4-byte single-precision floating point numbers at runtime. | FloatType() |  |\n",
    "| DoubleType | float | DoubleType() |  |\n",
    "| DecimalType | decimal.Decimal | DecimalType() |  |\n",
    "| StringType | string | StringType() |  |\n",
    "| BinaryType | bytearray | BinaryType() |  |\n",
    "| BooleanType | bool | BooleanType() |  |\n",
    "| TimestampType | datetime.datetime | TimestampType() |  |\n",
    "| DateType | datetime.date | DateType() |  |\n",
    "| ArrayType | list, tuple, or array | ArrayType(elementType, [containsNull]) Note:The default value of containsNull is True. |  |\n",
    "| MapType | dict | MapType(keyType, valueType, [valueContainsNull]) Note:The default value of valueContainsNull is True. |  |\n",
    "| StructType | list or tuple | StructType(fields) Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed. |  |\n",
    "| StructField | The value type in Python of the data type of this field (For example, Int for a StructField with the data type IntegerType) | StructField(name, dataType, [nullable]) Note: The default value of nullable is True. |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea50fe",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23f5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p flight\n",
    "hdfs dfs -put -f ./data/flight/*.csv flight/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf22b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- flightId: integer (nullable = true)\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- direction: integer (nullable = false)\n",
      "\n",
      "+-----------+--------+----+---+----------+---------+\n",
      "|passengerId|flightId|from|to |date      |direction|\n",
      "+-----------+--------+----+---+----------+---------+\n",
      "|48         |0       |cg  |ir |2017-01-01|0        |\n",
      "|94         |0       |cg  |ir |2017-01-01|0        |\n",
      "|82         |0       |cg  |ir |2017-01-01|0        |\n",
      "|21         |0       |cg  |ir |2017-01-01|0        |\n",
      "|51         |0       |cg  |ir |2017-01-01|0        |\n",
      "+-----------+--------+----+---+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight = spark.read\\\n",
    "    .option(\"compression\", \"none\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"sep\", ',')\\\n",
    "    .option(\"nullValue\", np.nan)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n",
    "    .csv(\"flight/flightData.csv\")\\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\\\n",
    "    .withColumn(\n",
    "        \"direction\", \n",
    "        when(lower(col(\"from\")) == \"uk\", 1)\n",
    "        .when(lower(col(\"to\"))   == \"uk\", -1)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "\n",
    "flight.printSchema()\n",
    "flight.createOrReplaceTempView(\"flight\")\n",
    "flight.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f1353b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===========================================================(1 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+----------+---------+\n",
      "|passengerId|flightId|from| to|      date|direction|\n",
      "+-----------+--------+----+---+----------+---------+\n",
      "|        382|      18|  jo| uk|2017-01-04|       -1|\n",
      "|        385|      18|  jo| uk|2017-01-04|       -1|\n",
      "|       1001|      18|  jo| uk|2017-01-04|       -1|\n",
      "|       1025|      18|  jo| uk|2017-01-04|       -1|\n",
      "|        378|      18|  jo| uk|2017-01-04|       -1|\n",
      "+-----------+--------+----+---+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flight.where(col(\"direction\") == -1).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1140b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      "\n",
      "+-----------+---------+--------+\n",
      "|passengerId|firstName|lastName|\n",
      "+-----------+---------+--------+\n",
      "|      14751| Napoleon| Gaylene|\n",
      "|       2359| Katherin| Shanell|\n",
      "|       5872|   Stevie|  Steven|\n",
      "|       3346|Margarita|   Gerri|\n",
      "|       3704|    Earle|  Candis|\n",
      "|       1226|    Trent|    Omer|\n",
      "|       2677|    Janee|  Lillia|\n",
      "|        179|     Gita|Chastity|\n",
      "|       9763|   Hilton|Jaquelyn|\n",
      "|      11414|      Leo|Margaret|\n",
      "|       6870|     Tama|     Bok|\n",
      "|       3290|    Logan|    Anya|\n",
      "|      13264|   Lowell|Kathryne|\n",
      "|        455|  Maritza|  Maxima|\n",
      "|      13006|     Yuri|   Joyce|\n",
      "|      10323|  Latasha|  Estell|\n",
      "|       7376|   Kaycee|Kiersten|\n",
      "|      15015|   Curtis| Abraham|\n",
      "|       9217|   Verena|Josefine|\n",
      "|       5183|     Loan| Latonya|\n",
      "+-----------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passenger = spark.read\\\n",
    "    .option(\"compression\", \"none\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"sep\", ',')\\\n",
    "    .option(\"nullValue\", np.nan)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .csv(\"flight/passengers.csv\")\n",
    "\n",
    "passenger.printSchema()\n",
    "passenger.createOrReplaceTempView(\"passenger\")\n",
    "passenger.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebbd70",
   "metadata": {},
   "source": [
    "# Number of flights per month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afec704",
   "metadata": {},
   "source": [
    "## Number of months betwen dates\n",
    "\n",
    "* [pyspark.sql.functions.months_between(date1, date2, roundOff=True)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.months_between.html)\n",
    "\n",
    "> Returns number of months between dates date1 and date2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c1c1426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 07:51:13,855 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|      date| base_date|month_index|\n",
      "+----------+----------+-----------+\n",
      "|2017-01-01|2017-01-01|          0|\n",
      "|2017-11-29|2017-01-01|         10|\n",
      "|2017-12-12|2017-01-01|         11|\n",
      "|2017-12-22|2017-01-01|         11|\n",
      "|2017-12-29|2017-01-01|         11|\n",
      "|2017-01-01|2017-01-01|          0|\n",
      "|2017-01-01|2017-01-01|          0|\n",
      "|2017-01-10|2017-01-01|          0|\n",
      "|2017-02-06|2017-01-01|          1|\n",
      "|2017-03-05|2017-01-01|          2|\n",
      "+----------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    date,\n",
    "    MIN(date) OVER (ORDER BY date) AS base_date,\n",
    "    FLOOR(months_between(date, (SELECT MIN(date) FROM flight))) AS month_index\n",
    "FROM\n",
    "    flight\n",
    "ORDER BY \n",
    "    passengerID, \n",
    "    month_index\n",
    "\"\"\"\n",
    "spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40980a5a",
   "metadata": {},
   "source": [
    "## Month +N\n",
    "\n",
    "* [pyspark.sql.functions.add_months(start, months)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.add_months.html)\n",
    "\n",
    "> Returns the date that is months months after start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a75a302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|plus_n_month|\n",
      "+------------+\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "|     2017-04|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    date_format(\n",
    "        add_months(to_date(\"2017-01-01\", \"yyyy-MM-dd\"), {}),\n",
    "        \"yyyy-MM\"\n",
    "    ) as plus_n_month\n",
    "FROM\n",
    "    flight\n",
    "\"\"\".format(N)\n",
    "spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41be329",
   "metadata": {},
   "source": [
    "## (flight, month) pairs to group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "635f1774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:======================================>                  (8 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|flightId|      date|month_index|\n",
      "+--------+----------+-----------+\n",
      "|       0|2017-01-01|          0|\n",
      "|       1|2017-01-01|          0|\n",
      "|       2|2017-01-01|          0|\n",
      "|       3|2017-01-01|          0|\n",
      "|       4|2017-01-01|          0|\n",
      "|       5|2017-01-02|          0|\n",
      "|       6|2017-01-02|          0|\n",
      "|       7|2017-01-02|          0|\n",
      "|       8|2017-01-02|          0|\n",
      "|       9|2017-01-02|          0|\n",
      "|      10|2017-01-02|          0|\n",
      "|      11|2017-01-03|          0|\n",
      "|      12|2017-01-03|          0|\n",
      "|      13|2017-01-03|          0|\n",
      "|      14|2017-01-03|          0|\n",
      "|      15|2017-01-03|          0|\n",
      "|      16|2017-01-04|          0|\n",
      "|      17|2017-01-04|          0|\n",
      "|      18|2017-01-04|          0|\n",
      "|      19|2017-01-05|          0|\n",
      "+--------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:===================================================>    (11 + 1) / 12]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH month_indexed AS (\n",
    "    SELECT DISTINCT\n",
    "        flightId,\n",
    "        date,\n",
    "        FLOOR(months_between(date, (SELECT MIN(date) FROM flight))) AS month_index\n",
    "    FROM\n",
    "        flight\n",
    ")\n",
    "SELECT \n",
    "    *\n",
    "FROM\n",
    "    month_indexed\n",
    "ORDER BY\n",
    "    date,\n",
    "    flightId\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80ec07",
   "metadata": {},
   "source": [
    "## Flights per month as GROUP BY month_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53ac87e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------------+\n",
      "|month_index|  month|number_of_flights|\n",
      "+-----------+-------+-----------------+\n",
      "|          0|2017-01|               97|\n",
      "|          1|2017-02|               73|\n",
      "|          2|2017-03|               82|\n",
      "|          3|2017-04|               92|\n",
      "|          4|2017-05|               92|\n",
      "|          5|2017-06|               71|\n",
      "|          6|2017-07|               87|\n",
      "|          7|2017-08|               76|\n",
      "|          8|2017-09|               85|\n",
      "|          9|2017-10|               76|\n",
      "|         10|2017-11|               75|\n",
      "|         11|2017-12|               94|\n",
      "+-----------+-------+-----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "* Sort (10)\n",
      "+- Exchange (9)\n",
      "   +- * HashAggregate (8)\n",
      "      +- Exchange (7)\n",
      "         +- * HashAggregate (6)\n",
      "            +- * HashAggregate (5)\n",
      "               +- Exchange (4)\n",
      "                  +- * HashAggregate (3)\n",
      "                     +- * Project (2)\n",
      "                        +- Scan csv  (1)\n",
      "\n",
      "\n",
      "(1) Scan csv \n",
      "Output [2]: [flightId#17, date#20]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv]\n",
      "ReadSchema: struct<flightId:int,date:string>\n",
      "\n",
      "(2) Project [codegen id : 1]\n",
      "Output [2]: [flightId#17, FLOOR(months_between(cast(cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) as timestamp), cast(Subquery scalar-subquery#228, [id=#324] as timestamp), true, Some(Australia/Sydney))) AS month_index#229L]\n",
      "Input [2]: [flightId#17, date#20]\n",
      "\n",
      "(3) HashAggregate [codegen id : 1]\n",
      "Input [2]: [flightId#17, month_index#229L]\n",
      "Keys [2]: [flightId#17, month_index#229L]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [2]: [flightId#17, month_index#229L]\n",
      "\n",
      "(4) Exchange\n",
      "Input [2]: [flightId#17, month_index#229L]\n",
      "Arguments: hashpartitioning(flightId#17, month_index#229L, 12), ENSURE_REQUIREMENTS, [id=#344]\n",
      "\n",
      "(5) HashAggregate [codegen id : 2]\n",
      "Input [2]: [flightId#17, month_index#229L]\n",
      "Keys [2]: [flightId#17, month_index#229L]\n",
      "Functions: []\n",
      "Aggregate Attributes: []\n",
      "Results [2]: [flightId#17, month_index#229L]\n",
      "\n",
      "(6) HashAggregate [codegen id : 2]\n",
      "Input [2]: [flightId#17, month_index#229L]\n",
      "Keys [1]: [month_index#229L]\n",
      "Functions [1]: [partial_count(flightId#17)]\n",
      "Aggregate Attributes [1]: [count#236L]\n",
      "Results [2]: [month_index#229L, count#237L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [2]: [month_index#229L, count#237L]\n",
      "Arguments: hashpartitioning(month_index#229L, 12), ENSURE_REQUIREMENTS, [id=#349]\n",
      "\n",
      "(8) HashAggregate [codegen id : 3]\n",
      "Input [2]: [month_index#229L, count#237L]\n",
      "Keys [1]: [month_index#229L]\n",
      "Functions [1]: [count(flightId#17)]\n",
      "Aggregate Attributes [1]: [count(flightId#17)#232L]\n",
      "Results [3]: [month_index#229L, date_format(cast(add_months(17167, cast(month_index#229L as int)) as timestamp), yyyy-MM, Some(Australia/Sydney)) AS month#226, count(flightId#17)#232L AS number_of_flights#227L]\n",
      "\n",
      "(9) Exchange\n",
      "Input [3]: [month_index#229L, month#226, number_of_flights#227L]\n",
      "Arguments: rangepartitioning(month_index#229L ASC NULLS FIRST, 12), ENSURE_REQUIREMENTS, [id=#353]\n",
      "\n",
      "(10) Sort [codegen id : 4]\n",
      "Input [3]: [month_index#229L, month#226, number_of_flights#227L]\n",
      "Arguments: [month_index#229L ASC NULLS FIRST], true, 0\n",
      "\n",
      "===== Subqueries =====\n",
      "\n",
      "Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery scalar-subquery#228, [id=#324]\n",
      "* HashAggregate (15)\n",
      "+- Exchange (14)\n",
      "   +- * HashAggregate (13)\n",
      "      +- * Project (12)\n",
      "         +- Scan csv  (11)\n",
      "\n",
      "\n",
      "(11) Scan csv \n",
      "Output [1]: [date#20]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv]\n",
      "ReadSchema: struct<date:string>\n",
      "\n",
      "(12) Project [codegen id : 1]\n",
      "Output [1]: [cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#26]\n",
      "Input [1]: [date#20]\n",
      "\n",
      "(13) HashAggregate [codegen id : 1]\n",
      "Input [1]: [date#26]\n",
      "Keys: []\n",
      "Functions [1]: [partial_min(date#26)]\n",
      "Aggregate Attributes [1]: [min#238]\n",
      "Results [1]: [min#239]\n",
      "\n",
      "(14) Exchange\n",
      "Input [1]: [min#239]\n",
      "Arguments: SinglePartition, ENSURE_REQUIREMENTS, [id=#320]\n",
      "\n",
      "(15) HashAggregate [codegen id : 2]\n",
      "Input [1]: [min#239]\n",
      "Keys: []\n",
      "Functions [1]: [min(date#26)]\n",
      "Aggregate Attributes [1]: [min(date#26)#230]\n",
      "Results [1]: [min(date#26)#230 AS min(date)#231]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH month_indexed AS (\n",
    "    SELECT DISTINCT\n",
    "        flightId,\n",
    "        FLOOR(months_between(date, (SELECT MIN(date) FROM flight))) AS month_index\n",
    "    FROM\n",
    "        flight\n",
    ")\n",
    "SELECT \n",
    "    month_index,\n",
    "    date_format(\n",
    "        add_months(to_date(\"2017-01-01\", \"yyyy-MM-dd\"), month_index),\n",
    "        \"yyyy-MM\"\n",
    "    ) as month,\n",
    "    count(flightId) as number_of_flights\n",
    "FROM\n",
    "    month_indexed\n",
    "GROUP BY \n",
    "    month_index\n",
    "ORDER BY\n",
    "    month_index\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "spark.sql(query).explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cb752",
   "metadata": {},
   "source": [
    "---\n",
    "# Frequent Flyers\n",
    "\n",
    "Find the names of the 10 most frequent flyers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553863fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|passengerId|number_of_flights|\n",
      "+-----------+-----------------+\n",
      "|       2068|               32|\n",
      "|       1677|               27|\n",
      "|       4827|               27|\n",
      "|       8961|               26|\n",
      "|       3173|               26|\n",
      "|        917|               25|\n",
      "|       5096|               25|\n",
      "|       8363|               25|\n",
      "|       6084|               25|\n",
      "|       2857|               25|\n",
      "+-----------+-----------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'GlobalLimit 10\n",
      "+- 'LocalLimit 10\n",
      "   +- 'Sort ['number_of_flights DESC NULLS LAST], true\n",
      "      +- 'Aggregate ['f.passengerId], ['f.passengerId, 'COUNT(1) AS number_of_flights#257]\n",
      "         +- 'SubqueryAlias f\n",
      "            +- 'UnresolvedRelation [flight], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "passengerId: int, number_of_flights: bigint\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [number_of_flights#257L DESC NULLS LAST], true\n",
      "      +- Aggregate [passengerId#16], [passengerId#16, count(1) AS number_of_flights#257L]\n",
      "         +- SubqueryAlias f\n",
      "            +- SubqueryAlias flight\n",
      "               +- Project [passengerId#16, flightId#17, from#18, to#19, date#26, CASE WHEN (lower(from#18) = uk) THEN 1 WHEN (lower(to#19) = uk) THEN -1 ELSE 0 END AS direction#32]\n",
      "                  +- Project [passengerId#16, flightId#17, from#18, to#19, to_date('date, Some(yyyy-MM-dd)) AS date#26]\n",
      "                     +- Relation[passengerId#16,flightId#17,from#18,to#19,date#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Sort [number_of_flights#257L DESC NULLS LAST], true\n",
      "      +- Aggregate [passengerId#16], [passengerId#16, count(1) AS number_of_flights#257L]\n",
      "         +- Project [passengerId#16]\n",
      "            +- Relation[passengerId#16,flightId#17,from#18,to#19,date#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=10, orderBy=[number_of_flights#257L DESC NULLS LAST], output=[passengerId#16,number_of_flights#257L])\n",
      "+- *(2) HashAggregate(keys=[passengerId#16], functions=[count(1)], output=[passengerId#16, number_of_flights#257L])\n",
      "   +- Exchange hashpartitioning(passengerId#16, 12), ENSURE_REQUIREMENTS, [id=#409]\n",
      "      +- *(1) HashAggregate(keys=[passengerId#16], functions=[partial_count(1)], output=[passengerId#16, count#263L])\n",
      "         +- FileScan csv [passengerId#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    f.passengerId,\n",
    "    COUNT(*) number_of_flights\n",
    "FROM\n",
    "    flight f\n",
    "GROUP BY \n",
    "    f.passengerId\n",
    "ORDER BY\n",
    "    number_of_flights DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e67b5f6",
   "metadata": {},
   "source": [
    "---\n",
    "# Longest run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2ec3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+---+---------+---+------+---------+\n",
      "|passengerId|from| to|direction|seq|return|countries|\n",
      "+-----------+----+---+---------+---+------+---------+\n",
      "|         16|  cg| uk|       -1|  5|  null|     null|\n",
      "|         16|  uk| cg|        1|  6|  null|     null|\n",
      "|         22|  iq| uk|       -1| 10|  null|     null|\n",
      "|         22|  uk| nl|        1| 11|    15|        4|\n",
      "|         22|  at| uk|       -1| 15|  null|     null|\n",
      "|         22|  uk| bm|        1| 16|  null|     null|\n",
      "|         52|  se| uk|       -1|  6|  null|     null|\n",
      "|         52|  uk| cn|        1|  7|  null|     null|\n",
      "|         53|  ch| uk|       -1|  6|  null|     null|\n",
      "|         53|  uk| se|        1|  7|     8|        1|\n",
      "|         53|  se| uk|       -1|  8|  null|     null|\n",
      "|         53|  uk| tj|        1|  9|    13|        4|\n",
      "|         53|  th| uk|       -1| 13|  null|     null|\n",
      "|         72|  tj| uk|       -1|  9|  null|     null|\n",
      "|         72|  uk| iq|        1| 10|  null|     null|\n",
      "|         82|  iq| uk|       -1|  4|  null|     null|\n",
      "|         82|  uk| se|        1|  5|  null|     null|\n",
      "|        108|  cg| uk|       -1|  5|  null|     null|\n",
      "|        108|  uk| cn|        1|  6|  null|     null|\n",
      "|        127|  il| uk|       -1|  8|  null|     null|\n",
      "+-----------+----+---+---------+---+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "CTE [sequencedRun, closedRun]\n",
      ":  :- 'SubqueryAlias sequencedRun\n",
      ":  :  +- 'Sort ['passengerId ASC NULLS FIRST, 'seq ASC NULLS FIRST], true\n",
      ":  :     +- 'Project ['passengerId, 'date, 'from, 'to, 'direction, 'ROW_NUMBER() windowspecdefinition('passengerId, 'date ASC NULLS FIRST, unspecifiedframe$()) AS seq#910]\n",
      ":  :        +- 'SubqueryAlias f\n",
      ":  :           +- 'UnresolvedRelation [flight], [], false\n",
      ":  +- 'SubqueryAlias closedRun\n",
      ":     +- 'Sort ['passengerId ASC NULLS FIRST, 'seq ASC NULLS FIRST], true\n",
      ":        +- 'Project ['passengerId, 'from, 'to, 'direction, 'seq, CASE WHEN ('direction = 1) THEN 'lead('seq) windowspecdefinition('passengerId, 'seq ASC NULLS FIRST, unspecifiedframe$()) END AS return#912, CASE WHEN ('direction = 1) THEN ('lead('seq) windowspecdefinition('passengerId, 'seq ASC NULLS FIRST, unspecifiedframe$()) - 'seq) END AS countries#913]\n",
      ":           +- 'Filter (NOT ('direction = 0) AND exists#911 [])\n",
      ":              :  +- 'UnresolvedHaving ('count(distinct 'direction) = 2)\n",
      ":              :     +- 'Aggregate ['passengerId], ['passengerId]\n",
      ":              :        +- 'Filter (NOT ('direction = 0) AND ('passengerId = 's.passengerId))\n",
      ":              :           +- 'UnresolvedRelation [sequencedRun], [], false\n",
      ":              +- 'SubqueryAlias s\n",
      ":                 +- 'UnresolvedRelation [sequencedRun], [], false\n",
      "+- 'Sort ['passengerId ASC NULLS FIRST, 'seq ASC NULLS FIRST], true\n",
      "   +- 'Project [*]\n",
      "      +- 'UnresolvedRelation [closedRun], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "passengerId: int, from: string, to: string, direction: int, seq: int, return: int, countries: int\n",
      "Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true\n",
      "+- Project [passengerId#439, from#441, to#442, direction#455, seq#910, return#912, countries#913]\n",
      "   +- SubqueryAlias closedRun\n",
      "      +- Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true\n",
      "         +- Project [passengerId#439, from#441, to#442, direction#455, seq#910, return#912, countries#913]\n",
      "            +- Project [passengerId#439, from#441, to#442, direction#455, seq#910, _we0#920, _we1#921, CASE WHEN (direction#455 = 1) THEN _we0#920 END AS return#912, CASE WHEN (direction#455 = 1) THEN (_we1#921 - seq#910) END AS countries#913]\n",
      "               +- Window [lead(seq#910, 1, null) windowspecdefinition(passengerId#439, seq#910 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we0#920, lead(seq#910, 1, null) windowspecdefinition(passengerId#439, seq#910 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we1#921], [passengerId#439], [seq#910 ASC NULLS FIRST]\n",
      "                  +- Project [passengerId#439, from#441, to#442, direction#455, seq#910]\n",
      "                     +- Filter (NOT (direction#455 = 0) AND exists#911 [passengerId#439])\n",
      "                        :  +- Project [passengerId#439]\n",
      "                        :     +- Filter (count(distinct direction#455)#918L = cast(2 as bigint))\n",
      "                        :        +- Aggregate [passengerId#439], [passengerId#439, count(distinct direction#455) AS count(distinct direction#455)#918L]\n",
      "                        :           +- Filter (NOT (direction#455 = 0) AND (passengerId#439 = outer(passengerId#439)))\n",
      "                        :              +- SubqueryAlias sequencedRun\n",
      "                        :                 +- Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true\n",
      "                        :                    +- Project [passengerId#439, date#449, from#441, to#442, direction#455, seq#910]\n",
      "                        :                       +- Project [passengerId#439, date#449, from#441, to#442, direction#455, seq#910, seq#910]\n",
      "                        :                          +- Window [row_number() windowspecdefinition(passengerId#439, date#449 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS seq#910], [passengerId#439], [date#449 ASC NULLS FIRST]\n",
      "                        :                             +- Project [passengerId#439, date#449, from#441, to#442, direction#455]\n",
      "                        :                                +- SubqueryAlias f\n",
      "                        :                                   +- SubqueryAlias flight\n",
      "                        :                                      +- Project [passengerId#439, flightId#440, from#441, to#442, date#449, CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END AS direction#455]\n",
      "                        :                                         +- Project [passengerId#439, flightId#440, from#441, to#442, to_date('date, Some(yyyy-MM-dd)) AS date#449]\n",
      "                        :                                            +- Relation[passengerId#439,flightId#440,from#441,to#442,date#443] csv\n",
      "                        +- SubqueryAlias s\n",
      "                           +- SubqueryAlias sequencedRun\n",
      "                              +- Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true\n",
      "                                 +- Project [passengerId#439, date#449, from#441, to#442, direction#455, seq#910]\n",
      "                                    +- Project [passengerId#439, date#449, from#441, to#442, direction#455, seq#910, seq#910]\n",
      "                                       +- Window [row_number() windowspecdefinition(passengerId#439, date#449 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS seq#910], [passengerId#439], [date#449 ASC NULLS FIRST]\n",
      "                                          +- Project [passengerId#439, date#449, from#441, to#442, direction#455]\n",
      "                                             +- SubqueryAlias f\n",
      "                                                +- SubqueryAlias flight\n",
      "                                                   +- Project [passengerId#439, flightId#440, from#441, to#442, date#449, CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END AS direction#455]\n",
      "                                                      +- Project [passengerId#439, flightId#440, from#441, to#442, to_date('date, Some(yyyy-MM-dd)) AS date#449]\n",
      "                                                         +- Relation[passengerId#439,flightId#440,from#441,to#442,date#443] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true\n",
      "+- Project [passengerId#439, from#441, to#442, direction#455, seq#910, CASE WHEN (direction#455 = 1) THEN _we0#920 END AS return#912, CASE WHEN (direction#455 = 1) THEN (_we1#921 - seq#910) END AS countries#913]\n",
      "   +- Window [lead(seq#910, 1, null) windowspecdefinition(passengerId#439, seq#910 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we0#920, lead(seq#910, 1, null) windowspecdefinition(passengerId#439, seq#910 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we1#921], [passengerId#439], [seq#910 ASC NULLS FIRST]\n",
      "      +- Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true\n",
      "         +- Project [passengerId#439, from#441, to#442, direction#455, seq#910]\n",
      "            +- Filter NOT (direction#455 = 0)\n",
      "               +- Window [row_number() windowspecdefinition(passengerId#439, date#449 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS seq#910], [passengerId#439], [date#449 ASC NULLS FIRST]\n",
      "                  +- Project [passengerId#439, cast(gettimestamp(date#443, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#449, from#441, to#442, CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END AS direction#455]\n",
      "                     +- Join LeftSemi, (passengerId#439#973 = passengerId#439)\n",
      "                        :- Project [passengerId#439, from#441, to#442, date#443]\n",
      "                        :  +- Relation[passengerId#439,flightId#440,from#441,to#442,date#443] csv\n",
      "                        +- Project [passengerId#439 AS passengerId#439#973]\n",
      "                           +- Filter (count(distinct direction#455)#918L = 2)\n",
      "                              +- Aggregate [passengerId#439], [passengerId#439, count(distinct direction#455) AS count(distinct direction#455)#918L]\n",
      "                                 +- Project [passengerId#439, CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END AS direction#455]\n",
      "                                    +- Filter NOT (CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END = 0)\n",
      "                                       +- Relation[passengerId#439,flightId#440,from#441,to#442,date#443] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(10) Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST, 12), ENSURE_REQUIREMENTS, [id=#2609]\n",
      "   +- *(9) Project [passengerId#439, from#441, to#442, direction#455, seq#910, CASE WHEN (direction#455 = 1) THEN _we0#920 END AS return#912, CASE WHEN (direction#455 = 1) THEN (_we1#921 - seq#910) END AS countries#913]\n",
      "      +- Window [lead(seq#910, 1, null) windowspecdefinition(passengerId#439, seq#910 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we0#920, lead(seq#910, 1, null) windowspecdefinition(passengerId#439, seq#910 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS _we1#921], [passengerId#439], [seq#910 ASC NULLS FIRST]\n",
      "         +- *(8) Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange hashpartitioning(passengerId#439, 12), ENSURE_REQUIREMENTS, [id=#2601]\n",
      "               +- *(7) Sort [passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST], true, 0\n",
      "                  +- Exchange rangepartitioning(passengerId#439 ASC NULLS FIRST, seq#910 ASC NULLS FIRST, 12), ENSURE_REQUIREMENTS, [id=#2597]\n",
      "                     +- *(6) Project [passengerId#439, from#441, to#442, direction#455, seq#910]\n",
      "                        +- *(6) Filter NOT (direction#455 = 0)\n",
      "                           +- Window [row_number() windowspecdefinition(passengerId#439, date#449 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS seq#910], [passengerId#439], [date#449 ASC NULLS FIRST]\n",
      "                              +- *(5) Sort [passengerId#439 ASC NULLS FIRST, date#449 ASC NULLS FIRST], false, 0\n",
      "                                 +- Exchange hashpartitioning(passengerId#439, 12), ENSURE_REQUIREMENTS, [id=#2588]\n",
      "                                    +- *(4) Project [passengerId#439, cast(gettimestamp(date#443, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#449, from#441, to#442, CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END AS direction#455]\n",
      "                                       +- *(4) BroadcastHashJoin [passengerId#439], [passengerId#439#973], LeftSemi, BuildRight, false\n",
      "                                          :- FileScan csv [passengerId#439,from#441,to#442,date#443] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,from:string,to:string,date:string>\n",
      "                                          +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#2583]\n",
      "                                             +- *(3) Project [passengerId#439 AS passengerId#439#973]\n",
      "                                                +- *(3) Filter (count(distinct direction#455)#918L = 2)\n",
      "                                                   +- *(3) HashAggregate(keys=[passengerId#439], functions=[count(distinct direction#455)], output=[passengerId#439, count(distinct direction#455)#918L])\n",
      "                                                      +- Exchange hashpartitioning(passengerId#439, 12), ENSURE_REQUIREMENTS, [id=#2577]\n",
      "                                                         +- *(2) HashAggregate(keys=[passengerId#439], functions=[partial_count(distinct direction#455)], output=[passengerId#439, count#953L])\n",
      "                                                            +- *(2) HashAggregate(keys=[passengerId#439, direction#455], functions=[], output=[passengerId#439, direction#455])\n",
      "                                                               +- Exchange hashpartitioning(passengerId#439, direction#455, 12), ENSURE_REQUIREMENTS, [id=#2572]\n",
      "                                                                  +- *(1) HashAggregate(keys=[passengerId#439, direction#455], functions=[], output=[passengerId#439, direction#455])\n",
      "                                                                     +- *(1) Project [passengerId#439, CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END AS direction#455]\n",
      "                                                                        +- *(1) Filter NOT (CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END = 0)\n",
      "                                                                           +- FileScan csv [passengerId#439,from#441,to#442] Batched: false, DataFilters: [NOT (CASE WHEN (lower(from#441) = uk) THEN 1 WHEN (lower(to#442) = uk) THEN -1 ELSE 0 END = 0)], Format: CSV, Location: InMemoryFileIndex[hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,from:string,to:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH sequencedRun AS (\n",
    "    SELECT\n",
    "        passengerId,\n",
    "        date,\n",
    "        from, \n",
    "        to,\n",
    "        direction,\n",
    "        ROW_NUMBER() OVER (PARTITION BY passengerId ORDER BY date) AS seq\n",
    "    FROM\n",
    "        flight f\n",
    "    ORDER BY \n",
    "        passengerId,\n",
    "        seq\n",
    "),\n",
    "closedRun AS (\n",
    "    SELECT \n",
    "        passengerId, \n",
    "        from, to, \n",
    "        direction, \n",
    "        seq,\n",
    "        -------------------------------------------------------------------------------- \n",
    "        -- For a departure flight, take the seq of the return flight, if there is.\n",
    "        -------------------------------------------------------------------------------- \n",
    "        CASE \n",
    "            WHEN direction == 1\n",
    "            THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq)\n",
    "        END AS return,\n",
    "        -------------------------------------------------------------------------------- \n",
    "        -- For a departure flight, count the visiting countries, if returned.\n",
    "        -------------------------------------------------------------------------------- \n",
    "        CASE \n",
    "            WHEN direction == 1\n",
    "            THEN lead(seq) OVER (PARTITION BY passengerId ORDER BY seq) - seq\n",
    "        END AS countries\n",
    "    FROM sequencedRun s\n",
    "    WHERE \n",
    "        -------------------------------------------------------------------------------- \n",
    "        -- Remove those without UK\n",
    "        -------------------------------------------------------------------------------- \n",
    "        direction != 0\n",
    "        -------------------------------------------------------------------------------- \n",
    "        -- Select passengers having both depart (+1) and return (-1), which is \n",
    "        -- distinct direction count is 2.\n",
    "        -------------------------------------------------------------------------------- \n",
    "        AND EXISTS (  \n",
    "            SELECT passengerId\n",
    "            FROM\n",
    "                sequencedRun\n",
    "            WHERE \n",
    "                direction != 0 AND\n",
    "                passengerId == s.passengerId\n",
    "            GROUP BY\n",
    "                passengerId\n",
    "            Having count(DISTINCT direction) == 2\n",
    "        )\n",
    "    ORDER BY \n",
    "        passengerId, seq\n",
    ")\n",
    "SELECT * FROM closedRun \n",
    "ORDER BY \n",
    "    passengerId, seq\n",
    "\"\"\"\n",
    "\n",
    "closed = spark.sql(query)\n",
    "closed.show()\n",
    "closed.explain(True)\n",
    "closed.createOrReplaceTempView(\"closedRun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c02e7709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|Passenger ID|Longest Run|\n",
      "+------------+-----------+\n",
      "|        2975|         16|\n",
      "|        2939|         15|\n",
      "|        3573|         15|\n",
      "|         760|         15|\n",
      "|        8562|         15|\n",
      "|        2982|         14|\n",
      "|        8590|         14|\n",
      "|        2926|         14|\n",
      "|        2935|         14|\n",
      "|        3600|         14|\n",
      "|         755|         14|\n",
      "|         917|         13|\n",
      "|        8363|         13|\n",
      "|        7643|         13|\n",
      "|        3565|         12|\n",
      "|        1982|         12|\n",
      "|        8961|         12|\n",
      "|        1053|         12|\n",
      "|        3466|         11|\n",
      "|        2967|         11|\n",
      "+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    passengerId as `Passenger ID`,\n",
    "    max(countries) as `Longest Run`\n",
    "FROM closedRun\n",
    "WHERE \n",
    "    countries IS NOT NULL\n",
    "GROUP BY \n",
    "    passengerId\n",
    "ORDER BY \n",
    "    max(countries) DESC\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1202"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d5b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
