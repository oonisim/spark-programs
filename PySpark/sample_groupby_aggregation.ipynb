{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# PySpark DatFrame GroupBy & Aggregate\n",
    "\n",
    "* [Aggregations with Spark (groupBy, cube, rollup)](https://mungingdata.com/apache-spark/aggregations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317443d",
   "metadata": {},
   "source": [
    "---\n",
    "# HDFS permission\n",
    "\n",
    "For a non-spark user to be able to submit a job, login to the HDFS node as the hadoop user to run:\n",
    "\n",
    "```\n",
    "hadoop fs -mkdir /user/${USERNAME}\n",
    "hadoop fs -chown ${USERNAME} /user/${USERNAME}\n",
    "hadoop fs -chmod g+w /user/${USERNAME}\n",
    "```\n",
    "\n",
    "Otherwise an error:\n",
    "```\n",
    "21/08/15 21:15:28 ERROR SparkContext: Error initializing SparkContext.\n",
    "org.apache.hadoop.security.AccessControlException: Permission denied: user=${USERNAME}, access=WRITE, inode=\"/user\":hadoop:hadoop:drwxrwxr-x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345d990",
   "metadata": {},
   "source": [
    "---\n",
    "# Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d50fc",
   "metadata": {},
   "source": [
    "## HADOOP_CONF_DIR\n",
    "\n",
    "Copy the **HADOOP_CONF_DIR** from the Hadoop/YARN master node and set the ```HADOOP_CONF_DIR``` environment variable locally to point to the directory.\n",
    "\n",
    "* [Launching Spark on YARN\n",
    "](http://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn)\n",
    "\n",
    "> Ensure that **HADOOP_CONF_DIR** or **YARN_CONF_DIR** points to the directory which contains the (client side) configuration files for the Hadoop cluster. These configs are used to write to HDFS and connect to the YARN ResourceManager. The configuration contained in this directory will be distributed to the YARN cluster so that all containers used by the application use the same configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/15 16:45:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/09/15 16:45:45 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201eac20",
   "metadata": {},
   "source": [
    "# PySpark DataFrame\n",
    "\n",
    "* [SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html)\n",
    "\n",
    "> Parameters\n",
    "> * data: RDD or iterable\n",
    "an RDD of any kind of SQL data representation(e.g. Row, tuple, int, boolean, etc.), or list, or pandas.DataFrame.\n",
    "> * schema: pyspark.sql.types.DataType, str or list, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30caa51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  name|goal|\n",
      "+------+----+\n",
      "| messi|   1|\n",
      "|ronald|   2|\n",
      "| messi|   3|\n",
      "|ronald|   4|\n",
      "| messi|   5|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    data=[\n",
    "        (\"messi\", 1), \n",
    "        (\"ronald\", 2), \n",
    "        (\"messi\", 3), \n",
    "        (\"ronald\", 4),\n",
    "        (\"messi\", 5), \n",
    "    ], \n",
    "    schema=[\n",
    "        \"name\", \"goal\"\n",
    "    ]\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5ddff",
   "metadata": {},
   "source": [
    "# GroupBy Aggregation\n",
    "\n",
    "* [Grouping Data](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html#Grouping-Data)\n",
    "\n",
    "> PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame.\n",
    "\n",
    "* [GroupedData.agg(*exprs)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html)\n",
    "\n",
    "> If exprs is a single dict mapping from string to string, then the key is the column to perform aggregation on, and the value is the aggregate function. Alternatively, exprs can also be a list of aggregate Column expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aff7fb",
   "metadata": {},
   "source": [
    "## Builtin functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223e4460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abs',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add_months',\n",
       " 'aggregate',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'array_distinct',\n",
       " 'array_except',\n",
       " 'array_intersect',\n",
       " 'array_join',\n",
       " 'array_max',\n",
       " 'array_min',\n",
       " 'array_position',\n",
       " 'array_remove',\n",
       " 'array_repeat',\n",
       " 'array_sort',\n",
       " 'array_union',\n",
       " 'arrays_overlap',\n",
       " 'arrays_zip',\n",
       " 'asc',\n",
       " 'asc_nulls_first',\n",
       " 'asc_nulls_last',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_true',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'bucket',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'date_trunc',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofweek',\n",
       " 'dayofyear',\n",
       " 'days',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'desc_nulls_first',\n",
       " 'desc_nulls_last',\n",
       " 'element_at',\n",
       " 'encode',\n",
       " 'exists',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'explode_outer',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatten',\n",
       " 'floor',\n",
       " 'forall',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_csv',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'hypot',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map_concat',\n",
       " 'map_entries',\n",
       " 'map_filter',\n",
       " 'map_from_arrays',\n",
       " 'map_from_entries',\n",
       " 'map_keys',\n",
       " 'map_values',\n",
       " 'map_zip_with',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'nth_value',\n",
       " 'ntile',\n",
       " 'overlay',\n",
       " 'pandas_udf',\n",
       " 'percent_rank',\n",
       " 'percentile_approx',\n",
       " 'posexplode',\n",
       " 'posexplode_outer',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'raise_error',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'schema_of_csv',\n",
       " 'schema_of_json',\n",
       " 'second',\n",
       " 'sequence',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'shuffle',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'slice',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'timestamp_seconds',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_csv',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_str',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'transform',\n",
       " 'transform_keys',\n",
       " 'transform_values',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'warnings',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'xxhash64',\n",
       " 'year',\n",
       " 'years',\n",
       " 'zip_with']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "import re\n",
    "[f for f in dir(F) if re.search(r'^[a-z].+', f) is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498ca3d4",
   "metadata": {},
   "source": [
    "\n",
    "## Causion: \n",
    "\n",
    "DO NOT use Python built-in aggregation funtions e.g. sum. It will cause the error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feed19e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26185/518621705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"goal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <--- Using Python built-in sum.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "df.groupby(\"name\").agg(sum(\"goal\"))  # <--- Using Python built-in sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4c409",
   "metadata": {},
   "source": [
    "## Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7442412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  name|sum(goal)|\n",
      "+------+---------+\n",
      "|ronald|        6|\n",
      "| messi|        9|\n",
      "+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=========================================>              (56 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  name|sum(goal)|\n",
      "+------+---------+\n",
      "|ronald|        6|\n",
      "| messi|        9|\n",
      "+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:===================================================>    (69 + 2) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby(\"name\").agg(F.sum(\"goal\")).show()\n",
    "df.groupby(\"name\").sum(\"goal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c506b96",
   "metadata": {},
   "source": [
    "## Renaming column\n",
    "\n",
    "* [Column alias after groupBy in pyspark](https://stackoverflow.com/questions/33516490/column-alias-after-groupby-in-pyspark)\n",
    "* [Spark - DataFrame - Select](https://montan.atlassian.net/wiki/spaces/~masayukionishi/pages/692158548/Spark+-+DataFrame+-+Select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a171c405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|scores|\n",
      "+------+\n",
      "|     6|\n",
      "|     9|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:===========================================>            (58 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|scores|\n",
      "+------+------+\n",
      "|ronald|     6|\n",
      "| messi|     9|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select function & alias\n",
    "df.groupby(\"name\").sum(\"goal\").select(F.col(\"sum(goal)\").alias(\"scores\")).show()\n",
    "\n",
    "# Aggregation function & alias\n",
    "df.groupby(\"name\").agg(\n",
    "    F.sum(\"goal\").alias(\"scores\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6a364",
   "metadata": {},
   "source": [
    "# Aggregate multiple columns\n",
    "\n",
    "List multiple column aggregations in the ```agg``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2bd87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+------------------+--------+\n",
      "|  name|scores|avearge|                sd|hattrick|\n",
      "+------+------+-------+------------------+--------+\n",
      "|ronald|     6|    3.0|1.4142135623730951|       1|\n",
      "| messi|     9|    3.0|               2.0|       2|\n",
      "+------+------+-------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"name\").agg(\n",
    "    F.sum(\"goal\").alias(\"scores\"),\n",
    "    F.avg(\"goal\").alias(\"avearge\"),\n",
    "    F.stddev(\"goal\").alias(\"sd\"),\n",
    "    F.count(F.when(F.col(\"goal\") >= 3, 1)).alias(\"hattrick\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
