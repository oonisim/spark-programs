{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# PySpark SparkSQL Analytics Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fc02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from datetime import (\n",
    "    datetime,\n",
    "    date\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ec8dc",
   "metadata": {},
   "source": [
    "## PySpark packages\n",
    "\n",
    "Execute after the PYTHONPATH setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    avg,\n",
    "    stddev,\n",
    "    isnan,\n",
    "    to_date,\n",
    "    to_timestamp,\n",
    "    hour,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b9883",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Student schema from [Oracle SQL by Example](https://learning.oreilly.com/library/view/oracle-sql-by/9780137047345/ch06.html) located in ```./data/student```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1a5c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  student.zip\n",
      "  inflating: COURSE_DATA_TABLE.csv   \n",
      "  inflating: COURSE_REVENUE_DATA_TABLE.csv  \n",
      "  inflating: EMPLOYEE_DATA_TABLE.csv  \n",
      "  inflating: ENROLLMENT_DATA_TABLE.csv  \n",
      "  inflating: GRADE_DATA_TABLE.csv    \n",
      "  inflating: GRADE_TYPE_DATA_TABLE.csv  \n",
      "  inflating: INSTRUCTOR_DATA_TABLE.csv  \n",
      "  inflating: SECTION_DATA_TABLE.csv  \n",
      "  inflating: SECTION_HISTORY_DATA_TABLE.csv  \n",
      "  inflating: STUDENT_DATA_TABLE.csv  \n",
      "  inflating: ZIPCODE_DATA_TABLE.csv  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ./data/student\n",
    "unzip -o student.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58ed336",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/student/\n",
    "hdfs dfs -mkdir -p student\n",
    "hdfs dfs -put -f *.csv student\n",
    "\n",
    "rm -rf *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 17:44:28,578 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-20 17:44:31,714 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc80e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "NUM_PARTITIONS = 3\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201eac20",
   "metadata": {},
   "source": [
    "# Stduent schema CSV \n",
    "\n",
    "* [SparkSQL CSV Files](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)\n",
    "\n",
    "> Spark SQL provides spark.read().csv(\"file_name\") to read a file or directory of files in CSV format into Spark DataFrame, and dataframe.write().csv(\"path\") to write to a CSV file. Function option() can be used to customize the behavior of reading or writing.\n",
    "\n",
    "[SparkSession.read()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.html#read--) returns [DataFrameReader](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html) instance which has [option](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-boolean-) method by which we can specify CSV options.\n",
    "\n",
    "The options are listed in [Data Source Option](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b687a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COURSE_NO: integer (nullable = true)\n",
      " |-- DESCRIPTION: string (nullable = true)\n",
      " |-- COST: integer (nullable = true)\n",
      " |-- PREREQUISITE: integer (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+---------+--------------------+----+------------+----------+------------+-----------+-------------+\n",
      "|COURSE_NO|         DESCRIPTION|COST|PREREQUISITE|CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+---------+--------------------+----+------------+----------+------------+-----------+-------------+\n",
      "|       10| Technology Concepts|1195|        null|  DSCHERER|  2007-03-29|   ARISCHER|   2007-04-05|\n",
      "|       20|Intro to Informat...|1195|        null|  DSCHERER|  2007-03-29|   ARISCHER|   2007-04-05|\n",
      "|       25|Intro to Programming|1195|         140|  DSCHERER|  2007-03-29|   ARISCHER|   2007-04-05|\n",
      "+---------+--------------------+----+------------+----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/COURSE_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"CREATED_DATE\", to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\", to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "course.printSchema()\n",
    "course.createOrReplaceTempView(\"course\")\n",
    "course.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "963ed672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SECTION_ID: integer (nullable = true)\n",
      " |-- COURSE_NO: integer (nullable = true)\n",
      " |-- SECTION_NO: integer (nullable = true)\n",
      " |-- START_DATE_TIME: date (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- INSTRUCTOR_ID: integer (nullable = true)\n",
      " |-- CAPACITY: integer (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+----------+---------+----------+---------------+--------+-------------+--------+----------+------------+-----------+-------------+\n",
      "|SECTION_ID|COURSE_NO|SECTION_NO|START_DATE_TIME|LOCATION|INSTRUCTOR_ID|CAPACITY|CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+----------+---------+----------+---------------+--------+-------------+--------+----------+------------+-----------+-------------+\n",
      "|        79|      350|         3|     2007-04-14|    L509|          107|      25|  CBRENNAN|  2007-01-02|   CBRENNAN|   2007-01-02|\n",
      "|        80|       10|         2|     2007-04-24|    L214|          102|      15|  CBRENNAN|  2007-01-02|   CBRENNAN|   2007-01-02|\n",
      "|        81|       20|         2|     2007-07-24|    L210|          103|      15|  CBRENNAN|  2007-01-02|   CBRENNAN|   2007-01-02|\n",
      "+----------+---------+----------+---------------+--------+-------------+--------+----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "section = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/SECTION_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"START_DATE_TIME\", to_date(col('START_DATE_TIME'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"CREATED_DATE\", to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\", to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "section.printSchema()\n",
    "section.createOrReplaceTempView(\"section\")\n",
    "section.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eecc4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INSTRUCTOR_ID: integer (nullable = true)\n",
      " |-- SALUTATION: string (nullable = true)\n",
      " |-- FIRST_NAME: string (nullable = true)\n",
      " |-- LAST_NAME: string (nullable = true)\n",
      " |-- STREET_ADDRESS: string (nullable = true)\n",
      " |-- ZIP: integer (nullable = true)\n",
      " |-- PHONE: integer (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+-------------+----------+----------+---------+--------------+-----+----------+----------+------------+-----------+-------------+\n",
      "|INSTRUCTOR_ID|SALUTATION|FIRST_NAME|LAST_NAME|STREET_ADDRESS|  ZIP|     PHONE|CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+-------------+----------+----------+---------+--------------+-----+----------+----------+------------+-----------+-------------+\n",
      "|          101|        Mr|   Fernand|    Hanks| 100 East 87th|10015|2125551212|  ESILVEST|  2007-01-02|   ESILVEST|   2007-01-02|\n",
      "|          102|        Mr|       Tom|   Wojick|518 West 120th|10025|2125551212|  ESILVEST|  2007-01-02|   ESILVEST|   2007-01-02|\n",
      "|          103|        Ms|      Nina|  Schorin|210 West 101st|10025|2125551212|  ESILVEST|  2007-01-02|   ESILVEST|   2007-01-02|\n",
      "+-------------+----------+----------+---------+--------------+-----+----------+----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instructor = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/INSTRUCTOR_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"CREATED_DATE\", to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\", to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "instructor.printSchema()\n",
    "instructor.createOrReplaceTempView(\"instructor\")\n",
    "instructor.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82e94d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STUDENT_ID: integer (nullable = true)\n",
      " |-- SALUTATION: string (nullable = true)\n",
      " |-- FIRST_NAME: string (nullable = true)\n",
      " |-- LAST_NAME: string (nullable = true)\n",
      " |-- STREET_ADDRESS: string (nullable = true)\n",
      " |-- ZIP: integer (nullable = true)\n",
      " |-- PHONE: string (nullable = true)\n",
      " |-- EMPLOYER: string (nullable = true)\n",
      " |-- REGISTRATION_DATE: date (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+----------+----------+----------+---------+------------------+-----+------------+---------------+-----------------+-----------+------------+-----------+-------------+\n",
      "|STUDENT_ID|SALUTATION|FIRST_NAME|LAST_NAME|    STREET_ADDRESS|  ZIP|       PHONE|       EMPLOYER|REGISTRATION_DATE| CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+----------+----------+----------+---------+------------------+-----+------------+---------------+-----------------+-----------+------------+-----------+-------------+\n",
      "|       167|       Mr.|       Jim|     Joas|   53-33 192nd St.|11365|718-555-5555|     Gaum, Inc.|       2007-02-02|BROSENZWEIG|  2007-02-02|   BROSENZW|   2007-02-02|\n",
      "|       168|       Ms.|     Sally|     Naso|      812 79th St.| 7047|201-555-5555|Motors National|       2007-02-02|BROSENZWEIG|  2007-02-02|   BROSENZW|   2007-02-02|\n",
      "|       169|       Mr.|    Frantz|   McLean|23-08 Newtown Ave.|11102|718-555-5555|Guenther Miller|       2007-02-02|BROSENZWEIG|  2007-02-02|   BROSENZW|   2007-02-02|\n",
      "+----------+----------+----------+---------+------------------+-----+------------+---------------+-----------------+-----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/STUDENT_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"REGISTRATION_DATE\", to_date(col('REGISTRATION_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"CREATED_DATE\",      to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\",     to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "student.cache()\n",
    "student.printSchema()\n",
    "student.createOrReplaceTempView(\"student\")\n",
    "student.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc1b01fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STUDENT_ID: integer (nullable = true)\n",
      " |-- SECTION_ID: integer (nullable = true)\n",
      " |-- ENROLL_DATE: string (nullable = true)\n",
      " |-- FINAL_GRADE: integer (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+----------+----------+-----------+-----------+----------+------------+-----------+-------------+\n",
      "|STUDENT_ID|SECTION_ID|ENROLL_DATE|FINAL_GRADE|CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+----------+----------+-----------+-----------+----------+------------+-----------+-------------+\n",
      "|       215|       146|  13-FEB-07|       null|  DSCHERER|  2007-12-14|   BROSENZW|   2007-01-05|\n",
      "|       215|       156|  13-FEB-07|       null|  DSCHERER|  2007-12-14|   BROSENZW|   2007-01-05|\n",
      "|       216|       154|  13-FEB-07|       null|  DSCHERER|  2007-12-14|   BROSENZW|   2007-01-05|\n",
      "+----------+----------+-----------+-----------+----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 63:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "enrollment = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/ENROLLMENT_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"CREATED_DATE\", to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\", to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "enrollment.cache()\n",
    "enrollment.printSchema()\n",
    "enrollment.createOrReplaceTempView(\"enrollment\")\n",
    "enrollment.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85e1ebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STUDENT_ID: integer (nullable = true)\n",
      " |-- SECTION_ID: integer (nullable = true)\n",
      " |-- GRADE_TYPE_CODE: string (nullable = true)\n",
      " |-- GRADE_CODE_OCCURRENCE: integer (nullable = true)\n",
      " |-- NUMERIC_GRADE: integer (nullable = true)\n",
      " |-- COMMENTS: string (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+----------+----------+---------------+---------------------+-------------+--------+----------+------------+-----------+-------------+\n",
      "|STUDENT_ID|SECTION_ID|GRADE_TYPE_CODE|GRADE_CODE_OCCURRENCE|NUMERIC_GRADE|COMMENTS|CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+----------+----------+---------------+---------------------+-------------+--------+----------+------------+-----------+-------------+\n",
      "|       111|       133|             PA|                    6|           80|    null|  CBRENNAN|  2007-02-11|     JAYCAF|   2007-02-11|\n",
      "|       111|       133|             PA|                    7|           70|    null|  CBRENNAN|  2007-02-11|     JAYCAF|   2007-02-11|\n",
      "|       111|       133|             PA|                    8|           70|    null|  CBRENNAN|  2007-02-11|     JAYCAF|   2007-02-11|\n",
      "+----------+----------+---------------+---------------------+-------------+--------+----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grade = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/GRADE_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"CREATED_DATE\", to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\", to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "grade.printSchema()\n",
    "grade.createOrReplaceTempView(\"grade\")\n",
    "grade.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fbc5d",
   "metadata": {},
   "source": [
    "Need to make sure 31-DEC-98 is converted to 1998-12-31, not 2098-12-31.\n",
    "\n",
    "* [spark to_date function - how to convert 31-DEC-98 to 1998-12-31 not 2098-12-31](https://stackoverflow.com/questions/71182230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "039a2116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GRADE_TYPE_CODE: string (nullable = true)\n",
      " |-- DESCRIPTION: string (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+---------------+-----------+----------+------------+-----------+-------------+\n",
      "|GRADE_TYPE_CODE|DESCRIPTION|CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+---------------+-----------+----------+------------+-----------+-------------+\n",
      "|             FI|      Final|  MCAFFREY|  1998-12-31|   MCAFFREY|   1998-12-31|\n",
      "|             HM|   Homework|  MCAFFREY|  1998-12-31|   MCAFFREY|   1998-12-31|\n",
      "|             MT|    Midterm|  MCAFFREY|  1998-12-31|   MCAFFREY|   1998-12-31|\n",
      "+---------------+-----------+----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grade_type = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/GRADE_TYPE_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"CREATED_DATE\", to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\", to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "grade_type.printSchema()\n",
    "grade_type.createOrReplaceTempView(\"grade_type\")\n",
    "grade_type.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81fcc5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ZIP: integer (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- CREATED_BY: string (nullable = true)\n",
      " |-- CREATED_DATE: date (nullable = true)\n",
      " |-- MODIFIED_BY: string (nullable = true)\n",
      " |-- MODIFIED_DATE: date (nullable = true)\n",
      "\n",
      "+-----+----------------+-----+----------+------------+-----------+-------------+\n",
      "|  ZIP|            CITY|STATE|CREATED_BY|CREATED_DATE|MODIFIED_BY|MODIFIED_DATE|\n",
      "+-----+----------------+-----+----------+------------+-----------+-------------+\n",
      "|11101|Long Island City|   NY|  AMORRISO|  2007-08-03|   AMORRISO|   2007-11-24|\n",
      "|11102|         Astoria|   NY|  AMORRISO|  2007-08-03|   AMORRISO|   2007-11-24|\n",
      "|11103|         Astoria|   NY|  AMORRISO|  2007-08-03|   AMORRISO|   2007-11-24|\n",
      "+-----+----------------+-----+----------+------------+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zipcode = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/ZIPCODE_DATA_TABLE.csv\")\\\n",
    "    .withColumn(\"CREATED_DATE\", to_date(col('CREATED_DATE'), \"dd-MMM-yy\"))\\\n",
    "    .withColumn(\"MODIFIED_DATE\", to_date(col('MODIFIED_DATE'), \"dd-MMM-yy\"))\n",
    "\n",
    "zipcode.printSchema()\n",
    "zipcode.createOrReplaceTempView(\"zipcode\")\n",
    "zipcode.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ddaeb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EMPLOYEE_ID: integer (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- SALARY: integer (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      "\n",
      "+-----------+------+------+---------+\n",
      "|EMPLOYEE_ID|  NAME|SALARY|    TITLE|\n",
      "+-----------+------+------+---------+\n",
      "|          1|  John|  1000|  Analyst|\n",
      "|          2|  Mary|  2000|  Manager|\n",
      "|          3|Stella|  5000|President|\n",
      "+-----------+------+------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"nullValue\", \"\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"student/EMPLOYEE_DATA_TABLE.csv\")\n",
    "\n",
    "employee.printSchema()\n",
    "employee.createOrReplaceTempView(\"employee\")\n",
    "employee.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b4e10",
   "metadata": {},
   "source": [
    "---\n",
    "# Pivot\n",
    "\n",
    "Transform from Long/Stack Format to Wide/Unstack Format. \n",
    "\n",
    "1. Find the ID (keys) that identifyes an class instance that will be a row in Wide Format.\n",
    "2. Create (ID, attribute, value) Long Format.\n",
    "3. ```PIVOT (FOR attribute IN (<attributes>))```\n",
    "\n",
    "\n",
    "* [PIVOT Clause](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-pivot.html)\n",
    "\n",
    "## Syntax\n",
    "```\n",
    "PIVOT (\n",
    "    aggregate_expression(value) [ AS aggregate_expression_alias ]\n",
    "    [ , ... ]   /* Continue with , for multipel aggregations */\n",
    "    FOR attributes IN ( \n",
    "        attribute_list \n",
    "    )\n",
    ") \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec54824",
   "metadata": {},
   "source": [
    "Find the number of classes starting at each location.\n",
    "\n",
    "1. Find the ```location``` as **ID**.\n",
    "2. Create (ID, attribute, value) Long Format table.\n",
    "3. PIVOT (Long to Wide Transformation) that generates rows where each row is an class instance with attributes.\n",
    "\n",
    "\n",
    "<img src=\"./image/pivot.jpg\" align=\"left\" width=600/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20add6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+\n",
      "|location|day_text|day_num|\n",
      "+--------+--------+-------+\n",
      "|    H310|     Tue|      3|\n",
      "|    L206|     Tue|      3|\n",
      "|    L210|     Tue|      3|\n",
      "|    L210|     Mon|      2|\n",
      "|    L210|     Tue|      3|\n",
      "|    L210|     Fri|      6|\n",
      "|    L210|     Sat|      7|\n",
      "|    L210|     Tue|      3|\n",
      "|    L210|     Sat|      7|\n",
      "|    L210|     Wed|      4|\n",
      "|    L210|     Mon|      2|\n",
      "|    L210|     Sun|      1|\n",
      "|    L211|     Thu|      5|\n",
      "|    L211|     Tue|      3|\n",
      "|    L211|     Sat|      7|\n",
      "|    L214|     Sat|      7|\n",
      "|    L214|     Fri|      6|\n",
      "|    L214|     Thu|      5|\n",
      "|    L214|     Sun|      1|\n",
      "|    L214|     Thu|      5|\n",
      "+--------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    location,\n",
    "    date_format(start_date_time, \"EEE\") AS day_text,\n",
    "    dayofweek(start_date_time) AS day_num\n",
    "FROM\n",
    "    section\n",
    "ORDER BY location\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3795d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+----+----+----+----+----+\n",
      "|location| MON| TUE| WED| THU| FRI| SAT| SUN|\n",
      "+--------+----+----+----+----+----+----+----+\n",
      "|    H310|null|   1|null|null|null|null|null|\n",
      "|    L206|null|   1|null|null|null|null|null|\n",
      "|    L210|   2|   3|   1|null|   1|   2|   1|\n",
      "|    L211|null|   1|null|   1|null|   1|null|\n",
      "|    L214|   2|   2|null|   2|   1|   4|   4|\n",
      "|    L500|   1|   1|null|null|null|null|null|\n",
      "|    L507|   4|   3|   3|null|   1|   3|   1|\n",
      "|    L509|   4|   5|   3|   2|   1|   4|   6|\n",
      "|    L511|null|null|null|null|null|   1|null|\n",
      "|    M200|   1|null|null|null|null|null|null|\n",
      "|    M311|   1|null|null|null|null|   1|   1|\n",
      "|    M500|null|null|null|null|null|   1|null|\n",
      "+--------+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "WITH location_day_cnt AS (\n",
    "    SELECT\n",
    "        location,\n",
    "        upper(date_format(start_date_time, \"EEE\")) AS day,\n",
    "        COUNT(*) AS cnt\n",
    "    FROM\n",
    "        section\n",
    "    GROUP BY\n",
    "        location, upper(date_format(start_date_time, \"EEE\"))\n",
    "    ORDER BY location\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    location_day_cnt\n",
    "PIVOT (\n",
    "    SUM(cnt)\n",
    "    FOR day IN (\n",
    "        'MON' AS MON,\n",
    "        'TUE' AS TUE,\n",
    "        'WED' AS WED,\n",
    "        'THU' AS THU,\n",
    "        'FRI' AS FRI,\n",
    "        'SAT' AS SAT,\n",
    "        'SUN' AS SUN\n",
    "    )\n",
    ")\n",
    "ORDER BY\n",
    "    location\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006c226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902d068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad1f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
