{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# SparkSQL Explain Plan\n",
    "\n",
    "* [Spark’s Logical and Physical plans … When, Why, How and Beyond](https://medium.com/datalex/sparks-logical-and-physical-plans-when-why-how-and-beyond-8cd1947b605a)\n",
    "* [SparkSQL - Explain Statement](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-explain.html)\n",
    "\n",
    "> The EXPLAIN statement is used to provide logical/physical plans for an input statement. By default, this clause provides information about a physical plan only.\n",
    "\n",
    "* [pyspark.sql.DataFrame.explain(extended=None, mode=None)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.explain.html)\n",
    "\n",
    "> #### extendedbool, optional\n",
    "> default False. If False, prints only the physical plan. When this is a string without specifying the mode, it works as the mode is specified.\n",
    "> \n",
    "> #### modestr, optional\n",
    "> specifies the expected output format of plans.\n",
    "> * simple: Print only a physical plan.\n",
    "> * extended: Print both logical and physical plans.\n",
    "> * codegen: Print a physical plan and generated codes if they are available.\n",
    "> * cost: Print a logical plan and statistics if they are available.\n",
    "> * formatted: Split explain output into two sections: a physical plan outline and node details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37098727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fc02f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from datetime import (\n",
    "    datetime,\n",
    "    date\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc892c",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n",
    "## Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21805809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=\"/opt/hadoop/hadoop-3.2.2/etc/hadoop\"\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc972bc5",
   "metadata": {},
   "source": [
    "## PySpark package imports\n",
    "\n",
    "Execute after the PYTHONPATH setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc37261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lit,\n",
    "    avg,\n",
    "    stddev,\n",
    "    isnan,\n",
    "    date_format,\n",
    "    to_date,\n",
    "    months_between,\n",
    "    add_months,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 15:59:34,169 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-22 15:59:36,525 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-02-22 15:59:38,903 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc80e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 4\n",
    "NUM_PARTITIONS = 3\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c757d",
   "metadata": {},
   "source": [
    "# DataFrame from Python data\n",
    "\n",
    "* [SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd05ad",
   "metadata": {},
   "source": [
    "# Schema Definition\n",
    "\n",
    "* [Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html#data-types)\n",
    "\n",
    "```from pyspark.sql.types import *```\n",
    "\n",
    "| Data type | Value type in Python | API to access or create a data type |  |\n",
    "|:---|:---|:---|:--|\n",
    "|ByteType | int or long Note: Numbers will be converted to 1-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -128 to 127. | ByteType() |  |\n",
    "| ShortType | int or long Note: Numbers will be converted to 2-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -32768 to 32767. | ShortType() |  |\n",
    "| IntegerType | int or long | IntegerType() |  |\n",
    "| LongType | long Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807.Otherwise, please convert data to decimal.Decimal and use DecimalType. | LongType() |  |\n",
    "| FloatType | float Note: Numbers will be converted to 4-byte single-precision floating point numbers at runtime. | FloatType() |  |\n",
    "| DoubleType | float | DoubleType() |  |\n",
    "| DecimalType | decimal.Decimal | DecimalType() |  |\n",
    "| StringType | string | StringType() |  |\n",
    "| BinaryType | bytearray | BinaryType() |  |\n",
    "| BooleanType | bool | BooleanType() |  |\n",
    "| TimestampType | datetime.datetime | TimestampType() |  |\n",
    "| DateType | datetime.date | DateType() |  |\n",
    "| ArrayType | list, tuple, or array | ArrayType(elementType, [containsNull]) Note:The default value of containsNull is True. |  |\n",
    "| MapType | dict | MapType(keyType, valueType, [valueContainsNull]) Note:The default value of valueContainsNull is True. |  |\n",
    "| StructType | list or tuple | StructType(fields) Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed. |  |\n",
    "| StructField | The value type in Python of the data type of this field (For example, Int for a StructField with the data type IntegerType) | StructField(name, dataType, [nullable]) Note: The default value of nullable is True. |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8776766",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74b8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir -p flight\n",
    "hdfs dfs -put -f ./data/flight/*.csv flight/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf22b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passengerId: integer (nullable = true)\n",
      " |-- flightId: integer (nullable = true)\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight = spark.read\\\n",
    "    .option(\"compression\", \"none\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"sep\", ',')\\\n",
    "    .option(\"nullValue\", np.nan)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\\\n",
    "    .csv(\"flight/flightData.csv\")\\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "flight.printSchema()\n",
    "flight.createOrReplaceTempView(\"flight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b46ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+---+----------+\n",
      "|passengerId|flightId|from|to |date      |\n",
      "+-----------+--------+----+---+----------+\n",
      "|48         |0       |cg  |ir |2017-01-01|\n",
      "|94         |0       |cg  |ir |2017-01-01|\n",
      "|82         |0       |cg  |ir |2017-01-01|\n",
      "|21         |0       |cg  |ir |2017-01-01|\n",
      "|51         |0       |cg  |ir |2017-01-01|\n",
      "|33         |0       |cg  |ir |2017-01-01|\n",
      "|20         |0       |cg  |ir |2017-01-01|\n",
      "|10         |0       |cg  |ir |2017-01-01|\n",
      "|49         |0       |cg  |ir |2017-01-01|\n",
      "|32         |0       |cg  |ir |2017-01-01|\n",
      "|70         |0       |cg  |ir |2017-01-01|\n",
      "|28         |0       |cg  |ir |2017-01-01|\n",
      "|42         |0       |cg  |ir |2017-01-01|\n",
      "|62         |0       |cg  |ir |2017-01-01|\n",
      "|80         |0       |cg  |ir |2017-01-01|\n",
      "|13         |0       |cg  |ir |2017-01-01|\n",
      "|46         |0       |cg  |ir |2017-01-01|\n",
      "|43         |0       |cg  |ir |2017-01-01|\n",
      "|17         |0       |cg  |ir |2017-01-01|\n",
      "|16         |0       |cg  |ir |2017-01-01|\n",
      "+-----------+--------+----+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcacf047",
   "metadata": {},
   "source": [
    "# Explain Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9715714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 16:00:39,899 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|      date| base_date|month_index|\n",
      "+----------+----------+-----------+\n",
      "|2017-01-01|2017-01-01|          0|\n",
      "|2017-11-29|2017-01-01|         10|\n",
      "|2017-12-12|2017-01-01|         11|\n",
      "|2017-12-22|2017-01-01|         11|\n",
      "|2017-12-29|2017-01-01|         11|\n",
      "|2017-01-01|2017-01-01|          0|\n",
      "|2017-01-01|2017-01-01|          0|\n",
      "|2017-01-10|2017-01-01|          0|\n",
      "|2017-02-06|2017-01-01|          1|\n",
      "|2017-03-05|2017-01-01|          2|\n",
      "+----------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [date#26, base_date#85, month_index#87L]\n",
      "+- *(3) Sort [passengerID#16 ASC NULLS FIRST, month_index#87L ASC NULLS FIRST], true, 0\n",
      "   +- *(3) Project [date#26, base_date#85, month_index#87L, passengerID#16]\n",
      "      +- Window [min(date#26) windowspecdefinition(date#26 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS base_date#85], [date#26 ASC NULLS FIRST]\n",
      "         +- *(2) Sort [date#26 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#141]\n",
      "               +- *(1) Project [cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#26, FLOOR(months_between(cast(cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) as timestamp), cast(Subquery scalar-subquery#86, [id=#126] as timestamp), true, Some(Australia/Sydney))) AS month_index#87L, passengerID#16]\n",
      "                  :  +- Subquery scalar-subquery#86, [id=#126]\n",
      "                  :     +- *(2) HashAggregate(keys=[], functions=[min(date#26)])\n",
      "                  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#122]\n",
      "                  :           +- *(1) HashAggregate(keys=[], functions=[partial_min(date#26)])\n",
      "                  :              +- *(1) Project [cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#26]\n",
      "                  :                 +- FileScan csv [date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:string>\n",
      "                  +- FileScan csv [passengerId#16,date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,date:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 16:00:46,539 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    date,\n",
    "    MIN(date) OVER (ORDER BY date) AS base_date,\n",
    "    FLOOR(months_between(date, (SELECT MIN(date) FROM flight))) AS month_index\n",
    "FROM\n",
    "    flight\n",
    "ORDER BY \n",
    "    passengerID, \n",
    "    month_index\n",
    "\"\"\"\n",
    "spark.sql(query).show(10)\n",
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d20c75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['passengerID ASC NULLS FIRST, 'month_index ASC NULLS FIRST], true\n",
      "+- 'Project ['date, 'MIN('date) windowspecdefinition('date ASC NULLS FIRST, unspecifiedframe$()) AS base_date#96, 'FLOOR('months_between('date, scalar-subquery#97 [])) AS month_index#98]\n",
      "   :  +- 'Project [unresolvedalias('MIN('date), None)]\n",
      "   :     +- 'UnresolvedRelation [flight], [], false\n",
      "   +- 'UnresolvedRelation [flight], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "date: date, base_date: date, month_index: bigint\n",
      "Project [date#26, base_date#96, month_index#98L]\n",
      "+- Sort [passengerID#16 ASC NULLS FIRST, month_index#98L ASC NULLS FIRST], true\n",
      "   +- Project [date#26, base_date#96, month_index#98L, passengerID#16]\n",
      "      +- Project [date#26, month_index#98L, base_date#96, base_date#96, passengerID#16]\n",
      "         +- Window [min(date#26) windowspecdefinition(date#26 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS base_date#96], [date#26 ASC NULLS FIRST]\n",
      "            +- Project [date#26, FLOOR(months_between(cast(date#26 as timestamp), cast(scalar-subquery#97 [] as timestamp), true, Some(Australia/Sydney))) AS month_index#98L, passengerID#16]\n",
      "               :  +- Aggregate [min(date#26) AS min(date)#101]\n",
      "               :     +- SubqueryAlias flight\n",
      "               :        +- Project [passengerId#16, flightId#17, from#18, to#19, to_date('date, Some(yyyy-MM-dd)) AS date#26]\n",
      "               :           +- Relation[passengerId#16,flightId#17,from#18,to#19,date#20] csv\n",
      "               +- SubqueryAlias flight\n",
      "                  +- Project [passengerId#16, flightId#17, from#18, to#19, to_date('date, Some(yyyy-MM-dd)) AS date#26]\n",
      "                     +- Relation[passengerId#16,flightId#17,from#18,to#19,date#20] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [date#26, base_date#96, month_index#98L]\n",
      "+- Sort [passengerID#16 ASC NULLS FIRST, month_index#98L ASC NULLS FIRST], true\n",
      "   +- Project [date#26, base_date#96, month_index#98L, passengerID#16]\n",
      "      +- Window [min(date#26) windowspecdefinition(date#26 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS base_date#96], [date#26 ASC NULLS FIRST]\n",
      "         +- Project [cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#26, FLOOR(months_between(cast(cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) as timestamp), cast(scalar-subquery#97 [] as timestamp), true, Some(Australia/Sydney))) AS month_index#98L, passengerID#16]\n",
      "            :  +- Aggregate [min(date#26) AS min(date)#101]\n",
      "            :     +- Project [cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#26]\n",
      "            :        +- Relation[passengerId#16,flightId#17,from#18,to#19,date#20] csv\n",
      "            +- Relation[passengerId#16,flightId#17,from#18,to#19,date#20] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [date#26, base_date#96, month_index#98L]\n",
      "+- *(3) Sort [passengerID#16 ASC NULLS FIRST, month_index#98L ASC NULLS FIRST], true, 0\n",
      "   +- *(3) Project [date#26, base_date#96, month_index#98L, passengerID#16]\n",
      "      +- Window [min(date#26) windowspecdefinition(date#26 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS base_date#96], [date#26 ASC NULLS FIRST]\n",
      "         +- *(2) Sort [date#26 ASC NULLS FIRST], false, 0\n",
      "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#205]\n",
      "               +- *(1) Project [cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#26, FLOOR(months_between(cast(cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) as timestamp), cast(Subquery scalar-subquery#97, [id=#190] as timestamp), true, Some(Australia/Sydney))) AS month_index#98L, passengerID#16]\n",
      "                  :  +- Subquery scalar-subquery#97, [id=#190]\n",
      "                  :     +- *(2) HashAggregate(keys=[], functions=[min(date#26)], output=[min(date)#101])\n",
      "                  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#186]\n",
      "                  :           +- *(1) HashAggregate(keys=[], functions=[partial_min(date#26)], output=[min#106])\n",
      "                  :              +- *(1) Project [cast(gettimestamp(date#20, yyyy-MM-dd, Some(Australia/Sydney), false) as date) AS date#26]\n",
      "                  :                 +- FileScan csv [date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:string>\n",
      "                  +- FileScan csv [passengerId#16,date#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[hdfs://ubuntu:8020/user/oonisim/flight/flightData.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,date:string>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 16:01:08,966 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1928af9",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a09e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
