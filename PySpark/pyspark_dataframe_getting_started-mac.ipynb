{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e18fde",
   "metadata": {},
   "source": [
    "# Pyspark installation and start on Apple Silicon\n",
    "\n",
    "* [Installation guide to pyspark on M1 Mac](https://gist.github.com/brianspiering/1e690b593db025b5acee920fa7330366)\n",
    "* [Pyspark: Exception: Java gateway process exited before sending the driver its port number](https://stackoverflow.com/a/75391117/4281353)\n",
    "\n",
    "## Prereauisites\n",
    "\n",
    "### JDK version\n",
    "Stick to JDK 8.\n",
    "```\n",
    "brew install --cask adoptopenjdk8\n",
    "```\n",
    "\n",
    "### Spark Installation with Homebrew\n",
    "\n",
    "Spark installation is expected to be done via brew as homebrew specific paths are used.\n",
    "\n",
    "```\n",
    "brew install scala\n",
    "brew install apache-spark\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8357009e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d8ecdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d64e91",
   "metadata": {},
   "source": [
    "# PySpark DataFrame Getting Started\n",
    "\n",
    "* [Spark SQL](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html) (MUST)\n",
    "\n",
    "## Spark SQL Core Classes\n",
    "\n",
    "Note that DataFrame is SparkSQL class.\n",
    "\n",
    "\n",
    "| SparkSession(sparkContext[, jsparkSession]) | The entry point to programming Spark with the Dataset and DataFrame API. |\n",
    "|:---|:---|\n",
    "| Catalog(sparkSession) | User-facing catalog API, accessible through SparkSession.catalog. |\n",
    "| DataFrame(jdf, sql_ctx) | A distributed collection of data grouped into named columns. |\n",
    "| Column(jc) | A column in a DataFrame. |\n",
    "| Row | A row in DataFrame. |\n",
    "| GroupedData(jgd, df) | A set of methods for aggregations on a DataFrame, created by DataFrame.groupBy(). |\n",
    "| PandasCogroupedOps(gd1, gd2) | A logical grouping of two GroupedData, created by GroupedData.cogroup(). |\n",
    "| DataFrameNaFunctions(df) | Functionality for working with missing data in DataFrame. |\n",
    "| DataFrameStatFunctions(df) | Functionality for statistic functions with DataFrame. |\n",
    "| Window | Utility functions for defining window in DataFrames. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8380d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad795c",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bc0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME = \"/opt/homebrew/Cellar/apache-spark/3.3.1/libexec\"\n",
    "JAVA_HOME = '/opt/homebrew/opt/openjdk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbec1c7",
   "metadata": {},
   "source": [
    "#  Environemnt Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc0801",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a17b6",
   "metadata": {},
   "source": [
    "## SPARK_HOME\n",
    "\n",
    "SPARK_HOME must be set to /opt/homebrew/Cellar/apache-spark/3.3.1/**libexec**\",  **NOT** /opt/homebrew/Cellar/apache-spark/3.3.1\". \n",
    "\n",
    "Otherwise **Java gateway process exited before sending its port number** in java_gateway.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "448f7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Environment Variables\n",
    "# --------------------------------------------------------------------------------\n",
    "os.environ['SPARK_HOME'] = SPARK_HOME\n",
    "os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "sys.path.extend([\n",
    "    f\"{SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip\",\n",
    "    f\"{SPARK_HOME}/python/lib/pyspark.zip\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ceff1",
   "metadata": {},
   "source": [
    "## PySpark package imports\n",
    "\n",
    "Execute after the PYTHONPATH setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b9755",
   "metadata": {},
   "source": [
    "## PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df35c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    avg,\n",
    "    stddev,\n",
    "    isnan,\n",
    "    round,\n",
    "    to_date,\n",
    "    date_format,\n",
    "    from_unixtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860cd83",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "* [The UC Irvine Machine Learning Repository  - Record Linkage Comparison Patterns Data Set](https://archive.ics.uci.edu/ml/datasets/Record+Linkage+Comparison+Patterns)\n",
    "\n",
    "The data are pairs of patient records to identify the two records refer to the same patient or not (na-yose in Japanse).It is from the record linkage study performed at a hospital in 2010 analyzing pairs of patient records that were matched according to several different criteria, such as the patient’s name (first and last), address, and birthday. \n",
    "\n",
    "Each matching field was assigned a numerical score from 0.0 to 1.0 based on how similar the strings were, and the data was then hand-labeled to identify which pairs represented the same person and which did not. \n",
    "\n",
    "\n",
    "| feature | description  |\n",
    "|:---------|:--------------|\n",
    "| is_match| if the pair is a match or not (1: match)          |\n",
    "| cmp_sex | if the gender of the pair is a match (1:match)             |\n",
    "|         |              |\n",
    "|         |              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abb10493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   163  100   163    0     0    617      0 --:--:-- --:--:-- --:--:--   624\n",
      "100 53.8M  100 53.8M    0     0  6588k      0  0:00:08  0:00:08 --:--:-- 8829k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  donation.zip\n",
      " extracting: block_10.zip            \n",
      " extracting: block_1.zip             \n",
      " extracting: block_2.zip             \n",
      " extracting: block_3.zip             \n",
      " extracting: block_4.zip             \n",
      " extracting: block_5.zip             \n",
      " extracting: block_6.zip             \n",
      " extracting: block_7.zip             \n",
      " extracting: block_8.zip             \n",
      " extracting: block_9.zip             \n",
      "  inflating: documentation           \n",
      "  inflating: frequencies.csv         \n",
      "Archive:  block_3.zip\n",
      "  inflating: block_3.csv             \n",
      "\n",
      "Archive:  block_2.zip\n",
      "  inflating: block_2.csv             \n",
      "\n",
      "Archive:  block_1.zip\n",
      "  inflating: block_1.csv             \n",
      "\n",
      "Archive:  block_5.zip\n",
      "  inflating: block_5.csv             \n",
      "\n",
      "Archive:  block_4.zip\n",
      "  inflating: block_4.csv             \n",
      "\n",
      "Archive:  block_6.zip\n",
      "  inflating: block_6.csv             \n",
      "\n",
      "Archive:  block_10.zip\n",
      "  inflating: block_10.csv            \n",
      "\n",
      "Archive:  block_7.zip\n",
      "  inflating: block_7.csv             \n",
      "\n",
      "Archive:  block_9.zip\n",
      "  inflating: block_9.csv             \n",
      "\n",
      "Archive:  block_8.zip\n",
      "  inflating: block_8.csv             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10 archives were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf ./data/linkage\n",
    "mkdir -p ./data/linkage\n",
    "cd ./data/linkage/\n",
    "curl -L -o donation.zip https://bit.ly/1Aoywaq\n",
    "unzip -o donation.zip\n",
    "unzip -o 'block_*.zip'\n",
    "rm -rf *.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14efc5",
   "metadata": {},
   "source": [
    "* [New York Times Best Sellers - Hardcover Fiction Best Sellers from 2008 to 2018](https://www.kaggle.com/cmenca/new-york-times-hardcover-fiction-best-sellers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45009512",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17385b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36e96bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 07:31:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356d2b0",
   "metadata": {},
   "source": [
    "To avoid ```You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2008-06-22 10:00:00' in the new parser.```.\n",
    "\n",
    "* [String to Date migration from Spark 2.0 to 3.0 gives Fail to recognize](https://stackoverflow.com/questions/62602720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a33a24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1787d",
   "metadata": {},
   "source": [
    "---\n",
    "# Read CSV\n",
    "\n",
    "* [DataFrameReader/Writer API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output)\n",
    "\n",
    "* [SparkSQL CSV Files](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)\n",
    "\n",
    "> Spark SQL provides spark.read().csv(\"file_name\") to read a file or directory of files in CSV format into Spark DataFrame, and dataframe.write().csv(\"path\") to write to a CSV file. Function option() can be used to customize the behavior of reading or writing.\n",
    "\n",
    "[SparkSession.read()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.html#read--) returns [DataFrameReader](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html) instance which has [option](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-boolean-) method by which we can specify CSV options.\n",
    "\n",
    "The options are listed in [Data Source Option](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516c8f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev = spark.read.csv(\"data/linkage\")\n",
    "prev.printSchema()\n",
    "del prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717bcb2c",
   "metadata": {},
   "source": [
    "## CSV Options\n",
    "\n",
    "* Schema Inference \n",
    "* Null value replacement\n",
    "* Header handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedd90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|id_1 |id_2 |cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|3148 |8326 |1           |null        |1           |null        |1      |1     |1     |1     |1      |true    |\n",
      "|14055|94934|1           |null        |1           |null        |1      |1     |1     |1     |1      |true    |\n",
      "|33948|34740|1           |null        |1           |null        |1      |1     |1     |1     |1      |true    |\n",
      "|946  |71870|1           |null        |1           |null        |1      |1     |1     |1     |1      |true    |\n",
      "|64880|71676|1           |null        |1           |null        |1      |1     |1     |1     |1      |true    |\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"nullValue\", \"?\")\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .csv(\"data/linkage\")\n",
    "\n",
    "parsed.printSchema()\n",
    "parsed.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939b6c9",
   "metadata": {},
   "source": [
    "---\n",
    "# Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86f52a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749213"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff3b0b",
   "metadata": {},
   "source": [
    "## Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e3e2954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 07:31:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 1. Title: Record Linkage Comparison Patterns , ?, ?, ?, ?, ?, ?, ?, ?, ?, ?\n",
      " Schema: id_1, id_2, cmp_fname_c1, cmp_fname_c2, cmp_lname_c1, cmp_lname_c2, cmp_sex, cmp_bd, cmp_bm, cmp_by, cmp_plz\n",
      "Expected: id_1 but found: 1. Title: Record Linkage Comparison Patterns \n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/documentation\n",
      "23/02/09 07:31:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: cmp_fname_c1, cmp_fname_c2, cmp_lname_c1, cmp_lname_c2, cmp_sex, cmp_bd, cmp_bm, cmp_by, cmp_plz, ?, ?\n",
      " Schema: id_1, id_2, cmp_fname_c1, cmp_fname_c2, cmp_lname_c1, cmp_lname_c2, cmp_sex, cmp_bd, cmp_bm, cmp_by, cmp_plz\n",
      "Expected: id_1 but found: cmp_fname_c1\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/frequencies.csv\n",
      "+-------+--------------------+-----------------+--------------------+--------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "|summary|                id_1|             id_2|        cmp_fname_c1|        cmp_fname_c2|       cmp_lname_c1|      cmp_lname_c2|           cmp_sex|             cmp_bd|            cmp_bm|            cmp_by|            cmp_plz|\n",
      "+-------+--------------------+-----------------+--------------------+--------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "|  count|             5749213|          5749158|             5748131|              103701|            5749133|              2465|           5749133|            5748338|           5748338|           5748337|            5736289|\n",
      "|   mean|   33324.47979999771|66587.42400114964|  0.7129023464249419|   0.900008998936421| 0.3156278513776009| 0.318296744405166|0.9550012294607436|0.22446522967751065|0.4888552135282675|0.2227485966810923|0.00552866147434343|\n",
      "| stddev|   23659.86139888655|23620.50188438175| 0.38875843950829186| 0.27133067681523776|0.33423361373861266|0.3685373395187368|0.2073014119031234| 0.4172296957328137|0.4998758217099875|0.4160909629831756|0.07414914925420046|\n",
      "|    min|                 ...| 2 non-predictive|                    | Aslihan Gerhold-Ay:|                  0|                 0|                 0|                  0|                 0|                 0|                  0|\n",
      "|    max|                9999|            99999|2.68694413843136e-05|                   1|                  1|                 1|                 1|                  1|                 1|                 1|                  1|\n",
      "+-------+--------------------+-----------------+--------------------+--------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summary = parsed.describe()\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36825a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 07:31:18 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 1, schema size: 12\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/documentation\n",
      "23/02/09 07:31:18 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 9, schema size: 12\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/frequencies.csv\n",
      "+-------+-----------------+-----------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|summary|             id_1|             id_2|       cmp_fname_c1|       cmp_fname_c2|        cmp_lname_c1|       cmp_lname_c2|            cmp_sex|             cmp_bd|              cmp_bm|             cmp_by|            cmp_plz|\n",
      "+-------+-----------------+-----------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|  count|            20931|            20931|              20922|               1333|               20931|                475|              20931|              20925|               20925|              20925|              20902|\n",
      "|   mean|34575.72117911232|51259.95939037791| 0.9973163859635038| 0.9898900320318176|  0.9970152595958817|  0.969370167843852|  0.987291577086618| 0.9970848267622461|  0.9979450418160095| 0.9961290322580645| 0.9584250310975027|\n",
      "| stddev|21950.31285196913|24345.73345377519|0.03650667584833679|0.08251973727615237|0.043118807533945126|0.15345280740388917|0.11201570591216435|0.05391487659807981|0.045286127452170664|0.06209804856731055|0.19962063345931919|\n",
      "|    min|            10001|            10010|                  0|                  0|                   0|                  0|                  0|                  0|                   0|                  0|                  0|\n",
      "|    max|            99946|            99996|                  1|                  1|                   1|                  1|                  1|                  1|                   1|                  1|                  1|\n",
      "+-------+-----------------+-----------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "matched_summary = parsed.where(col(\"is_match\") == True).describe()\n",
    "matched_summary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0a1618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 07:31:23 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 1, schema size: 12\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/documentation\n",
      "23/02/09 07:31:23 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 9, schema size: 12\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/frequencies.csv\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|              id_1|              id_2|       cmp_fname_c1|       cmp_fname_c2|      cmp_lname_c1|       cmp_lname_c2|            cmp_sex|            cmp_bd|            cmp_bm|            cmp_by|             cmp_plz|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|           5728201|           5728201|            5727203|             102365|           5728201|               1989|            5728201|           5727412|           5727412|           5727412|             5715387|\n",
      "|   mean|33319.913548075565| 66643.44259218557| 0.7118634802175091| 0.8988473514090158|0.3131380113364304|0.16295544855122532| 0.9548833918362851|0.2216425149788421| 0.486995347986141|0.2199230647280133|0.002043781112285135|\n",
      "| stddev|23665.760130330676|23599.551728241313|0.38908060096985553|0.27272090294010215|0.3322812130572686| 0.1930236663528703|0.20755988859217375|0.4153518275558732|0.4998308940493865|0.4141943267142977|0.045161979893625206|\n",
      "|    min|                 1|             10000|                  0|                  0|                 0|                  0|                  0|                 0|                 0|                 0|                   0|\n",
      "|    max|              9999|             99999|                  1|                  1|                 1|                  1|                  1|                 1|                 1|                 1|                   1|\n",
      "+-------+------------------+------------------+-------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unmatched_summary = parsed.where(col(\"is_match\") == False).describe()\n",
    "unmatched_summary.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f19b2",
   "metadata": {},
   "source": [
    "## Rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8743d37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|17186|64804|        null|        null|         0.5|        null|      1|     0|     0|     1|      0|   false|\n",
      "|58872|80686|        null|        null|       0.125|        null|      0|     1|     1|     1|      0|   false|\n",
      "|19093|75754|        null|        null|           1|        null|      1|     0|     0|     0|      0|   false|\n",
      "|51568|69136|        null|        null|       0.625|        null|      1|     0|     0|     0|      0|   false|\n",
      "|36952|63401|        null|        null|           0|        null|      1|     1|     1|     1|      0|   false|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.where(\n",
    "    col(\"cmp_fname_c1\").isNull() &\n",
    "    col(\"cmp_fname_c2\").isNull() & \n",
    "    (col(\"is_match\") == False)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4fbba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:=====>                                                   (1 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 07:31:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: ?\n",
      " Schema: cmp_fname_c2\n",
      "Expected: cmp_fname_c2 but found: ?\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/documentation\n",
      "23/02/09 07:31:24 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: cmp_lname_c2\n",
      " Schema: cmp_fname_c2\n",
      "Expected: cmp_fname_c2 but found: cmp_lname_c2\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/frequencies.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:=============================================>           (8 + 2) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5645512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.where(\n",
    "    col(\"cmp_fname_c2\").isNull() \n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1e43f",
   "metadata": {},
   "source": [
    "---\n",
    "# DataFrame Structure\n",
    "\n",
    "## Comparison with HTML\n",
    "\n",
    "| HTML       | HTML Description             | Spark       | Spark Description                                                                                                                                                                                                                                                                                                                       |   |\n",
    "|------------|-------------------------|-------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|\n",
    "| ```<table>```    | List of ```<tr>```            | DataFrame   | List of Row, and DataFrame is an alias of type DataSet[Row].                                                                                                                                                                                                                                                                      |   |\n",
    "| ```<tr>```       | Table row. List of ```<td> ```| Row         | List of typed values.  ```Row``` has the schema of type **StructType** |   |\n",
    "|            |                         | StructType  | Defines the type of a Row and a list of StructField that defines a field of a Row.                                                                                                                                                                                                                                                |   |\n",
    "|```<td>```Table  | Table field             | StructField | Specify the DataType of a field of a row.                                                                                                                                                                                                                                                                                         |   |\n",
    "|            |                         | DataType    |                                                                                                                                                                                                                                                                                                                                   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf23066",
   "metadata": {},
   "source": [
    "## DataFrame Schema Definition\n",
    "\n",
    "Instead of using **Schema Inference**, you can provide the **Schema Definition** for  ```SparkSession.read.schema(schema).csv(file)``` to use when reading the CSV file.\n",
    "\n",
    "\n",
    "* [Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html#data-types)\n",
    "\n",
    "```from pyspark.sql.types import *```\n",
    "\n",
    "| Data type | Value type in Python | API to access or create a data type |  |\n",
    "|:---|:---|:---|:--|\n",
    "|ByteType | int or long Note: Numbers will be converted to 1-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -128 to 127. | ByteType() |  |\n",
    "| ShortType | int or long Note: Numbers will be converted to 2-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -32768 to 32767. | ShortType() |  |\n",
    "| IntegerType | int or long | IntegerType() |  |\n",
    "| LongType | long Note: Numbers will be converted to 8-byte signed integer numbers at runtime. Please make sure that numbers are within the range of -9223372036854775808 to 9223372036854775807.Otherwise, please convert data to decimal.Decimal and use DecimalType. | LongType() |  |\n",
    "| FloatType | float Note: Numbers will be converted to 4-byte single-precision floating point numbers at runtime. | FloatType() |  |\n",
    "| DoubleType | float | DoubleType() |  |\n",
    "| DecimalType | decimal.Decimal | DecimalType() |  |\n",
    "| StringType | string | StringType() |  |\n",
    "| BinaryType | bytearray | BinaryType() |  |\n",
    "| BooleanType | bool | BooleanType() |  |\n",
    "| TimestampType | datetime.datetime | TimestampType() |  |\n",
    "| DateType | datetime.date | DateType() |  |\n",
    "| ArrayType | list, tuple, or array | ArrayType(elementType, [containsNull]) Note:The default value of containsNull is True. |  |\n",
    "| MapType | dict | MapType(keyType, valueType, [valueContainsNull]) Note:The default value of valueContainsNull is True. |  |\n",
    "| StructType | list or tuple | StructType(fields) Note: fields is a Seq of StructFields. Also, two fields with the same name are not allowed. |  |\n",
    "| StructField | The value type in Python of the data type of this field (For example, Int for a StructField with the data type IntegerType) | StructField(name, dataType, [nullable]) Note: The default value of nullable is True. |  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cee5988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField('id_1', IntegerType(), False)\n",
      "StructField('id_2', StringType(), False)\n",
      "StructField('cmp_fname_c1', DoubleType(), False)\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id_1\", IntegerType(), False),\n",
    "    StructField(\"id_2\", StringType(), False),\n",
    "    StructField(\"cmp_fname_c1\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "for element in schema:\n",
    "    print(element)\n",
    "    \n",
    "# spark.read.schema(schema).csv(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13275995",
   "metadata": {},
   "source": [
    "## Row\n",
    "\n",
    "Each row of the DataFrame is an instance of ```pyspark.sql.Row```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6b7686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id_1='3148', id_2='8326', cmp_fname_c1='1', cmp_fname_c2=None, cmp_lname_c1='1', cmp_lname_c2=None, cmp_sex='1', cmp_bd='1', cmp_bm='1', cmp_by=1, cmp_plz=1, is_match=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row: pyspark.sql.Row = parsed.first()\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950cd56",
   "metadata": {},
   "source": [
    "## Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a80b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_1 type:<class 'str'> value: 3148\n"
     ]
    }
   ],
   "source": [
    "id_1 = row['id_1']\n",
    "print(f\"id_1 type:{type(id_1)} value: {id_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34d198",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc7e45",
   "metadata": {},
   "source": [
    "# Caching the dataframe\n",
    "\n",
    "The call to ```cache``` indicates that the contents of the DataFrame should be stored in memory the next time it’s computed. Spark defines a few different mechanisms, or StorageLevel values, for persisting data. cache() is shorthand for [DataFrame.persist](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.persist.html)(StorageLevel.MEMORY), which stores the rows as unserialized Java objects (NOT Python objects).\n",
    "\n",
    "* [class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html)\n",
    "\n",
    "## [RDD Persistence](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)\n",
    "\n",
    "> One of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n",
    "> \n",
    "> You can mark an RDD to be persisted using the persist() or cache() methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.\n",
    "> \n",
    "> In addition, each persisted RDD can be stored using a different **storage level**, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a StorageLevel object (Scala, Java, Python) to persist(). The ```cache()``` method is a shorthand for using the default storage level, which is **StorageLevel.MEMORY_ONLY** (store deserialized objects in memory). The full set of storage levels is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "641b5365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_1: string, id_2: string, cmp_fname_c1: string, cmp_fname_c2: string, cmp_lname_c1: string, cmp_lname_c2: string, cmp_sex: string, cmp_bd: string, cmp_bm: string, cmp_by: int, cmp_plz: int, is_match: boolean]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0c571",
   "metadata": {},
   "source": [
    "---\n",
    "# Cast column type\n",
    "\n",
    "Estimates of the data has string columns which should be numeric. Convert them to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "651e81f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- id_1: string (nullable = true)\n",
      " |-- id_2: string (nullable = true)\n",
      " |-- cmp_fname_c1: string (nullable = true)\n",
      " |-- cmp_fname_c2: string (nullable = true)\n",
      " |-- cmp_lname_c1: string (nullable = true)\n",
      " |-- cmp_lname_c2: string (nullable = true)\n",
      " |-- cmp_sex: string (nullable = true)\n",
      " |-- cmp_bd: string (nullable = true)\n",
      " |-- cmp_bm: string (nullable = true)\n",
      " |-- cmp_by: string (nullable = true)\n",
      " |-- cmp_plz: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1b3ab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+--------------------+--------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "|summary|                id_1|             id_2|        cmp_fname_c1|        cmp_fname_c2|       cmp_lname_c1|      cmp_lname_c2|           cmp_sex|             cmp_bd|            cmp_bm|            cmp_by|            cmp_plz|\n",
      "+-------+--------------------+-----------------+--------------------+--------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "|  count|             5749213|          5749158|             5748131|              103701|            5749133|              2465|           5749133|            5748338|           5748338|           5748337|            5736289|\n",
      "|   mean|   33324.47979999771|66587.42400114964|  0.7129023464249419|   0.900008998936421| 0.3156278513776009| 0.318296744405166|0.9550012294607436|0.22446522967751065|0.4888552135282675|0.2227485966810923|0.00552866147434343|\n",
      "| stddev|   23659.86139888655|23620.50188438175| 0.38875843950829186| 0.27133067681523776|0.33423361373861266|0.3685373395187368|0.2073014119031234| 0.4172296957328137|0.4998758217099875|0.4160909629831756|0.07414914925420046|\n",
      "|    min|                 ...| 2 non-predictive|                    | Aslihan Gerhold-Ay:|                  0|                 0|                 0|                  0|                 0|                 0|                  0|\n",
      "|    max|                9999|            99999|2.68694413843136e-05|                   1|                  1|                 1|                 1|                  1|                 1|                 1|                  1|\n",
      "+-------+--------------------+-----------------+--------------------+--------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d1a5b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- id_1: double (nullable = true)\n",
      " |-- id_2: double (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: double (nullable = true)\n",
      " |-- cmp_bd: double (nullable = true)\n",
      " |-- cmp_bm: double (nullable = true)\n",
      " |-- cmp_by: double (nullable = true)\n",
      " |-- cmp_plz: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in summary.columns[1:]:\n",
    "    summary = summary.withColumn(column, summary[column].cast(DoubleType()))\n",
    "\n",
    "summary.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ac42d",
   "metadata": {},
   "source": [
    "# Transpose Dataframe\n",
    "\n",
    "Spark DataFrame does not have transpose method. When the data size is small, convert first to Pandas to transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ce5851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------------+-------------------+---+-------------------+\n",
      "|       field|    count|               mean|             stddev|min|                max|\n",
      "+------------+---------+-------------------+-------------------+---+-------------------+\n",
      "|        id_1|5749213.0|  33324.47979999771|  23659.86139888655|NaN|             9999.0|\n",
      "|        id_2|5749158.0|  66587.42400114964|  23620.50188438175|NaN|            99999.0|\n",
      "|cmp_fname_c1|5748131.0| 0.7129023464249419|0.38875843950829186|NaN|2.68694413843136E-5|\n",
      "|cmp_fname_c2| 103701.0|  0.900008998936421|0.27133067681523776|NaN|                1.0|\n",
      "|cmp_lname_c1|5749133.0| 0.3156278513776009|0.33423361373861266|0.0|                1.0|\n",
      "|cmp_lname_c2|   2465.0|  0.318296744405166| 0.3685373395187368|0.0|                1.0|\n",
      "|     cmp_sex|5749133.0| 0.9550012294607436| 0.2073014119031234|0.0|                1.0|\n",
      "|      cmp_bd|5748338.0|0.22446522967751065| 0.4172296957328137|0.0|                1.0|\n",
      "|      cmp_bm|5748338.0| 0.4888552135282675| 0.4998758217099875|0.0|                1.0|\n",
      "|      cmp_by|5748337.0| 0.2227485966810923| 0.4160909629831756|0.0|                1.0|\n",
      "|     cmp_plz|5736289.0|0.00552866147434343|0.07414914925420046|0.0|                1.0|\n",
      "+------------+---------+-------------------+-------------------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transpose(df: pyspark.sql.DataFrame):\n",
    "    assert df.count() < 1000\n",
    "    pdf = df.toPandas()\n",
    "    pdf = pdf.set_index(df.columns[0]).transpose().reset_index()\n",
    "    pdf = pdf.rename(columns={\"index\":\"field\"})\n",
    "    pdf = pdf.rename_axis(None, axis=1)\n",
    "    \n",
    "    transposed: pyspark.sql.Dataframe = spark.createDataFrame(pdf)\n",
    "    del pdf\n",
    "    return transposed\n",
    "   \n",
    "\n",
    "transpose(summary).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e66fe",
   "metadata": {},
   "source": [
    "---\n",
    "# DataFrame API\n",
    "\n",
    "* [DataFrame APIs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis) (MUST)\n",
    "* [PySpark and SparkSQL Basics](https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132f3b6",
   "metadata": {},
   "source": [
    "## Conditional filtering on Column\n",
    "\n",
    "* [Column API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis)\n",
    "\n",
    "where() is alias of filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cb77966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/09 07:31:29 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 1, schema size: 12\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/documentation\n",
      "23/02/09 07:31:29 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 9, schema size: 12\n",
      "CSV file: file:///Users/oonisim/home/repository/git/oonisim/spark-programs/PySpark/data/linkage/frequencies.csv\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed.where(col(\"is_match\") == True).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2ded801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|      cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|           1|        null|                 1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|           1|        null|                 1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|           1|        null|                 1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|61261|           0|        null| 0.111111111111111|        null|      0|     1|     1|     1|      0|   false|\n",
      "|  946|35374|       0.125|        null|               0.5|        null|      1|     0|     0|     0|      0|   false|\n",
      "|  946|39254|           0|        null| 0.428571428571429|        null|      1|     0|     0|     0|      0|   false|\n",
      "|  946|83671|           1|        null| 0.333333333333333|        null|      1|     1|     0|     0|      0|   false|\n",
      "|  946|50784|           1|        null|             0.125|        null|      1|     0|     0|     1|      0|   false|\n",
      "|  946|86499|           1|        null| 0.142857142857143|        null|      1|     1|     0|     0|      0|   false|\n",
      "|  946| 9273|           1|        null| 0.333333333333333|        null|      1|     0|     1|     0|      0|   false|\n",
      "|  946|57276|        0.25|        null| 0.666666666666667|        null|      1|     0|     0|     1|      0|   false|\n",
      "|  946|63367|         0.2|        null| 0.142857142857143|        null|      0|     1|     1|     1|      0|   false|\n",
      "|  946|63746|         0.5|        null| 0.666666666666667|        null|      1|     0|     0|     0|      0|   false|\n",
      "|  946|70956|       0.375|        null| 0.428571428571429|        null|      1|     0|     1|     0|      0|   false|\n",
      "|  946| 2453|       0.125|        null| 0.428571428571429|        null|      1|     0|     0|     0|      0|   false|\n",
      "|  946|85116|           1|        null|0.0769230769230769|        null|      1|     0|     1|     0|      0|   false|\n",
      "|  946|34019|           0|        null|                 1|        null|      1|     0|     0|     0|      0|   false|\n",
      "|  946|87507|           1|        null| 0.285714285714286|        null|      1|     0|     0|     1|      0|   false|\n",
      "|  946|92994|           1|        null|             0.375|        null|      1|     0|     1|     0|      0|   false|\n",
      "|  946|27664|         0.5|        null| 0.666666666666667|        null|      1|     0|     0|     0|      0|   false|\n",
      "+-----+-----+------------+------------+------------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.where(col(\"id_1\").isin([3148, 946, 64880])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04b5611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|25739|45991|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|62415|93584|           1|        null|           1|        null|      1|     1|     1|     1|      0|    true|\n",
      "|27995|31399|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "| 4909|12238|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|15161|16743|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|31703|37310|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|30213|36558|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|56596|56630|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|16481|21174|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|32649|37094|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|34268|37260|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|66117|69253|           1|        null|           1|        null|      1|     1|     1|     1|      0|    true|\n",
      "| 2771|31982|           1|        null|           1|        null|      0|     1|     1|     1|      1|    true|\n",
      "|23557|29673|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|37156|39557|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.where(\n",
    "    col(\"cmp_fname_c1\").isNotNull() & col(\"cmp_fname_c2\").isNull()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b2b54",
   "metadata": {},
   "source": [
    "## Conditional Annotation\n",
    "\n",
    "```WHEN X THEN ... ELSE WHEN Y ... ELSE ...```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8feef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+------+\n",
      "| id_1| id_2|is_match|oddity|\n",
      "+-----+-----+--------+------+\n",
      "| 3148| 8326|    true|   2nd|\n",
      "|14055|94934|    true|   1st|\n",
      "|33948|34740|    true|   1st|\n",
      "|  946|71870|    true|   2nd|\n",
      "|64880|71676|    true|   3rd|\n",
      "+-----+-----+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.withColumn(\n",
    "    \"oddity\",\n",
    "    when(col(\"id_1\") % 3 == 0, \"1st\")\\\n",
    "    .when(col(\"id_1\") % 3 == 1, \"2nd\")\\\n",
    "    .when(col(\"id_1\") % 3 == 2, \"3rd\")\\\n",
    "    .otherwise(\"last\") \n",
    ")\\\n",
    ".select(\"id_1\", \"id_2\", \"is_match\", \"oddity\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de0e75",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e8a0d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|                id_1|id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+--------------------+----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|                 ...|null|        null|        null|        null|        null|   null|  null|  null|  null|   null|    null|\n",
      "|             achi...|null|        null|        null|        null|        null|   null|  null|  null|  null|   null|    null|\n",
      "|             link...|null|        null|        null|        null|        null|   null|  null|  null|  null|   null|    null|\n",
      "|             that...|null|        null|        null|        null|        null|   null|  null|  null|  null|   null|    null|\n",
      "|          -- A ne...|null|        null|        null|        null|        null|   null|  null|  null|  null|   null|    null|\n",
      "+--------------------+----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.orderBy(\"id_1\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d065da",
   "metadata": {},
   "source": [
    "## Group By and Aggregation\n",
    "\n",
    "### Column name reference\n",
    "Two ways we can reference the names of the columns in the DataFrame: either as literal strings, like in groupBy(\"is_match\"), or as Column objects by using the \"col()\" function. Need to use the ```col ```function to call the ```desc``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c466ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "|    null|     81|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.groupby('is_match').count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e636101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:>                                                        (0 + 8) / 10]\r",
      "\r",
      "[Stage 38:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+\n",
      "|is_match|cmp_plz_mean|cmp_sex_mean|\n",
      "+--------+------------+------------+\n",
      "|    true|        0.96|        0.11|\n",
      "|   false|         0.0|        0.21|\n",
      "|    null|        null|        null|\n",
      "+--------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed.groupby('is_match').agg(\n",
    "    round(avg(\"cmp_plz\"),2),\n",
    "    round(stddev(\"cmp_sex\"),2)\n",
    ")\\\n",
    ".withColumnRenamed(\"round(avg(cmp_plz), 2)\", \"cmp_plz_mean\")\\\n",
    ".withColumnRenamed(\"round(stddev_samp(cmp_sex), 2)\", \"cmp_sex_mean\")\\\n",
    ".orderBy(col(\"is_match\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae647efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------------+\n",
      "|is_match|        cmp_plz_mean|    stddev(cmp_sex)|\n",
      "+--------+--------------------+-------------------+\n",
      "|    true|  0.9584250310975027|0.11201570591216435|\n",
      "|   false|0.002043781112285135|0.20755988859217375|\n",
      "|    null|                null|               null|\n",
      "+--------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:=============================================>           (8 + 2) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parsed\\\n",
    "    .groupby('is_match')\\\n",
    "    .agg({\n",
    "        \"cmp_plz\": \"avg\",\n",
    "        \"cmp_sex\": \"stddev\"\n",
    "    })\\\n",
    "    .withColumnRenamed(\"avg(cmp_plz)\", \"cmp_plz_mean\")\\\n",
    "    .orderBy(col(\"is_match\").desc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1df0f0",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f407f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Currently correlation calculation for columns with dataType string not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcmp_fname_c1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcmp_fname_c2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/dataframe.py:2883\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[0;34m(self, col1, col2, method)\u001b[0m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2879\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently only the calculation of the Pearson Correlation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficient is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m     )\n\u001b[0;32m-> 2883\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Currently correlation calculation for columns with dataType string not supported."
     ]
    }
   ],
   "source": [
    "parsed.corr(\"cmp_fname_c1\", \"cmp_fname_c2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b985f",
   "metadata": {},
   "source": [
    "## Clearning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113dc000",
   "metadata": {},
   "source": [
    "### Remove duplicates\n",
    "\n",
    "* draop_duplicates()\n",
    "* distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61775785",
   "metadata": {},
   "outputs": [],
   "source": [
    "prased = parsed.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d046f1d",
   "metadata": {},
   "source": [
    "### Imputate NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be2c9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|           1|        null|           1|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed = parsed.fillna(0)\n",
    "parsed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c49ad1",
   "metadata": {},
   "source": [
    "---\n",
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bacc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09a51607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+\n",
      "|    cnt|avg_plz|       std(cmp_sex)|\n",
      "+-------+-------+-------------------+\n",
      "|  20931| 0.9571|0.11201570591216435|\n",
      "|5728201|0.00204|0.20755988859217375|\n",
      "|      0|    0.0|               null|\n",
      "+-------+-------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:=============================================>           (8 + 2) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(is_match) AS cnt,\n",
    "    ROUND(AVG(cmp_plz),5) AS avg_plz,\n",
    "    STD(cmp_sex)\n",
    "FROM linkage\n",
    "GROUP BY is_match\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584a6e0",
   "metadata": {},
   "source": [
    "## Join\n",
    "\n",
    "Calculate the diffence of mean values of the fields between matched records and unmatched records to identify the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5556a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------------------+-------------------+-----+-----+\n",
      "|       field|count|              mean|             stddev|  min|  max|\n",
      "+------------+-----+------------------+-------------------+-----+-----+\n",
      "|        id_1|20931| 34575.72117911232|  21950.31285196913|10001|99946|\n",
      "|        id_2|20931| 51259.95939037791|  24345.73345377519|10010|99996|\n",
      "|cmp_fname_c1|20922|0.9973163859635038|0.03650667584833679|    0|    1|\n",
      "+------------+-----+------------------+-------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------+-------+------------------+-------------------+-----+-----+\n",
      "|       field|  count|              mean|             stddev|  min|  max|\n",
      "+------------+-------+------------------+-------------------+-----+-----+\n",
      "|        id_1|5728201|33319.913548075565| 23665.760130330676|    1| 9999|\n",
      "|        id_2|5728201| 66643.44259218557| 23599.551728241313|10000|99999|\n",
      "|cmp_fname_c1|5727203|0.7118634802175091|0.38908060096985553|    0|    1|\n",
      "+------------+-------+------------------+-------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "matched_summary_transposed = transpose(matched_summary)\n",
    "matched_summary_transposed.show(3)\n",
    "\n",
    "unmatched_summary_transposed = transpose(unmatched_summary)\n",
    "unmatched_summary_transposed.show(3)\n",
    "\n",
    "matched_summary_transposed.createOrReplaceTempView(\"matched\")\n",
    "unmatched_summary_transposed.createOrReplaceTempView(\"unmatched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18dba2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+---------+----------+\n",
      "|matched_count|unmatch_count|    total|mean_delta|\n",
      "+-------------+-------------+---------+----------+\n",
      "|        20902|      5715387|5736289.0|     0.956|\n",
      "|          475|         1989|   2464.0|     0.806|\n",
      "|        20925|      5727412|5748337.0|     0.776|\n",
      "|        20925|      5727412|5748337.0|     0.775|\n",
      "|        20931|      5728201|5749132.0|     0.684|\n",
      "|        20925|      5727412|5748337.0|     0.511|\n",
      "|        20922|      5727203|5748125.0|     0.285|\n",
      "|         1333|       102365| 103698.0|     0.091|\n",
      "|        20931|      5728201|5749132.0|     0.032|\n",
      "+-------------+-------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    m.count AS matched_count,\n",
    "    u.count AS unmatch_count,\n",
    "    m.count + u.count as total,\n",
    "    ROUND(m.mean - u.mean, 3) as mean_delta\n",
    "FROM\n",
    "    matched AS m \n",
    "    INNER JOIN unmatched u ON m.field = u.field\n",
    "WHERE\n",
    "    m.field NOT IN ('id_1', 'id_2')\n",
    "ORDER BY \n",
    "    mean_delta desc\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1e5eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1bf4f",
   "metadata": {},
   "source": [
    "# Read JSON\n",
    "\n",
    "* [SparkSQL Guide - JSON Files](https://spark.apache.org/docs/latest/sql-data-sources-json.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "587aef2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- amazon_product_url: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- bestsellers_date: struct (nullable = true)\n",
      " |    |-- $date: struct (nullable = true)\n",
      " |    |    |-- $numberLong: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- price: struct (nullable = true)\n",
      " |    |-- $numberDouble: string (nullable = true)\n",
      " |    |-- $numberInt: string (nullable = true)\n",
      " |-- published_date: struct (nullable = true)\n",
      " |    |-- $date: struct (nullable = true)\n",
      " |    |    |-- $numberLong: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- rank: struct (nullable = true)\n",
      " |    |-- $numberInt: string (nullable = true)\n",
      " |-- rank_last_week: struct (nullable = true)\n",
      " |    |-- $numberInt: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- weeks_on_list: struct (nullable = true)\n",
      " |    |-- $numberInt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preview = spark.read\\\n",
    "    .option(\"compression\", \"none\")\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .json(\"./data/books/nyt2.json\")\n",
    "\n",
    "preview.printSchema()\n",
    "del preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd88020e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- published: date (nullable = true)\n",
      " |-- best_seller_date: date (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n",
      "23/02/09 07:34:30 ERROR Executor: Exception in task 0.0 in stage 65.0 (TID 197)\n",
      "org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2008-06-08 10:00:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1084)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2008-06-08 10:00:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2109)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 17 more\n",
      "23/02/09 07:34:30 WARN TaskSetManager: Lost task 0.0 in stage 65.0 (TID 197) (192.168.1.104 executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2008-06-08 10:00:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1084)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '2008-06-08 10:00:00' could not be parsed, unparsed text found at index 10\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2109)\n",
      "\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n",
      "\t... 17 more\n",
      "\n",
      "23/02/09 07:34:30 ERROR TaskSetManager: Task 0 in stage 65.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o383.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 197) (192.168.1.104 executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2008-06-08 10:00:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1084)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.time.format.DateTimeParseException: Text '2008-06-08 10:00:00' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2109)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2008-06-08 10:00:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1084)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '2008-06-08 10:00:00' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2109)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [43], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m books \u001b[38;5;241m=\u001b[39m spark\\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mread\\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazon_product_url\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m books\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mbooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/dataframe.py:615\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncate=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be either bool or int.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(truncate)\n\u001b[1;32m    613\u001b[0m     )\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.1/libexec/python/lib/pyspark.zip/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/venv/ml/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o383.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 197) (192.168.1.104 executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2008-06-08 10:00:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1084)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.time.format.DateTimeParseException: Text '2008-06-08 10:00:00' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2109)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '2008-06-08 10:00:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:1084)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:176)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '2008-06-08 10:00:00' could not be parsed, unparsed text found at index 10\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2109)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1934)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:168)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "books = spark\\\n",
    "    .read\\\n",
    "    .option(\"compression\", \"none\")\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .json(\"./data/books/nyt2.json\")\\\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"author\",\n",
    "        col(\"price.$numberDouble\").alias(\"price\"),    \n",
    "        to_date(\n",
    "            from_unixtime(col(\"published_date.$date.$numberLong\") / 1000),\n",
    "            \"yyyy-MM-dd\"\n",
    "        ).alias(\"published\"),\n",
    "        to_date(\n",
    "            from_unixtime(col(\"bestsellers_date.$date.$numberLong\") / 1000),\n",
    "            \"yyyy-MM-dd\"\n",
    "        ).alias(\"best_seller_date\"),\n",
    "        col(\"amazon_product_url\").alias(\"url\")\n",
    "    )\n",
    "books.printSchema()\n",
    "books.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06249d6b",
   "metadata": {},
   "source": [
    "## Extract nested JSON elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edb86919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- published: date (nullable = true)\n",
      " |-- best_seller_date: date (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n",
      "+--------------------------------------------------+----------------------------------------+-----+----------+----------------+--------------------------------------------------------------------------------------------------+\n",
      "|title                                             |author                                  |price|published |best_seller_date|url                                                                                               |\n",
      "+--------------------------------------------------+----------------------------------------+-----+----------+----------------+--------------------------------------------------------------------------------------------------+\n",
      "|ODD HOURS                                         |Dean R Koontz                           |null |2008-06-08|2008-05-24      |http://www.amazon.com/Odd-Hours-Dean-Koontz/dp/0553807056?tag=NYTBS-20                            |\n",
      "|THE HOST                                          |Stephenie Meyer                         |25.99|2008-06-08|2008-05-24      |http://www.amazon.com/The-Host-Novel-Stephenie-Meyer/dp/0316218502?tag=NYTBS-20                   |\n",
      "|LOVE THE ONE YOU'RE WITH                          |Emily Giffin                            |24.95|2008-06-08|2008-05-24      |http://www.amazon.com/Love-Youre-With-Emily-Giffin/dp/0312348665?tag=NYTBS-20                     |\n",
      "|THE FRONT                                         |Patricia Cornwell                       |22.95|2008-06-08|2008-05-24      |http://www.amazon.com/The-Front-Garano-Patricia-Cornwell-ebook/dp/B0017T0C9M?tag=NYTBS-20         |\n",
      "|SNUFF                                             |Chuck Palahniuk                         |24.95|2008-06-08|2008-05-24      |http://www.amazon.com/Snuff-Chuck-Palahniuk/dp/0385517882?tag=NYTBS-20                            |\n",
      "|SUNDAYS AT TIFFANY’S                              |James Patterson and Gabrielle Charbonnet|24.99|2008-06-08|2008-05-24      |http://www.amazon.com/Sundays-at-Tiffanys-James-Patterson/dp/0446199443?tag=NYTBS-20              |\n",
      "|PHANTOM PREY                                      |John Sandford                           |26.95|2008-06-08|2008-05-24      |http://www.amazon.com/Phantom-Prey-John-Sandford/dp/0425227987?tag=NYTBS-20                       |\n",
      "|SWINE NOT?                                        |Jimmy Buffett                           |21.99|2008-06-08|2008-05-24      |http://www.amazon.com/From-Worse-Southern-Vampire-Mysteries/dp/0441015891?tag=NYTBS-20            |\n",
      "|CARELESS IN RED                                   |Elizabeth George                        |27.95|2008-06-08|2008-05-24      |http://www.amazon.com/Where-Are-You-Now-Novel/dp/1416566384?tag=NYTBS-20                          |\n",
      "|THE WHOLE TRUTH                                   |David Baldacci                          |26.99|2008-06-08|2008-05-24      |http://www.amazon.com/The-Whole-Truth-David-Baldacci/dp/0446539686?tag=NYTBS-20                   |\n",
      "|INVINCIBLE                                        |Troy Denning                            |null |2008-06-08|2008-05-24      |http://www.amazon.com/Careless-Red-Novel-Elizabeth-George/dp/0061160873?tag=NYTBS-20              |\n",
      "|BRIGHT SHINY MORNING                              |James Frey                              |26.95|2008-06-08|2008-05-24      |http://www.amazon.com/Unaccustomed-Earth-Jhumpa-Lahiri/dp/0307265730?tag=NYTBS-20                 |\n",
      "|THE ART OF RACING IN THE RAIN                     |Garth Stein                             |23.95|2008-06-08|2008-05-24      |http://www.amazon.com/The-Racing-Rain-Garth-Stein-ebook/dp/B0017SWPXY?tag=NYTBS-20                |\n",
      "|TWENTY WISHES                                     |Debbie Macomber                         |24.95|2008-06-08|2008-05-24      |http://www.amazon.com/Twenty-Wishes-Blossom-Street-Books/dp/0778326314?tag=NYTBS-20               |\n",
      "|THE STEEL WAVE                                    |Jeff Shaara                             |null |2008-06-08|2008-05-24      |http://www.amazon.com/Hold-Tight-Harlan-Coben/dp/0525950605?tag=NYTBS-20                          |\n",
      "|EXECUTIVE PRIVILEGE                               |Phillip Margolin                        |null |2008-06-08|2008-05-24      |http://www.amazon.com/Executive-Privilege-Novel-Phillip-Margolin/dp/0061236217?tag=NYTBS-20       |\n",
      "|UNACCUSTOMED EARTH                                |Jhumpa Lahiri                           |null |2008-06-08|2008-05-24      |http://www.amazon.com/Unaccustomed-Vintage-Contemporaries-Jhumpa-Lahiri/dp/0307278255?tag=NYTBS-20|\n",
      "|NETHERLAND                                        |Joseph O'Neill                          |null |2008-06-08|2008-05-24      |http://www.amazon.com/Netherland-A-Novel-Joseph-ONeill/dp/0307377040?tag=NYTBS-20                 |\n",
      "|THE APPEAL                                        |John Grisham                            |null |2008-06-08|2008-05-24      |http://www.amazon.com/The-Appeal-John-Grisham/dp/0440243815?tag=NYTBS-20                          |\n",
      "|INDIANA JONES AND THE KINGDOM OF THE CRYSTAL SKULL|James Rollins                           |null |2008-06-08|2008-05-24      |http://www.amazon.com/Indiana-Jones-Kingdom-Crystal-Skull/dp/0345501284?tag=NYTBS-20              |\n",
      "+--------------------------------------------------+----------------------------------------+-----+----------+----------------+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books = spark\\\n",
    "    .read\\\n",
    "    .option(\"compression\", \"none\")\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .json(\"./data/books/nyt2.json\")\\\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"author\",\n",
    "        col(\"price.$numberDouble\").alias(\"price\"),    \n",
    "        to_date(\n",
    "            from_unixtime(col(\"published_date.$date.$numberLong\") / 1000),\n",
    "            \"yyyy-MM-dd\"\n",
    "        ).alias(\"published\"),\n",
    "        to_date(\n",
    "            from_unixtime(col(\"bestsellers_date.$date.$numberLong\") / 1000),\n",
    "            \"yyyy-MM-dd\"\n",
    "        ).alias(\"best_seller_date\"),\n",
    "        col(\"amazon_product_url\").alias(\"url\")\n",
    "    )\n",
    "books.printSchema()\n",
    "books.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c2b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3aabbe9",
   "metadata": {},
   "source": [
    "## Conditional String Filering on Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2296a4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "|               title|      author|price| published|best_seller_date|                 url|\n",
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "| THE SPIES OF WARSAW|  Alan Furst| null|2008-06-22|      2008-06-07|http://www.amazon...|\n",
      "| THE SPIES OF WARSAW|  Alan Furst| null|2008-06-29|      2008-06-14|http://www.amazon...|\n",
      "| THE SPIES OF WARSAW|  Alan Furst| null|2008-07-06|      2008-06-21|http://www.amazon...|\n",
      "|SPIES OF THE BALKANS|  Alan Furst| null|2010-07-04|      2010-06-20|http://www.amazon...|\n",
      "|SPIES OF THE BALKANS|  Alan Furst| null|2010-07-11|      2010-06-27|http://www.amazon...|\n",
      "|A RED HERRING WIT...|Alan Bradley| null|2011-02-27|      2011-02-12|http://www.amazon...|\n",
      "|I AM HALF-SICK OF...|Alan Bradley| null|2011-11-20|      2011-11-05|http://www.amazon...|\n",
      "|    MISSION TO PARIS|  Alan Furst| null|2012-07-01|      2012-06-16|http://www.amazon...|\n",
      "|    MISSION TO PARIS|  Alan Furst| null|2012-07-08|      2012-06-23|http://www.amazon...|\n",
      "|    MISSION TO PARIS|  Alan Furst| null|2012-07-15|      2012-06-30|http://www.amazon...|\n",
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books.where(col(\"author\").like(\"Alan%\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35f8f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "|               title|      author|price| published|best_seller_date|                 url|\n",
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "| THE SPIES OF WARSAW|  Alan Furst| null|2008-06-22|      2008-06-07|http://www.amazon...|\n",
      "| THE SPIES OF WARSAW|  Alan Furst| null|2008-06-29|      2008-06-14|http://www.amazon...|\n",
      "| THE SPIES OF WARSAW|  Alan Furst| null|2008-07-06|      2008-06-21|http://www.amazon...|\n",
      "|SPIES OF THE BALKANS|  Alan Furst| null|2010-07-04|      2010-06-20|http://www.amazon...|\n",
      "|SPIES OF THE BALKANS|  Alan Furst| null|2010-07-11|      2010-06-27|http://www.amazon...|\n",
      "|A RED HERRING WIT...|Alan Bradley| null|2011-02-27|      2011-02-12|http://www.amazon...|\n",
      "|I AM HALF-SICK OF...|Alan Bradley| null|2011-11-20|      2011-11-05|http://www.amazon...|\n",
      "|    MISSION TO PARIS|  Alan Furst| null|2012-07-01|      2012-06-16|http://www.amazon...|\n",
      "|    MISSION TO PARIS|  Alan Furst| null|2012-07-08|      2012-06-23|http://www.amazon...|\n",
      "|    MISSION TO PARIS|  Alan Furst| null|2012-07-15|      2012-06-30|http://www.amazon...|\n",
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books.where(col(\"author\").rlike(\"^Alan [BF].*\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cad66fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "|               title|      author|price| published|best_seller_date|                 url|\n",
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "|A RED HERRING WIT...|Alan Bradley| null|2011-02-27|      2011-02-12|http://www.amazon...|\n",
      "|I AM HALF-SICK OF...|Alan Bradley| null|2011-11-20|      2011-11-05|http://www.amazon...|\n",
      "|SPEAKING FROM AMO...|Alan Bradley| null|2013-02-17|      2013-02-02|http://www.amazon...|\n",
      "|THE DEAD IN THEIR...|Alan Bradley| null|2014-02-02|      2014-01-18|http://www.amazon...|\n",
      "|AS CHIMNEY SWEEPE...|Alan Bradley| null|2015-01-25|      2015-01-10|http://www.amazon...|\n",
      "+--------------------+------------+-----+----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books.where(\n",
    "    col(\"author\").startswith(\"Alan\") & col(\"author\").endswith(\"Bradley\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b3423",
   "metadata": {},
   "source": [
    "---\n",
    "# Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1455df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce1c6c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "838583c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4289"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bce46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
