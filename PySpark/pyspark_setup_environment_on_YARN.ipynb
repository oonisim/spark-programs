{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# Setup the environment to submit PySpark jobs on YARN\n",
    "\n",
    "The basis is to submit Spark jobs to a Spark cluster. \n",
    "\n",
    "* [Submitting pyspark script to a remote Spark server?\n",
    "](https://stackoverflow.com/questions/54641574/submitting-pyspark-script-to-a-remote-spark-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317443d",
   "metadata": {},
   "source": [
    "---\n",
    "# HDFS permission\n",
    "\n",
    "For a non-spark user to be able to submit a job, login to the HDFS node as the hadoop user to run:\n",
    "\n",
    "```\n",
    "hadoop fs -mkdir /user/${USERNAME}\n",
    "hadoop fs -chown ${USERNAME} /user/${USERNAME}\n",
    "hadoop fs -chmod g+w /user/${USERNAME}\n",
    "```\n",
    "\n",
    "Otherwise an error:\n",
    "```\n",
    "21/08/15 21:15:28 ERROR SparkContext: Error initializing SparkContext.\n",
    "org.apache.hadoop.security.AccessControlException: Permission denied: user=${USERNAME}, access=WRITE, inode=\"/user\":hadoop:hadoop:drwxrwxr-x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345d990",
   "metadata": {},
   "source": [
    "---\n",
    "# Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d50fc",
   "metadata": {},
   "source": [
    "## HADOOP_CONF_DIR\n",
    "\n",
    "Copy the **HADOOP_CONF_DIR** from the Hadoop/YARN master node and set the ```HADOOP_CONF_DIR``` environment variable locally to point to the directory.\n",
    "\n",
    "* [Launching Spark on YARN\n",
    "](http://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn)\n",
    "\n",
    "> Ensure that **HADOOP_CONF_DIR** or **YARN_CONF_DIR** points to the directory which contains the (client side) configuration files for the Hadoop cluster. These configs are used to write to HDFS and connect to the YARN ResourceManager. The configuration contained in this directory will be distributed to the YARN cluster so that all containers used by the application use the same configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50939d",
   "metadata": {},
   "source": [
    "## HADOOP_CONF_DIR access permission\n",
    "\n",
    "* [spark - java.io.FileNotFoundException: File file:/home/user/.sparkStaging/](https://stackoverflow.com/a/71173475/4281353)\n",
    "\n",
    "Make sure the user who submits the spark job can access the files under ```HADDOP_CONF_DIR```.\n",
    "\n",
    "If there is no access:\n",
    "\n",
    "```\n",
    "$ ls $HADOOP_CONF_DIR\n",
    "ls: cannot access '/opt/hadoop/hadoop-3.2.2/etc/hadoop': Permission denied\n",
    "```\n",
    "\n",
    "Then you will see the error:\n",
    "\n",
    "```\n",
    "Failing this attempt.Diagnostics: [2022-02-18 22:27:36.433]File file:$HOME/.sparkStaging/application_1645180452555_0008/__spark_libs__7275973196548620925.zip does not exist\n",
    "java.io.FileNotFoundException: File file:$HOME/.sparkStaging/application_1645180452555_0008/__spark_libs__7275973196548620925.zip does not exist\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "Failing this attempt.Diagnostics: [2022-02-18 21:35:35.917]File file:/home/oonisim/.sparkStaging/application_1645180452555_0001/pyspark.zip does not exist\n",
    "java.io.FileNotFoundException: File file:$HOME/sparkStaging/application_1645180452555_0001/pyspark.zip does not exist\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e14ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\n",
      "configuration.xsl\n",
      "container-executor.cfg\n",
      "core-site.xml\n",
      "core-site.xml.48132.2022-02-15@12:29:41~\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=/opt/hadoop/hadoop-3.2.2/etc/hadoop\n",
    "ls $HADOOP_CONF_DIR | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6159b3",
   "metadata": {},
   "source": [
    "## PYSPARK_SUBMIT_ARGS\n",
    "\n",
    "Specify the [spark-submit](https://spark.apache.org/docs/3.1.2/submitting-applications.html#launching-applications-with-spark-submit) parameters.\n",
    "\n",
    "```\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```\n",
    "\n",
    "The ```conf``` paramters are [Spark properties](https://spark.apache.org/docs/latest/configuration.html#available-properties) e.g. ```spark.executor.memory```\n",
    "\n",
    "Alternatively, use [SparkSession.builder](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession).\n",
    "\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "./bin/spark-submit \\\n",
    "  --class org.apache.spark.examples.SparkPi \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --supervise \\\n",
    "  --executor-memory 20G \\\n",
    "  --total-executor-cores 100 \\\n",
    "  /path/to/examples.jar \\\n",
    "  1000\n",
    "```\n",
    "\n",
    "### Environment variable\n",
    "\n",
    "```\n",
    "export PYSPARK_SUBMIT_ARGS='--master yarn --executor-memory 20G --total-executor-cores 100 --num-executors 5 --driver-memory 2g --executor-memory 2g pyspark-submit'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a10d4",
   "metadata": {},
   "source": [
    "# Test spark-submit command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec515378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 23:17:36,473 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-02-18 23:17:36,832 INFO spark.SparkContext: Running Spark version 3.1.2\n",
      "2022-02-18 23:17:36,947 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-02-18 23:17:36,948 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2022-02-18 23:17:36,949 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-02-18 23:17:36,951 INFO spark.SparkContext: Submitted application: Spark Pi\n",
      "2022-02-18 23:17:37,089 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2022-02-18 23:17:37,139 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2022-02-18 23:17:37,146 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2022-02-18 23:17:37,271 INFO spark.SecurityManager: Changing view acls to: oonisim\n",
      "2022-02-18 23:17:37,271 INFO spark.SecurityManager: Changing modify acls to: oonisim\n",
      "2022-02-18 23:17:37,271 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2022-02-18 23:17:37,271 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2022-02-18 23:17:37,272 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oonisim); groups with view permissions: Set(); users  with modify permissions: Set(oonisim); groups with modify permissions: Set()\n",
      "2022-02-18 23:17:37,656 INFO util.Utils: Successfully started service 'sparkDriver' on port 46463.\n",
      "2022-02-18 23:17:37,707 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2022-02-18 23:17:37,784 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2022-02-18 23:17:37,868 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2022-02-18 23:17:37,870 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2022-02-18 23:17:37,942 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2022-02-18 23:17:37,984 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-4eb8bf4a-d1e9-4b4a-9ac5-0deede2f8ade\n",
      "2022-02-18 23:17:38,035 INFO memory.MemoryStore: MemoryStore started with capacity 912.3 MiB\n",
      "2022-02-18 23:17:38,134 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2022-02-18 23:17:38,372 INFO util.log: Logging initialized @3574ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2022-02-18 23:17:38,598 INFO server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07\n",
      "2022-02-18 23:17:38,652 INFO server.Server: Started @3858ms\n",
      "2022-02-18 23:17:38,744 INFO server.AbstractConnector: Started ServerConnector@6b85300e{HTTP/1.1, (http/1.1)}{127.0.1.1:4040}\n",
      "2022-02-18 23:17:38,744 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2022-02-18 23:17:38,818 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61a5b4ae{/jobs,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,823 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62577d6{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,826 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b5f8707{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,834 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e5bfdfc{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,836 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71652c98{/stages,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,838 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b85ba1{/stages/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,841 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@117632cf{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,845 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a1d3c1a{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,846 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@159e366{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,848 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24528a25{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,850 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59221b97{/storage,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,852 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a772895{/storage/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,854 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@704b2127{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,856 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d332969{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,858 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e27d72f{/environment,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,861 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4837595f{/environment/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,863 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b718392{/executors,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,865 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f2d2181{/executors/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,868 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ee55e70{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,870 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7668d560{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,900 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@126be319{/static,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a67318f{/,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,906 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17f9344b{/api,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,908 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44ea608c{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,911 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@450794b4{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:38,918 INFO ui.SparkUI: Bound SparkUI to 127.0.1.1, and started at http://ubuntu:4040\n",
      "2022-02-18 23:17:38,958 INFO spark.SparkContext: Added JAR file:/opt/spark/spark-3.1.2/examples/jars/spark-examples_2.12-3.1.2.jar at spark://ubuntu:46463/jars/spark-examples_2.12-3.1.2.jar with timestamp 1645186656818\n",
      "2022-02-18 23:17:39,556 INFO client.RMProxy: Connecting to ResourceManager at ubuntu/127.0.1.1:8032\n",
      "2022-02-18 23:17:39,919 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2022-02-18 23:17:40,569 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-02-18 23:17:40,570 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-02-18 23:17:40,600 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
      "2022-02-18 23:17:40,602 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2022-02-18 23:17:40,603 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2022-02-18 23:17:40,603 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2022-02-18 23:17:40,617 INFO yarn.Client: Preparing resources for our AM container\n",
      "2022-02-18 23:17:40,783 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2022-02-18 23:17:44,478 INFO yarn.Client: Uploading resource file:/tmp/spark-1858b913-f59c-41a5-8931-d0ca9ccdb66f/__spark_libs__3755746956498147110.zip -> hdfs://ubuntu:8020/user/oonisim/.sparkStaging/application_1645186583689_0002/__spark_libs__3755746956498147110.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 23:17:46,808 INFO yarn.Client: Uploading resource file:/tmp/spark-1858b913-f59c-41a5-8931-d0ca9ccdb66f/__spark_conf__5166803083425083188.zip -> hdfs://ubuntu:8020/user/oonisim/.sparkStaging/application_1645186583689_0002/__spark_conf__.zip\n",
      "2022-02-18 23:17:46,935 INFO spark.SecurityManager: Changing view acls to: oonisim\n",
      "2022-02-18 23:17:46,935 INFO spark.SecurityManager: Changing modify acls to: oonisim\n",
      "2022-02-18 23:17:46,935 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2022-02-18 23:17:46,935 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2022-02-18 23:17:46,936 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oonisim); groups with view permissions: Set(); users  with modify permissions: Set(oonisim); groups with modify permissions: Set()\n",
      "2022-02-18 23:17:47,011 INFO yarn.Client: Submitting application application_1645186583689_0002 to ResourceManager\n",
      "2022-02-18 23:17:47,532 INFO impl.YarnClientImpl: Submitted application application_1645186583689_0002\n",
      "2022-02-18 23:17:48,539 INFO yarn.Client: Application report for application_1645186583689_0002 (state: ACCEPTED)\n",
      "2022-02-18 23:17:48,546 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1645186667231\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ubuntu:8088/proxy/application_1645186583689_0002/\n",
      "\t user: oonisim\n",
      "2022-02-18 23:17:49,550 INFO yarn.Client: Application report for application_1645186583689_0002 (state: ACCEPTED)\n",
      "2022-02-18 23:17:50,554 INFO yarn.Client: Application report for application_1645186583689_0002 (state: ACCEPTED)\n",
      "2022-02-18 23:17:51,561 INFO yarn.Client: Application report for application_1645186583689_0002 (state: ACCEPTED)\n",
      "2022-02-18 23:17:52,565 INFO yarn.Client: Application report for application_1645186583689_0002 (state: ACCEPTED)\n",
      "2022-02-18 23:17:53,574 INFO yarn.Client: Application report for application_1645186583689_0002 (state: ACCEPTED)\n",
      "2022-02-18 23:17:54,578 INFO yarn.Client: Application report for application_1645186583689_0002 (state: ACCEPTED)\n",
      "2022-02-18 23:17:54,891 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ubuntu, PROXY_URI_BASES -> http://ubuntu:8088/proxy/application_1645186583689_0002), /proxy/application_1645186583689_0002\n",
      "2022-02-18 23:17:55,582 INFO yarn.Client: Application report for application_1645186583689_0002 (state: RUNNING)\n",
      "2022-02-18 23:17:55,583 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 192.168.13.129\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1645186667231\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ubuntu:8088/proxy/application_1645186583689_0002/\n",
      "\t user: oonisim\n",
      "2022-02-18 23:17:55,585 INFO cluster.YarnClientSchedulerBackend: Application application_1645186583689_0002 has started running.\n",
      "2022-02-18 23:17:55,648 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36329.\n",
      "2022-02-18 23:17:55,648 INFO netty.NettyBlockTransferService: Server created on ubuntu:36329\n",
      "2022-02-18 23:17:55,654 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2022-02-18 23:17:55,690 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 36329, None)\n",
      "2022-02-18 23:17:55,703 INFO storage.BlockManagerMasterEndpoint: Registering block manager ubuntu:36329 with 912.3 MiB RAM, BlockManagerId(driver, ubuntu, 36329, None)\n",
      "2022-02-18 23:17:55,712 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 36329, None)\n",
      "2022-02-18 23:17:55,716 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 36329, None)\n",
      "2022-02-18 23:17:56,082 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2022-02-18 23:17:56,175 INFO ui.ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "2022-02-18 23:17:56,180 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@470d183{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2022-02-18 23:17:56,244 INFO history.SingleEventLogFileWriter: Logging events to hdfs://ubuntu:8020/logs_spark/application_1645186583689_0002.inprogress\n",
      "2022-02-18 23:18:02,477 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:56196) with ID 1,  ResourceProfileId 0\n",
      "2022-02-18 23:18:02,784 INFO storage.BlockManagerMasterEndpoint: Registering block manager ubuntu:35703 with 366.3 MiB RAM, BlockManagerId(1, ubuntu, 35703, None)\n",
      "2022-02-18 23:18:03,374 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:56200) with ID 2,  ResourceProfileId 0\n",
      "2022-02-18 23:18:03,396 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2022-02-18 23:18:03,663 INFO storage.BlockManagerMasterEndpoint: Registering block manager ubuntu:38153 with 366.3 MiB RAM, BlockManagerId(2, ubuntu, 38153, None)\n",
      "2022-02-18 23:18:04,134 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:38\n",
      "2022-02-18 23:18:04,175 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n",
      "2022-02-18 23:18:04,178 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
      "2022-02-18 23:18:04,180 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-02-18 23:18:04,182 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-02-18 23:18:04,210 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
      "2022-02-18 23:18:04,412 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.1 KiB, free 912.3 MiB)\n",
      "2022-02-18 23:18:04,991 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1787.0 B, free 912.3 MiB)\n",
      "2022-02-18 23:18:04,995 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:36329 (size: 1787.0 B, free: 912.3 MiB)\n",
      "2022-02-18 23:18:05,001 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1388\n",
      "2022-02-18 23:18:05,054 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "2022-02-18 23:18:05,057 INFO cluster.YarnScheduler: Adding task set 0.0 with 10 tasks resource profile 0\n",
      "2022-02-18 23:18:05,163 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu, executor 2, partition 0, PROCESS_LOCAL, 4346 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:05,171 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ubuntu, executor 1, partition 1, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:06,856 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:38153 (size: 1787.0 B, free: 366.3 MiB)\n",
      "2022-02-18 23:18:07,193 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:35703 (size: 1787.0 B, free: 366.3 MiB)\n",
      "2022-02-18 23:18:08,453 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ubuntu, executor 2, partition 2, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:08,510 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3355 ms on ubuntu (executor 2) (1/10)\n",
      "2022-02-18 23:18:08,565 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (ubuntu, executor 2, partition 3, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:08,571 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 120 ms on ubuntu (executor 2) (2/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 23:18:08,871 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (ubuntu, executor 2, partition 4, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:08,892 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 329 ms on ubuntu (executor 2) (3/10)\n",
      "2022-02-18 23:18:08,984 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (ubuntu, executor 2, partition 5, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:08,989 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 119 ms on ubuntu (executor 2) (4/10)\n",
      "2022-02-18 23:18:09,132 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (ubuntu, executor 2, partition 6, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:09,152 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 170 ms on ubuntu (executor 2) (5/10)\n",
      "2022-02-18 23:18:09,275 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (ubuntu, executor 2, partition 7, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:09,283 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 155 ms on ubuntu (executor 2) (6/10)\n",
      "2022-02-18 23:18:09,529 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (ubuntu, executor 2, partition 8, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:09,530 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 257 ms on ubuntu (executor 2) (7/10)\n",
      "2022-02-18 23:18:09,590 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (ubuntu, executor 1, partition 9, PROCESS_LOCAL, 4348 bytes) taskResourceAssignments Map()\n",
      "2022-02-18 23:18:09,594 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4424 ms on ubuntu (executor 1) (8/10)\n",
      "2022-02-18 23:18:09,709 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 121 ms on ubuntu (executor 1) (9/10)\n",
      "2022-02-18 23:18:09,943 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 415 ms on ubuntu (executor 2) (10/10)\n",
      "2022-02-18 23:18:09,947 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2022-02-18 23:18:09,962 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 5.676 s\n",
      "2022-02-18 23:18:09,999 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-02-18 23:18:10,002 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "2022-02-18 23:18:10,015 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 5.880288 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.1376351376351375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 23:18:10,457 INFO server.AbstractConnector: Stopped Spark@6b85300e{HTTP/1.1, (http/1.1)}{127.0.1.1:4040}\n",
      "2022-02-18 23:18:10,464 INFO ui.SparkUI: Stopped Spark web UI at http://ubuntu:4040\n",
      "2022-02-18 23:18:10,735 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:38153 in memory (size: 1787.0 B, free: 366.3 MiB)\n",
      "2022-02-18 23:18:10,737 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:36329 in memory (size: 1787.0 B, free: 912.3 MiB)\n",
      "2022-02-18 23:18:10,744 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:35703 in memory (size: 1787.0 B, free: 366.3 MiB)\n",
      "2022-02-18 23:18:10,943 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "2022-02-18 23:18:11,031 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\n",
      "2022-02-18 23:18:11,034 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "2022-02-18 23:18:11,063 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n",
      "2022-02-18 23:18:11,601 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2022-02-18 23:18:11,636 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2022-02-18 23:18:11,636 INFO storage.BlockManager: BlockManager stopped\n",
      "2022-02-18 23:18:11,668 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2022-02-18 23:18:11,677 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2022-02-18 23:18:11,760 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "2022-02-18 23:18:11,816 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2022-02-18 23:18:11,819 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1858b913-f59c-41a5-8931-d0ca9ccdb66f\n",
      "2022-02-18 23:18:11,829 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-2c5d2769-bb1f-460b-8bdd-da1cc5f0a49e\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HADOOP_CONF_DIR=/opt/hadoop/hadoop-3.2.2/etc/hadoop\n",
    "export HADOOP_HOME=/opt/hadoop/hadoop-3.2.2\n",
    "export SPARK_MASTER=yarn\n",
    "export SPARK_HOME=/opt/spark/spark-3.1.2\n",
    "export SPARK_DEPLOY_MODE=cluster\n",
    "export SPARK_EXAMPLE_JAR=\"spark-examples_2.12-3.1.2.jar\"\n",
    "\n",
    "$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n",
    "  $SPARK_HOME/examples/jars/$SPARK_EXAMPLE_JAR 10 \\\n",
    "  --master $SPARK_MASTER \\\n",
    "  --deploy-mode $SPARK_DEPLOY_MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eda802",
   "metadata": {},
   "source": [
    "---\n",
    "# Build Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e37afc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528b4ac",
   "metadata": {},
   "source": [
    "---\n",
    "# PySprk Code Example\n",
    "\n",
    "* [PySpark - QuickStart](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb767f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f30c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
