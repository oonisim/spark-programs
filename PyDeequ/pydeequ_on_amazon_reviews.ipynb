{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a609ffd2",
   "metadata": {},
   "source": [
    "# Setup the environment to submit PySpark jobs on YARN\n",
    "\n",
    "The basis is to submit Spark jobs to a Spark cluster. \n",
    "\n",
    "* [How to submit a PyDeequ job from Jupyter Notebook to a Spark/YARN\n",
    "](https://stackoverflow.com/questions/68796543/how-to-use-a-pydeequ-job-from-jupyter-notebook-to-a-spark-yarn/68796544#68796544)\n",
    "* [Submitting pyspark script to a remote Spark server?\n",
    "](https://stackoverflow.com/questions/54641574/submitting-pyspark-script-to-a-remote-spark-server)\n",
    "\n",
    "* [Test data quality at scale with Deequ](https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/)\n",
    "\n",
    ">  Deequ, an open source tool developed and used at Amazon. Deequ allows you to calculate data quality metrics on your dataset, define and verify data quality constraints, and be informed about changes in the data distribution. Instead of implementing checks and verification algorithms on your own, you can focus on describing how your data should look. Deequ supports you by suggesting checks for you. Deequ is implemented on top of Apache Spark and is designed to scale with large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse.\n",
    "\n",
    "* [AWS labs PyDeequ](https://github.com/awslabs/python-deequ)\n",
    "\n",
    "> PyDeequ is a Python API for Deequ, a library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets. PyDeequ is written to support usage of Deequ in Python.\n",
    "\n",
    "> <img src=\"image/pydeequ_architecture.jpg\" align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e765d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e421f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.realpath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317443d",
   "metadata": {},
   "source": [
    "---\n",
    "# HDFS permission\n",
    "\n",
    "For a non-spark user to be able to submit a job, login to the HDFS node as the hadoop user to run:\n",
    "\n",
    "```\n",
    "hadoop fs -mkdir /user/${USERNAME}\n",
    "hadoop fs -chown ${USERNAME} /user/${USERNAME}\n",
    "hadoop fs -chmod g+w /user/${USERNAME}\n",
    "```\n",
    "\n",
    "Otherwise an error:\n",
    "```\n",
    "21/08/15 21:15:28 ERROR SparkContext: Error initializing SparkContext.\n",
    "org.apache.hadoop.security.AccessControlException: Permission denied: user=${USERNAME}, access=WRITE, inode=\"/user\":hadoop:hadoop:drwxrwxr-x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345d990",
   "metadata": {},
   "source": [
    "---\n",
    "# Environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d50fc",
   "metadata": {},
   "source": [
    "## HADOOP_CONF_DIR\n",
    "\n",
    "Copy the **HADOOP_CONF_DIR** from the Hadoop/YARN master node and set the ```HADOOP_CONF_DIR``` environment variable locally to point to the directory.\n",
    "\n",
    "* [Launching Spark on YARN\n",
    "](http://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn)\n",
    "\n",
    "> Ensure that **HADOOP_CONF_DIR** or **YARN_CONF_DIR** points to the directory which contains the (client side) configuration files for the Hadoop cluster. These configs are used to write to HDFS and connect to the YARN ResourceManager. The configuration contained in this directory will be distributed to the YARN cluster so that all containers used by the application use the same configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ee72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR'] = \"/opt/hadoop/hadoop-3.2.2/etc/hadoop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ea383",
   "metadata": {},
   "source": [
    "## PYTHONPATH\n",
    "\n",
    "Refer to the **pyspark** modules to load from the ```$SPARK_HOME/python/lib``` in the Spark installation.\n",
    "\n",
    "* [PySpark Getting Started](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "\n",
    "> Ensure the SPARK_HOME environment variable points to the directory where the tar file has been extracted. Update PYTHONPATH environment variable such that it can find the PySpark and Py4J under SPARK_HOME/python/lib. One example of doing this is shown below:\n",
    "\n",
    "```\n",
    "export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "```\n",
    "\n",
    "Alternatively install **pyspark** with pip or conda locally which installs the Spark runtime libararies (for standalone).\n",
    "\n",
    "* [Can PySpark work without Spark?](https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark)\n",
    "\n",
    "> As of v2.2, executing pip install pyspark will install Spark. If you're going to use Pyspark it's clearly the simplest way to get started. On my system Spark is installed inside my virtual environment (miniconda) at lib/python3.6/site-packages/pyspark/jars  \n",
    "> PySpark has a Spark installation installed. If installed through pip3, you can find it with pip3 show pyspark. Ex. for me it is at ~/.local/lib/python3.8/site-packages/pyspark. This is a standalone configuration so it can't be used for managing clusters like a full Spark installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fbbd15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTHONPATH'] = \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip:/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "sys.path.extend([\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/py4j-0.10.9-src.zip\",\n",
    "    \"/opt/spark/spark-3.1.2/python/lib/pyspark.zip\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6159b3",
   "metadata": {},
   "source": [
    "## PYSPARK_SUBMIT_ARGS\n",
    "\n",
    "Specify the [spark-submit](https://spark.apache.org/docs/3.1.2/submitting-applications.html#launching-applications-with-spark-submit) parameters.\n",
    "\n",
    "```\n",
    "./bin/spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```\n",
    "\n",
    "The ```conf``` paramters are [Spark properties](https://spark.apache.org/docs/latest/configuration.html#available-properties) e.g. ```spark.executor.memory```\n",
    "\n",
    "Alternatively, use [SparkSession.builder](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession).\n",
    "\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "./bin/spark-submit \\\n",
    "  --class org.apache.spark.examples.SparkPi \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --supervise \\\n",
    "  --executor-memory 20G \\\n",
    "  --total-executor-cores 100 \\\n",
    "  /path/to/examples.jar \\\n",
    "  1000\n",
    "```\n",
    "\n",
    "### Environment variable\n",
    "\n",
    "```\n",
    "export PYSPARK_SUBMIT_ARGS='--master yarn --executor-memory 20G --total-executor-cores 100 --num-executors 5 --driver-memory 2g --executor-memory 2g pyspark-submit'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1a266d",
   "metadata": {},
   "source": [
    "---\n",
    "# PyDeequ\n",
    "\n",
    "* [Deequ - Unit Tests for Data](https://github.com/awslabs/deequ)\n",
    "\n",
    ">Deequ is a library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets. We are happy to receive feedback and contributions.  \n",
    ">Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on GitHub, readthedocs, and PyPI.\n",
    "\n",
    "* [Maven repository com.amazon.deequ](https://mvnrepository.com/artifact/com.amazon.deequ/deequ)\n",
    "\n",
    "* [Testing data quality at scale with PyDeequ](https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df41fc",
   "metadata": {},
   "source": [
    "## Deequ jar to classpath\n",
    "To use the PyDeequ, need the deequ jar file. Download the one for the Spark/Deequ version from the [Maven repository com.amazon.deequ](https://mvnrepository.com/artifact/com.amazon.deequ/deequ).\n",
    "\n",
    "Specify them to the Spark jar properties as specified in:\n",
    "\n",
    "* [Deequ Analyzers Basic Tutorial](https://github.com/awslabs/python-deequ/blob/master/tutorials/analyzers.ipynb)\n",
    "* [TypeError: 'JavaPackage' object is not callable when running pydeequ](https://github.com/awslabs/python-deequ/issues/1)\n",
    "\n",
    "Note that need all of them. Missing any will cause errors.\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\           <-----\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord) \\   <-----\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord) \\     <-----\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The Spark session creation in the documentation does NOT work. Need to add the downloaded jar file.\n",
    "\n",
    "* [Set up a PySpark session](https://pydeequ.readthedocs.io/en/latest/README.html#set-up-a-pyspark-session)\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pydeequ\n",
    "\n",
    "spark = (SparkSession  <----- Causes \"TypeError: 'JavaPackage' object is not callable when running pydeequ\"\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47aac203",
   "metadata": {},
   "outputs": [],
   "source": [
    "deequ_jar = \"https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.0-spark-3.1/deequ-2.0.0-spark-3.1.jar\"\n",
    "classpath = f\"{dir}/jar/deequ-2.0.0-spark-3.1.jar\"\n",
    "\n",
    "!wget -q -O $classpath $deequ_jar\n",
    "#!ls $classpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c968984c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deequ is still not supported in spark version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pydeequ\n",
    "os.environ['SPARK_VERSION'] = '3.1.2'\n",
    "import pydeequ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cdb11",
   "metadata": {},
   "source": [
    "---\n",
    "# Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d4da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4882cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/spark-3.1.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/oonisim/.ivy2/cache\n",
      "The jars for the packages stored in: /home/oonisim/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c4974c74-ce97-4224-8164-694b7daf3b72;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 1048ms :: artifacts dl 50ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c4974c74-ce97-4224-8164-694b7daf3b72\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/35ms)\n",
      "21/08/17 09:05:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/08/17 09:05:55 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/com.amazon.deequ_deequ-1.2.2-spark-3.0.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.scalanlp_breeze_2.12-0.13.2.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.scalanlp_breeze-macros_2.12-0.13.2.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/com.github.fommil.netlib_core-1.1.2.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/net.sf.opencsv_opencsv-2.3.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/com.github.rwl_jtransforms-2.4.0.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.apache.commons_commons-math3-3.2.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.spire-math_spire_2.12-0.13.0.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/com.chuusai_shapeless_2.12-2.3.2.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.scala-lang_scala-reflect-2.12.1.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/junit_junit-4.8.2.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.spire-math_spire-macros_2.12-0.13.0.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.typelevel_machinist_2.12-0.6.1.jar added multiple times to distributed cache.\n",
      "21/08/17 09:06:10 WARN Client: Same path resource file:///home/oonisim/.ivy2/jars/org.typelevel_macro-compat_2.12-1.1.1.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config(\"spark.driver.extraClassPath\", classpath) \\\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord) \\\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord) \\\n",
    "    .config('spark.debug.maxToStringFields', 100) \\\n",
    "    .config('spark.executor.memory', '2g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fb5ea",
   "metadata": {},
   "source": [
    "---\n",
    "# AWS Product Review data\n",
    "\n",
    "<img src=\"image/amazon_product_review.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1b8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- star_rating: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    path=f\"file:///{dir}/data/amazon_product_reviews.csv.gz\",\n",
    "    header=True,\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887db6e",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "* [Data Science on AWS - Chapter 5 Explore the Dataset](https://github.com/data-science-on-aws/workshop)\n",
    "<img src=\"image/deequ_metrics.png\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58d29c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/17 09:06:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------------+------+\n",
      "| entity|       instance|               name| value|\n",
      "+-------+---------------+-------------------+------+\n",
      "| Column|      review_id|       Completeness|   1.0|\n",
      "| Column|      review_id|ApproxCountDistinct|1040.0|\n",
      "|Dataset|              *|               Size|1000.0|\n",
      "| Column|top star_rating|         Compliance| 0.657|\n",
      "+-------+---------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addAnalyzer(Size()) \\\n",
    "    .addAnalyzer(Completeness(\"review_id\")) \\\n",
    "    .addAnalyzer(ApproxCountDistinct(\"review_id\")) \\\n",
    "    .addAnalyzer(Mean(\"star_rating\")) \\\n",
    "    .addAnalyzer(Compliance(\"top star_rating\", \"star_rating >= 4.0\")) \\\n",
    "    .addAnalyzer(Correlation(\"total_votes\", \"star_rating\")) \\\n",
    "    .addAnalyzer(Correlation(\"total_votes\", \"helpful_votes\")) \\\n",
    "    .run()\n",
    "                    \n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684ecb9",
   "metadata": {},
   "source": [
    "# Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bf621ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|               check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Amazon Digital So...|    Warning|     Warning|SizeConstraint(Si...|          Success|                    |\n",
      "|Amazon Digital So...|    Warning|     Warning|MinimumConstraint...|          Failure|Expected type of ...|\n",
      "|Amazon Digital So...|    Warning|     Warning|MaximumConstraint...|          Failure|Expected type of ...|\n",
      "|Amazon Digital So...|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "|Amazon Digital So...|    Warning|     Warning|UniquenessConstra...|          Success|                    |\n",
      "|Amazon Digital So...|    Warning|     Warning|CompletenessConst...|          Success|                    |\n",
      "|Amazon Digital So...|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "|Amazon Digital So...|    Warning|     Warning|ComplianceConstra...|          Success|                    |\n",
      "+--------------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.verification import *\n",
    "from pydeequ.checks import CheckLevel\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Amazon Digital Software Review Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >=1000) \\\n",
    "        .hasMin(\"star_rating\", lambda x: x == 1.0) \\\n",
    "        .hasMax(\"star_rating\", lambda x: x == 5.0)  \\\n",
    "        .isComplete(\"review_id\")  \\\n",
    "        .isUnique(\"review_id\")  \\\n",
    "        .isComplete(\"marketplace\")  \\\n",
    "        .isContainedIn(\"marketplace\", [\"US\", \"UK\", \"DE\", \"JP\", \"FR\"]) \\\n",
    "        .isNonNegative(\"year\")) \\\n",
    "    .run()\n",
    "    \n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe597fb",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del spark\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
