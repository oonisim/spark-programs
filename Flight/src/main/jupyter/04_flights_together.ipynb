{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 04\n",
    "Find the passengers who have been on more than N flights together within the range (from,to).\n",
    "\n",
    "## Assumptions\n",
    "1. \"more than 3 flights together\" means 4, 5, ... which is bigger than 3. \n",
    "2. Data is clearned and not errorneous\n",
    "3. Timezone consideration is not required\n",
    "\n",
    "## Approaches\n",
    "\n",
    "1. SQL self join that matches with non-self passengerId that have the same flightId.\n",
    "2. Matrix M * M<sup>T</sup>\n",
    "Self product matrix Matrix M * M<sup>T</sup> has diagonal represents the number of flghts of respective passenger, and right top part (row/passenger 1, column/passenger2) represents how many flights passenger2 shares with passenger 1.\n",
    "\n",
    "### TODO\n",
    "Implement a matrix way. \n",
    "\n",
    "## Output\n",
    "\n",
    "To avoid having duplicates, passegner 1 ID < passenger ID 2. For instance there will be no (0, 1, 14) and (1, 0, 14) togeher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 4\n",
       "NUM_PARTITIONS = 3\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .appName(\"flight_together\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CSV_DELIMITER = ,\n",
       "PROTOCOL = file://\n",
       "DATA_DIR = /home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/\n",
       "FLIGHTDATA_CSV_PATH = file:///home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/../resources/flightData.csv\n",
       "PASSENGER_CSV_PATH = file:///home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/../resources/passengers.csv\n",
       "DATE_FORMAT = yyyy-MM-dd\n",
       "FLIGHT_DATE_FROM = 2017-01-01\n",
       "FLIGHT_DATE_TO = 2017-12-31\n",
       "NUM_FLIGHT_TOGETHER = 3\n",
       "RESULT_DIR = results/flightsTogether\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "results/flightsTogether"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CSV_DELIMITER = \",\"\n",
    "val PROTOCOL=\"file://\"\n",
    "val DATA_DIR=\"/home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main/jupyter/\"\n",
    "val FLIGHTDATA_CSV_PATH = PROTOCOL + DATA_DIR + \"../resources/flightData.csv\"\n",
    "val PASSENGER_CSV_PATH = PROTOCOL + DATA_DIR + \"../resources/passengers.csv\"\n",
    "\n",
    "val DATE_FORMAT = \"yyyy-MM-dd\"\n",
    "val FLIGHT_DATE_FROM = \"2017-01-01\"\n",
    "val FLIGHT_DATE_TO   = \"2017-12-31\"\n",
    "val NUM_FLIGHT_TOGETHER = 3\n",
    "\n",
    "val RESULT_DIR = \"results/flightsTogether\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n",
       "times = ListBuffer()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "clear: ()Unit\n",
       "average: ()Long\n",
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ListBuffer()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val timing = new StringBuffer\n",
    "val times = new ListBuffer[Long]()\n",
    "\n",
    "def clear(): Unit = {\n",
    "    timing.setLength(0)\n",
    "    times.clear\n",
    "}\n",
    "def average(): Long = {\n",
    "    times.reduce(_+_) / times.length\n",
    "}\n",
    "\n",
    "/**\n",
    "@param label Description about the run\n",
    "@code code to execute\n",
    "@return execution\n",
    "*/\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    times.append(stop - start)\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:63: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save: (df: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save(df: DataFrame) = {\n",
    "    df.coalesce(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(RESULT_DIR)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightData = [passengerId: int, flightId: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightData = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(FLIGHTDATA_CSV_PATH)\n",
    "    .select(\n",
    "        \"passengerId\",\n",
    "        \"flightId\",\n",
    "        \"date\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryFlightsTogether = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "WITH\n",
       "    --------------------------------------------------------------------------------\n",
       "    -- Passengers flew more than NUM_FLIGHT_TOGETHER times.\n",
       "    --------------------------------------------------------------------------------\n",
       "    more_than_n_flights AS (\n",
       "        SELECT passengerId\n",
       "        FROM flightData\n",
       "        GROUP BY passengerId\n",
       "        --------------------------------------------------------------------------------\n",
       "        -- \"More than 3\" means 4, 5, 6, ... not including 3.\n",
       "        --------------------------------------------------------------------------------\n",
       "        HAVING count(flightId) > 3\n",
       "        ORDER BY passengerId\n",
       "    )\n",
       "SELECT\n",
       "    f.passengerId AS `Passenger 1 ID`,\n",
       "    s.passengerId AS `Passenger 2 ID`,\n",
       "    count(s.flightId) AS ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val queryFlightsTogether = s\"\"\"\n",
    "WITH \n",
    "    --------------------------------------------------------------------------------\n",
    "    -- Passengers flew more than NUM_FLIGHT_TOGETHER times.\n",
    "    --------------------------------------------------------------------------------\n",
    "    more_than_n_flights AS (\n",
    "        SELECT passengerId\n",
    "        FROM flightData\n",
    "        GROUP BY passengerId\n",
    "        --------------------------------------------------------------------------------\n",
    "        -- \"More than 3\" means 4, 5, 6, ... not including 3.\n",
    "        --------------------------------------------------------------------------------\n",
    "        HAVING count(flightId) > $NUM_FLIGHT_TOGETHER\n",
    "        ORDER BY passengerId\n",
    "    )\n",
    "\n",
    "SELECT \n",
    "    f.passengerId AS `Passenger 1 ID`, \n",
    "    s.passengerId AS `Passenger 2 ID`, \n",
    "    count(s.flightId) AS `Number of flights together`,\n",
    "    '$FLIGHT_DATE_FROM' as From,\n",
    "    '$FLIGHT_DATE_TO' as To\n",
    "FROM\n",
    "    flightData f \n",
    "    --------------------------------------------------------------------------------\n",
    "    -- Passengers more than NUM_FLIGHT_TOGETHER flights\n",
    "    --------------------------------------------------------------------------------\n",
    "    INNER JOIN more_than_n_flights m \n",
    "        ON f.passengerId == m.passengerId\n",
    "    --------------------------------------------------------------------------------\n",
    "    -- Passengers who shared same flights\n",
    "    --------------------------------------------------------------------------------\n",
    "    INNER JOIN flightData s \n",
    "        ON f.flightId == s.flightId\n",
    "WHERE\n",
    "    --------------------------------------------------------------------------------\n",
    "    -- Only have (passenger 1 > passenger 2) to avoid having both (p1, p2) and (p2, p1) \n",
    "    --------------------------------------------------------------------------------\n",
    "    f.passengerId < s.passengerId AND  \n",
    "    f.date >= to_timestamp('$FLIGHT_DATE_FROM', '$DATE_FORMAT') AND\n",
    "    f.date <= to_timestamp('$FLIGHT_DATE_TO',   '$DATE_FORMAT') AND\n",
    "    s.date >= to_timestamp('$FLIGHT_DATE_FROM', '$DATE_FORMAT') AND\n",
    "    s.date <= to_timestamp('$FLIGHT_DATE_TO',   '$DATE_FORMAT')\n",
    "GROUP BY \n",
    "    f.passengerId, s.passengerId\n",
    "HAVING \n",
    "    count(s.flightId) > $NUM_FLIGHT_TOGETHER\n",
    "--ORDER BY \n",
    "--    `Number of flights together`, \n",
    "--    f.passengerId, \n",
    "--    s.passengerId\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run: (label: String, repeats: Int, toSave: Boolean)Long\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Run the SparkSQL \n",
    "@param label Lable to describe this run\n",
    "@param query SQL \n",
    "@param repeats Number of run\n",
    "@return Average execution time in msec\n",
    "*/\n",
    "def run(label: String, repeats: Int, toSave: Boolean = false): Long = {\n",
    "    val result = spark\n",
    "        .sql(queryFlightsTogether)\n",
    "        .sort(\n",
    "//            desc(\"Number of flights together\"), \n",
    "            asc(\"Passenger 1 ID\"),\n",
    "            asc(\"Passenger 2 ID\")\n",
    "        )\n",
    "\n",
    "    clear()\n",
    "    for (i <- (0 until repeats)){\n",
    "        timed(\n",
    "            label,\n",
    "            result.show(3)\n",
    "        )\n",
    "        println(timing)\n",
    "        println(s\"Average time $average ms\")\n",
    "    }\n",
    "    println(result.rdd.toDebugString)    \n",
    "\n",
    "    if(toSave) save(result)\n",
    "    \n",
    "    average\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (passengerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId) took 10103 ms.\n",
      "\n",
      "Average time 10103 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId) took 10103 ms.\n",
      "Processing Order by (passengerId) took 2499 ms.\n",
      "\n",
      "Average time 6301 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId) took 10103 ms.\n",
      "Processing Order by (passengerId) took 2499 ms.\n",
      "Processing Order by (passengerId) took 2326 ms.\n",
      "\n",
      "Average time 4976 ms\n",
      "(12) MapPartitionsRDD[87] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[86] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[85] at rdd at <console>:99 []\n",
      " |   ShuffledRowRDD[84] at rdd at <console>:99 []\n",
      " +-(12) MapPartitionsRDD[83] at rdd at <console>:99 []\n",
      "    |   MapPartitionsRDD[79] at rdd at <console>:99 []\n",
      "    |   MapPartitionsRDD[78] at rdd at <console>:99 []\n",
      "    |   MapPartitionsRDD[77] at rdd at <console>:99 []\n",
      "    |   *(2) Sort [passengerId#32 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(passengerId#32 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#32,flightId#33,date#36] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[17] at run at ThreadPoolExecutor.java:1149 []\n",
      "    |       CachedPartitions: 12; MemorySize: 513.8 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "    |   MapPartitionsRDD[16] at run at ThreadPoolExecutor.java:1149 []\n",
      "    |   ShuffledRowRDD[15] at run at ThreadPoolExecutor.java:1149 []\n",
      "    +-(1) MapPartitionsRDD[14] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |  MapPartitionsRDD[10] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |  FileScanRDD[9] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "timeOrderPassenger = 4976\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"passengerId\")\n",
    "    .persist\n",
    "    .createOrReplaceTempView(\"flightData\")\n",
    "\n",
    "val timeOrderPassenger = run(\n",
    "    \"Order by (passengerId)\",\n",
    "    3,\n",
    "    true\n",
    ")\n",
    "spark.catalog.dropTempView(\"flightData\")\n",
    "\n",
    "flightData.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (flightId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (flightId) took 7569 ms.\n",
      "\n",
      "Average time 7569 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (flightId) took 7569 ms.\n",
      "Processing Order by (flightId) took 3187 ms.\n",
      "\n",
      "Average time 5378 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (flightId) took 7569 ms.\n",
      "Processing Order by (flightId) took 3187 ms.\n",
      "Processing Order by (flightId) took 2908 ms.\n",
      "\n",
      "Average time 4554 ms\n",
      "(12) MapPartitionsRDD[237] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[236] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[235] at rdd at <console>:99 []\n",
      " |   ShuffledRowRDD[234] at rdd at <console>:99 []\n",
      " +-(12) MapPartitionsRDD[233] at rdd at <console>:99 []\n",
      "    |   MapPartitionsRDD[229] at rdd at <console>:99 []\n",
      "    |   ShuffledRowRDD[228] at rdd at <console>:99 []\n",
      "    +-(12) MapPartitionsRDD[227] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[226] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[225] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[224] at rdd at <console>:99 []\n",
      "       |   *(2) Sort [flightId#33 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(flightId#33 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#32,flightId#33,date#36] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[119] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |       CachedPartitions: 12; MemorySize: 392.3 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |   MapPartitionsRDD[118] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |   ShuffledRowRDD[117] at run at ThreadPoolExecutor.java:1149 []\n",
      "       +-(1) MapPartitionsRDD[116] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  MapPartitionsRDD[112] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  FileScanRDD[111] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "timeOrderFlight = 4554\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"flightId\")\n",
    "    .persist\n",
    "    .createOrReplaceTempView(\"flightData\")\n",
    "\n",
    "val timeOrderFlight = run(\n",
    "    \"Order by (flightId)\",\n",
    "    3\n",
    ")\n",
    "\n",
    "spark.catalog.dropTempView(\"flightData\")\n",
    "flightData.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (\"passengerId\", \"flightId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId, flightId) took 4181 ms.\n",
      "\n",
      "Average time 4181 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId, flightId) took 4181 ms.\n",
      "Processing Order by (passengerId, flightId) took 2634 ms.\n",
      "\n",
      "Average time 3407 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId, flightId) took 4181 ms.\n",
      "Processing Order by (passengerId, flightId) took 2634 ms.\n",
      "Processing Order by (passengerId, flightId) took 2199 ms.\n",
      "\n",
      "Average time 3004 ms\n",
      "(12) MapPartitionsRDD[364] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[363] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[362] at rdd at <console>:99 []\n",
      " |   ShuffledRowRDD[361] at rdd at <console>:99 []\n",
      " +-(12) MapPartitionsRDD[360] at rdd at <console>:99 []\n",
      "    |   MapPartitionsRDD[356] at rdd at <console>:99 []\n",
      "    |   ShuffledRowRDD[355] at rdd at <console>:99 []\n",
      "    +-(12) MapPartitionsRDD[354] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[353] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[352] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[351] at rdd at <console>:99 []\n",
      "       |   *(2) Sort [passengerId#32 ASC NULLS FIRST, flightId#33 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(passengerId#32 ASC NULLS FIRST, flightId#33 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#32,flightId#33,date#36] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[246] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |       CachedPartitions: 12; MemorySize: 599.3 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |   MapPartitionsRDD[245] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |   ShuffledRowRDD[244] at run at ThreadPoolExecutor.java:1149 []\n",
      "       +-(1) MapPartitionsRDD[243] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  MapPartitionsRDD[239] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  FileScanRDD[238] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "timeOrderPassengerFlight = 3004\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"passengerId\", \"flightId\")\n",
    "    .persist\n",
    "    .createOrReplaceTempView(\"flightData\")\n",
    "\n",
    "val timeOrderPassengerFlight= run(\n",
    "    \"Order by (passengerId, flightId)\",\n",
    "    3\n",
    ")\n",
    "\n",
    "spark.catalog.dropTempView(\"flightData\")\n",
    "flightData.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (\"passengerId\", \"date\")Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId, date) took 4744 ms.\n",
      "\n",
      "Average time 4744 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId, date) took 4744 ms.\n",
      "Processing Order by (passengerId, date) took 3426 ms.\n",
      "\n",
      "Average time 4085 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by (passengerId, date) took 4744 ms.\n",
      "Processing Order by (passengerId, date) took 3426 ms.\n",
      "Processing Order by (passengerId, date) took 2158 ms.\n",
      "\n",
      "Average time 3442 ms\n",
      "(12) MapPartitionsRDD[491] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[490] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[489] at rdd at <console>:99 []\n",
      " |   ShuffledRowRDD[488] at rdd at <console>:99 []\n",
      " +-(12) MapPartitionsRDD[487] at rdd at <console>:99 []\n",
      "    |   MapPartitionsRDD[483] at rdd at <console>:99 []\n",
      "    |   ShuffledRowRDD[482] at rdd at <console>:99 []\n",
      "    +-(12) MapPartitionsRDD[481] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[480] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[479] at rdd at <console>:99 []\n",
      "       |   MapPartitionsRDD[478] at rdd at <console>:99 []\n",
      "       |   *(2) Sort [passengerId#32 ASC NULLS FIRST, date#36 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(passengerId#32 ASC NULLS FIRST, date#36 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#32,flightId#33,date#36] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[373] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |       CachedPartitions: 12; MemorySize: 600.1 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |   MapPartitionsRDD[372] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |   ShuffledRowRDD[371] at run at ThreadPoolExecutor.java:1149 []\n",
      "       +-(1) MapPartitionsRDD[370] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  MapPartitionsRDD[366] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  FileScanRDD[365] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "timeOrderPassengerDate = 3442\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"passengerId\", \"date\")\n",
    "    .persist\n",
    "    .createOrReplaceTempView(\"flightData\")\n",
    "\n",
    "val timeOrderPassengerDate = run(\n",
    "    \"Order by (passengerId, date)\",\n",
    "    3\n",
    ")\n",
    "\n",
    "spark.catalog.dropTempView(\"flightData\")\n",
    "flightData.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by none took 4658 ms.\n",
      "\n",
      "Average time 4658 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by none took 4658 ms.\n",
      "Processing Order by none took 4494 ms.\n",
      "\n",
      "Average time 4576 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Processing Order by none took 4658 ms.\n",
      "Processing Order by none took 4494 ms.\n",
      "Processing Order by none took 3638 ms.\n",
      "\n",
      "Average time 4263 ms\n",
      "(12) MapPartitionsRDD[612] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[611] at rdd at <console>:99 []\n",
      " |   MapPartitionsRDD[610] at rdd at <console>:99 []\n",
      " |   ShuffledRowRDD[609] at rdd at <console>:99 []\n",
      " +-(12) MapPartitionsRDD[608] at rdd at <console>:99 []\n",
      "    |   MapPartitionsRDD[604] at rdd at <console>:99 []\n",
      "    |   ShuffledRowRDD[603] at rdd at <console>:99 []\n",
      "    +-(1) MapPartitionsRDD[602] at rdd at <console>:99 []\n",
      "       |  MapPartitionsRDD[601] at rdd at <console>:99 []\n",
      "       |  MapPartitionsRDD[600] at rdd at <console>:99 []\n",
      "       |  MapPartitionsRDD[599] at rdd at <console>:99 []\n",
      "       |  *(1) FileScan csv [passengerId#32,flightId#33,date#36] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/oonisim/home/repositories/git/oonisim/spark-programs/Flight/src/main..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[494] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |      CachedPartitions: 1; MemorySize: 389.6 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |  MapPartitionsRDD[493] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |  FileScanRDD[492] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "timeOrderNone = 4263\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData\n",
    "    .persist\n",
    "    .createOrReplaceTempView(\"flightData\")\n",
    "\n",
    "val timeOrderNone = run(\n",
    "    \"Order by none\",\n",
    "    3\n",
    ")\n",
    "\n",
    "spark.catalog.dropTempView(\"flightData\")\n",
    "flightData.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elepased Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Order by:\n",
      "Non                     is 4263 ms\n",
      "(passengerId)           is 4976 ms\n",
      "(flightId)              is 4554 ms\n",
      "(passengerId, date)     is 3442 ms\n",
      "(passengerId, flightId) is 3004 ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "report = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "Order by:\n",
       "Non                     is 4263 ms\n",
       "(passengerId)           is 4976 ms\n",
       "(flightId)              is 4554 ms\n",
       "(passengerId, date)     is 3442 ms\n",
       "(passengerId, flightId) is 3004 ms\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val report = s\"\"\"\n",
    "Order by:\n",
    "Non                     is $timeOrderNone ms\n",
    "(passengerId)           is $timeOrderPassenger ms\n",
    "(flightId)              is $timeOrderFlight ms\n",
    "(passengerId, date)     is $timeOrderPassengerDate ms\n",
    "(passengerId, flightId) is $timeOrderPassengerFlight ms\n",
    "\"\"\"\n",
    "println(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData.createOrReplaceTempView(\"flightData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|passengerId|count|\n",
      "+-----------+-----+\n",
      "|          1|    5|\n",
      "|         37|    4|\n",
      "|         38|    4|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+--------+\n",
      "|passengerId|flightId|\n",
      "+-----------+--------+\n",
      "|          1|       0|\n",
      "|          1|     972|\n",
      "|          1|     901|\n",
      "|          1|     993|\n",
      "|          1|     940|\n",
      "|         37|     972|\n",
      "|         37|       0|\n",
      "|         37|     940|\n",
      "|         37|     901|\n",
      "|         38|       0|\n",
      "|         38|     901|\n",
      "|         38|     940|\n",
      "|         38|     972|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query = \n",
       "query = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT passengerId, flightId\n",
       "FROM flightData\n",
       "WHERE passengerId in (1, 37, 38)\n",
       "ORDER BY passengerId\n",
       "\"\n",
       "SELECT passengerId, flightId\n",
       "FROM flightData\n",
       "WHERE passengerId in (1, 37, 38)\n",
       "ORDER BY passengerId\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var query = \n",
    "s\"\"\"\n",
    "SELECT passengerId, count(flightId) as count\n",
    "FROM flightData\n",
    "WHERE passengerId in (1, 37, 38)\n",
    "GROUP BY passengerId\n",
    "HAVING count(flightId) > $NUM_FLIGHT_TOGETHER\n",
    "ORDER BY passengerId\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    ".show()\n",
    "\n",
    "query = \n",
    "s\"\"\"\n",
    "SELECT passengerId, flightId\n",
    "FROM flightData\n",
    "WHERE passengerId in (1, 37, 38)\n",
    "ORDER BY passengerId\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|           618|           648|                         5|2017-01-01|2017-12-31|\n",
      "|           704|           749|                         5|2017-01-01|2017-12-31|\n",
      "|           712|          1043|                         4|2017-01-01|2017-12-31|\n",
      "|           366|           759|                         6|2017-01-01|2017-12-31|\n",
      "|          1878|          1891|                         5|2017-01-01|2017-12-31|\n",
      "|          2165|          2181|                         4|2017-01-01|2017-12-31|\n",
      "|          3532|          3552|                         4|2017-01-01|2017-12-31|\n",
      "|           246|          3195|                         4|2017-01-01|2017-12-31|\n",
      "|          2510|          3180|                         4|2017-01-01|2017-12-31|\n",
      "|          1693|          2560|                         4|2017-01-01|2017-12-31|\n",
      "|          2699|          3913|                         5|2017-01-01|2017-12-31|\n",
      "|          4260|          4282|                         5|2017-01-01|2017-12-31|\n",
      "|          4329|          4345|                         4|2017-01-01|2017-12-31|\n",
      "|          2223|          3540|                         5|2017-01-01|2017-12-31|\n",
      "|          4617|          4630|                         4|2017-01-01|2017-12-31|\n",
      "|          1844|          2789|                         4|2017-01-01|2017-12-31|\n",
      "|           245|          1755|                         4|2017-01-01|2017-12-31|\n",
      "|          2867|          4083|                         5|2017-01-01|2017-12-31|\n",
      "|          5001|          5098|                         6|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "seed = 42\n",
       "withReplacement = false\n",
       "fraction = 0.01\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 42\n",
    "val withReplacement = false\n",
    "val fraction = 0.01\n",
    "spark.sql(queryFlightsTogether)\n",
    "    .sample(withReplacement, fraction, seed)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|            58|          1381|                         7|2017-01-01|2017-12-31|\n",
      "|            58|          2942|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filter01 = (Passenger 2 ID = 2942)\n",
       "filter02 = (Passenger 2 ID = 1381)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:67: warning: a pure expression does nothing in statement position; you may be omitting necessary parentheses\n",
       "\"\"\"\n",
       "^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Passenger 2 ID = 1381)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "comm -12 \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^1381,/{print $2}' | sort ) \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^58,/{print $2}' | sort)\n",
    "\n",
    "131\n",
    "189\n",
    "217\n",
    "247\n",
    "272\n",
    "283\n",
    "331\n",
    "\n",
    "comm -12 \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^58,/{print $2}' | sort ) \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^2942,/{print $2}' | sort)\n",
    "\n",
    "131\n",
    "189\n",
    "217\n",
    "247\n",
    "$ comm -12 \\\n",
    "> <(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^701,/{print $2}' | sort ) \\\n",
    "> <(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^760,/{print $2}' | sort)\n",
    "13\n",
    "18\n",
    "333\n",
    "34\n",
    "361\n",
    "374\n",
    "391\n",
    "404\n",
    "432\n",
    "45\n",
    "58\n",
    "7\n",
    "77\n",
    "91\n",
    "96\n",
    "\"\"\"\n",
    "\n",
    "val filter01 = col(\"Passenger 2 ID\") === 2942\n",
    "val filter02 = col(\"Passenger 2 ID\") === 1381\n",
    "spark.sql(queryFlightsTogether)\n",
    "    .where(col(\"Passenger 1 ID\") === 58)\n",
    "    .where(filter01.or(filter02))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----------------+----------------+\n",
      "|count(DISTINCT passengerId)|min(passengerId)|max(passengerId)|\n",
      "+---------------------------+----------------+----------------+\n",
      "|                      15500|               1|           15500|\n",
      "+---------------------------+----------------+----------------+\n",
      "\n",
      "+------------------------+-------------+-------------+\n",
      "|count(DISTINCT flightId)|min(flightId)|max(flightId)|\n",
      "+------------------------+-------------+-------------+\n",
      "|                    1000|            0|          999|\n",
      "+------------------------+-------------+-------------+\n",
      "\n",
      "+--------+-----+\n",
      "|flightId|count|\n",
      "+--------+-----+\n",
      "|      28|  100|\n",
      "|      31|  100|\n",
      "|      32|  100|\n",
      "|      36|  100|\n",
      "|      58|  100|\n",
      "|      76|  100|\n",
      "|      77|  100|\n",
      "|      79|  100|\n",
      "|      98|  100|\n",
      "|     104|  100|\n",
      "|     118|  100|\n",
      "|     144|  100|\n",
      "|     153|  100|\n",
      "|     156|  100|\n",
      "|     163|  100|\n",
      "|     184|  100|\n",
      "|     191|  100|\n",
      "|     195|  100|\n",
      "|     197|  100|\n",
      "|     201|  100|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+\n",
      "|total_passengers|\n",
      "+----------------+\n",
      "|          100000|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "queryPassengersPerFlight = \n",
       "totalPassengers = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT flightId, count(passengerId) as count\n",
       "FROM flightData\n",
       "GROUP BY flightId\n",
       "ORDER BY count DESC\n",
       "\"\n",
       "\"\n",
       "SELECT count(passengerId) as total_passengers\n",
       "FROM flightData\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flightData.select(\n",
    "    countDistinct(\"passengerId\"),\n",
    "    min(\"passengerId\"),\n",
    "    max(\"passengerId\")\n",
    ").show\n",
    "\n",
    "flightData.select(\n",
    "    countDistinct(\"flightId\"),\n",
    "    min(\"flightId\"),\n",
    "    max(\"flightId\")\n",
    ").show\n",
    "\n",
    "val queryPassengersPerFlight = \"\"\"\n",
    "SELECT flightId, count(passengerId) as count\n",
    "FROM flightData\n",
    "GROUP BY flightId\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "spark.sql(queryPassengersPerFlight)\n",
    "    .show()\n",
    "\n",
    "val totalPassengers = \"\"\"\n",
    "SELECT count(passengerId) as total_passengers\n",
    "FROM flightData\n",
    "\"\"\"\n",
    "spark.sql(totalPassengers)\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView(\"flightData\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
