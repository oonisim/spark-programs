{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 04\n",
    "Find the passengers who have been on more than N flights together within the range (from,to).\n",
    "\n",
    "## Assumptions\n",
    "1. Data is clearned and not errorneous\n",
    "2. Timezone consideration is not required\n",
    "\n",
    "## Approaches\n",
    "\n",
    "1. SQL self join that matches with non-self passengerId that have the same flightId.\n",
    "2. Matrix M<sup>T</sup> * M\n",
    "Self product matrix M<sup>T</sup> * M has diagonal represents the number of flghts of respective passenger, and right top part (row/passenger 1, column/passenger2) represents how many flights passenger2 shares with passenger 1.\n",
    "\n",
    "### TODO\n",
    "Implement a matrix way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.time.{Period, LocalDate, Instant}\n",
    "import java.sql.Timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark parition control based on core availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUM_CORES = 4\n",
       "NUM_PARTITIONS = 3\n",
       "spark = <lazy>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<lazy>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NUM_CORES = 4\n",
    "val NUM_PARTITIONS = 3\n",
    "\n",
    "lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"flight\")\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", NUM_CORES * NUM_PARTITIONS)\n",
    "spark.conf.set(\"spark.default.parallelism\", NUM_CORES * NUM_PARTITIONS)\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CSV_DELIMITER = ,\n",
       "FLIGHTDATA_CSV_PATH = ../resources/flightData.csv\n",
       "PASSENGER_CSV_PATH = ../resources/passengers.csv\n",
       "DATE_FORMAT = yyyy-MM-dd\n",
       "FLIGHT_DATE_FROM = 2017-01-01\n",
       "FLIGHT_DATE_TO = 2017-12-31\n",
       "NUM_FLIGHT_TOGETHER = 3\n",
       "RESULT_DIR = results/flightsTogether\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "results/flightsTogether"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CSV_DELIMITER = \",\"\n",
    "val FLIGHTDATA_CSV_PATH = \"../resources/flightData.csv\"\n",
    "val PASSENGER_CSV_PATH = \"../resources/passengers.csv\"\n",
    "\n",
    "val DATE_FORMAT = \"yyyy-MM-dd\"\n",
    "val FLIGHT_DATE_FROM = \"2017-01-01\"\n",
    "val FLIGHT_DATE_TO   = \"2017-12-31\"\n",
    "val NUM_FLIGHT_TOGETHER = 3\n",
    "\n",
    "val RESULT_DIR = \"results/flightsTogether\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed time profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timing = \n",
       "times = ListBuffer()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "clear: ()Unit\n",
       "average: ()Long\n",
       "timed: [T](label: String, code: => T)T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ListBuffer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "val timing = new StringBuffer\n",
    "val times = new ListBuffer[Long]()\n",
    "\n",
    "def clear(): Unit = {\n",
    "    timing.setLength(0)\n",
    "    times.clear\n",
    "}\n",
    "def average(): Long = {\n",
    "    times.reduce(_+_) / times.length\n",
    "}\n",
    "\n",
    "/**\n",
    "@param label Description about the run\n",
    "@code code to execute\n",
    "@return execution\n",
    "*/\n",
    "def timed[T](label: String, code: => T): T = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val result = code\n",
    "    val stop = System.currentTimeMillis()\n",
    "    timing.append(s\"Processing $label took ${stop - start} ms.\\n\")\n",
    "    times.append(stop - start)\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<console>:45: error: missing argument list for method timed\n",
       "Unapplied methods are only converted to functions when a function type is expected.\n",
       "You can make this conversion explicit by writing `timed _` or `timed(_,_)` instead of `timed`.\n",
       "       timed\n",
       "       ^\n",
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// To flush out error: missing argument list for method timed\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save: (df: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save(df: DataFrame) = {\n",
    "    df.coalesce(1)\n",
    "    .write\n",
    "    .format(\"csv\")\n",
    "    .mode(SaveMode.Overwrite)\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(RESULT_DIR)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run: (label: String, query: String, repeats: Int, toSave: Boolean)Long\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    "Run the SparkSQL \n",
    "@param label Lable to describe this run\n",
    "@param query SQL \n",
    "@param repeats Number of run\n",
    "@return Average execution time in msec\n",
    "*/\n",
    "def run(label: String, query: String, repeats: Int, toSave: Boolean = false): Long = {\n",
    "    val result = spark.sql(query)\n",
    "\n",
    "    clear()\n",
    "    for (i <- (0 until repeats)){\n",
    "        timed(\n",
    "            label,\n",
    "            result.show(5)\n",
    "        )\n",
    "        println(timing)\n",
    "        println(s\"Average time $average ms\")\n",
    "    }\n",
    "    println(result.rdd.toDebugString)    \n",
    "\n",
    "    if(toSave) save(result)\n",
    "    average\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightData = [passengerId: int, flightId: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transformations, no action yet\n",
    "val flightData = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"../resources/flightData.csv\")\n",
    "    .select(\n",
    "        \"passengerId\",\n",
    "        \"flightId\",\n",
    "        \"date\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "queryFlightsTogether = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "WITH\n",
       "    --------------------------------------------------------------------------------\n",
       "    -- Passengers flew more than NUM_FLIGHT_TOGETHER times.\n",
       "    --------------------------------------------------------------------------------\n",
       "    more_than_n_flights AS (\n",
       "        SELECT passengerId\n",
       "        FROM flightData\n",
       "        GROUP BY passengerId\n",
       "        HAVING count(flightId) > 3\n",
       "        ORDER BY passengerId\n",
       "    )\n",
       "SELECT\n",
       "    f.passengerId AS `Passenger 1 ID`,\n",
       "    s.passengerId AS `Passenger 2 ID`,\n",
       "    count(s.flightId) AS `Number of flights together`,\n",
       "    '2017-01-01' as From,\n",
       "    '2017-12-31' as To\n",
       "FROM\n",
       "    flightData f\n",
       "    --------------------------------------------------------------------------------\n",
       "    -- Passengers more than NUM_FLIGHT_TOGETHER fligh...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val queryFlightsTogether = s\"\"\"\n",
    "WITH \n",
    "    --------------------------------------------------------------------------------\n",
    "    -- Passengers flew more than NUM_FLIGHT_TOGETHER times.\n",
    "    --------------------------------------------------------------------------------\n",
    "    more_than_n_flights AS (\n",
    "        SELECT passengerId\n",
    "        FROM flightData\n",
    "        GROUP BY passengerId\n",
    "        HAVING count(flightId) > $NUM_FLIGHT_TOGETHER\n",
    "        ORDER BY passengerId\n",
    "    )\n",
    "\n",
    "SELECT \n",
    "    f.passengerId AS `Passenger 1 ID`, \n",
    "    s.passengerId AS `Passenger 2 ID`, \n",
    "    count(s.flightId) AS `Number of flights together`,\n",
    "    '$FLIGHT_DATE_FROM' as From,\n",
    "    '$FLIGHT_DATE_TO' as To\n",
    "FROM\n",
    "    flightData f \n",
    "    --------------------------------------------------------------------------------\n",
    "    -- Passengers more than NUM_FLIGHT_TOGETHER flights\n",
    "    --------------------------------------------------------------------------------\n",
    "    INNER JOIN more_than_n_flights m \n",
    "        ON f.passengerId == m.passengerId\n",
    "    --------------------------------------------------------------------------------\n",
    "    -- Passengers who shared same flights\n",
    "    --------------------------------------------------------------------------------\n",
    "    INNER JOIN flightData s \n",
    "        ON f.flightId == s.flightId\n",
    "WHERE\n",
    "    f.passengerId != s.passengerId AND\n",
    "    f.date >= to_timestamp('$FLIGHT_DATE_FROM', '$DATE_FORMAT') AND\n",
    "    f.date <= to_timestamp('$FLIGHT_DATE_TO',   '$DATE_FORMAT') AND\n",
    "    s.date >= to_timestamp('$FLIGHT_DATE_FROM', '$DATE_FORMAT') AND\n",
    "    s.date <= to_timestamp('$FLIGHT_DATE_TO',   '$DATE_FORMAT')\n",
    "GROUP BY \n",
    "    f.passengerId, s.passengerId\n",
    "HAVING \n",
    "    count(s.flightId) > $NUM_FLIGHT_TOGETHER\n",
    "ORDER BY \n",
    "    f.passengerId, s.passengerId\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by none took 12293 ms.\n",
      "\n",
      "Average time 12293 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by none took 12293 ms.\n",
      "Processing Order by none took 9375 ms.\n",
      "\n",
      "Average time 10834 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by none took 12293 ms.\n",
      "Processing Order by none took 9375 ms.\n",
      "Processing Order by none took 9268 ms.\n",
      "\n",
      "Average time 10312 ms\n",
      "(12) MapPartitionsRDD[129] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[128] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[127] at rdd at <console>:68 []\n",
      " |   ShuffledRowRDD[126] at rdd at <console>:68 []\n",
      " +-(12) MapPartitionsRDD[125] at rdd at <console>:68 []\n",
      "    |   MapPartitionsRDD[121] at rdd at <console>:68 []\n",
      "    |   ShuffledRowRDD[120] at rdd at <console>:68 []\n",
      "    +-(1) MapPartitionsRDD[119] at rdd at <console>:68 []\n",
      "       |  MapPartitionsRDD[118] at rdd at <console>:68 []\n",
      "       |  MapPartitionsRDD[117] at rdd at <console>:68 []\n",
      "       |  MapPartitionsRDD[116] at rdd at <console>:68 []\n",
      "       |  *(1) FileScan csv [passengerId#10,flightId#11,date#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/masa/home/repository/git/oonisim/spark-programs/Flight/src/main/reso..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[11] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |      CachedPartitions: 1; MemorySize: 389.6 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |  MapPartitionsRDD[10] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |  FileScanRDD[9] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [passengerId: int, flightId: int ... 1 more field]\n",
       "timeOrderNone = 10312\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .persist\n",
    "\n",
    "df.createOrReplaceTempView(\"flightData\")\n",
    "val timeOrderNone = run(\n",
    "    \"Order by none\",\n",
    "    queryFlightsTogether,\n",
    "    3,\n",
    "    true\n",
    ")\n",
    "spark.catalog.dropTempView(\"df\")\n",
    "\n",
    "df.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (passengerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId) took 4529 ms.\n",
      "\n",
      "Average time 4529 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId) took 4529 ms.\n",
      "Processing Order by (passengerId) took 3078 ms.\n",
      "\n",
      "Average time 3803 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId) took 4529 ms.\n",
      "Processing Order by (passengerId) took 3078 ms.\n",
      "Processing Order by (passengerId) took 3140 ms.\n",
      "\n",
      "Average time 3582 ms\n",
      "(12) MapPartitionsRDD[237] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[236] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[235] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[234] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[233] at rdd at <console>:68 []\n",
      " |   *(2) Sort [passengerId#10 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(passengerId#10 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#10,flightId#11,date#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/masa/home/repository/git/oonisim/spark-programs/Flight/src/main/reso..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[173] at run at ThreadPoolExecutor.java:1149 []\n",
      " |       CachedPartitions: 12; MemorySize: 513.9 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      " |   MapPartitionsRDD[172] at run at ThreadPoolExecutor.java:1149 []\n",
      " |   ShuffledRowRDD[171] at run at ThreadPoolExecutor.java:1149 []\n",
      " +-(1) MapPartitionsRDD[170] at run at ThreadPoolExecutor.java:1149 []\n",
      "    |  MapPartitionsRDD[166] at run at ThreadPoolExecutor.java:1149 []\n",
      "    |  FileScanRDD[165] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [passengerId: int, flightId: int ... 1 more field]\n",
       "timeOrderPassenger = 3582\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"passengerId\")\n",
    "    .persist\n",
    "\n",
    "df.createOrReplaceTempView(\"flightData\")\n",
    "val timeOrderPassenger = run(\n",
    "    \"Order by (passengerId)\",\n",
    "    queryFlightsTogether,\n",
    "    3,\n",
    "    true\n",
    ")\n",
    "spark.catalog.dropTempView(\"df\")\n",
    "\n",
    "df.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (flightId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (flightId) took 7048 ms.\n",
      "\n",
      "Average time 7048 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (flightId) took 7048 ms.\n",
      "Processing Order by (flightId) took 5934 ms.\n",
      "\n",
      "Average time 6491 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (flightId) took 7048 ms.\n",
      "Processing Order by (flightId) took 5934 ms.\n",
      "Processing Order by (flightId) took 5860 ms.\n",
      "\n",
      "Average time 6280 ms\n",
      "(12) MapPartitionsRDD[381] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[380] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[379] at rdd at <console>:68 []\n",
      " |   ShuffledRowRDD[378] at rdd at <console>:68 []\n",
      " +-(12) MapPartitionsRDD[377] at rdd at <console>:68 []\n",
      "    |   MapPartitionsRDD[373] at rdd at <console>:68 []\n",
      "    |   ShuffledRowRDD[372] at rdd at <console>:68 []\n",
      "    +-(12) MapPartitionsRDD[371] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[370] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[369] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[368] at rdd at <console>:68 []\n",
      "       |   *(2) Sort [flightId#11 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(flightId#11 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#10,flightId#11,date#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/masa/home/repository/git/oonisim/spark-programs/Flight/src/main/reso..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[263] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |       CachedPartitions: 12; MemorySize: 392.3 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |   MapPartitionsRDD[262] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |   ShuffledRowRDD[261] at run at ThreadPoolExecutor.java:1149 []\n",
      "       +-(1) MapPartitionsRDD[260] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  MapPartitionsRDD[256] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  FileScanRDD[255] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [passengerId: int, flightId: int ... 1 more field]\n",
       "timeOrderFlight = 6280\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"flightId\")\n",
    "    .persist\n",
    "\n",
    "df.createOrReplaceTempView(\"flightData\")\n",
    "val timeOrderFlight = run(\n",
    "    \"Order by (flightId)\",\n",
    "    queryFlightsTogether,\n",
    "    3\n",
    ")\n",
    "spark.catalog.dropTempView(\"df\")\n",
    "\n",
    "df.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (\"passengerId\", \"flightId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId, flightId) took 6211 ms.\n",
      "\n",
      "Average time 6211 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId, flightId) took 6211 ms.\n",
      "Processing Order by (passengerId, flightId) took 5067 ms.\n",
      "\n",
      "Average time 5639 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId, flightId) took 6211 ms.\n",
      "Processing Order by (passengerId, flightId) took 5067 ms.\n",
      "Processing Order by (passengerId, flightId) took 5097 ms.\n",
      "\n",
      "Average time 5458 ms\n",
      "(12) MapPartitionsRDD[508] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[507] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[506] at rdd at <console>:68 []\n",
      " |   ShuffledRowRDD[505] at rdd at <console>:68 []\n",
      " +-(12) MapPartitionsRDD[504] at rdd at <console>:68 []\n",
      "    |   MapPartitionsRDD[500] at rdd at <console>:68 []\n",
      "    |   ShuffledRowRDD[499] at rdd at <console>:68 []\n",
      "    +-(12) MapPartitionsRDD[498] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[497] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[496] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[495] at rdd at <console>:68 []\n",
      "       |   *(2) Sort [passengerId#10 ASC NULLS FIRST, flightId#11 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(passengerId#10 ASC NULLS FIRST, flightId#11 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#10,flightId#11,date#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/masa/home/repository/git/oonisim/spark-programs/Flight/src/main/reso..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[390] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |       CachedPartitions: 12; MemorySize: 513.7 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |   MapPartitionsRDD[389] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |   ShuffledRowRDD[388] at run at ThreadPoolExecutor.java:1149 []\n",
      "       +-(1) MapPartitionsRDD[387] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  MapPartitionsRDD[383] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  FileScanRDD[382] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [passengerId: int, flightId: int ... 1 more field]\n",
       "timeOrderPassengerFlight = 5458\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"passengerId\", \"flightId\")\n",
    "    .persist\n",
    "\n",
    "df.createOrReplaceTempView(\"flightData\")\n",
    "val timeOrderPassengerFlight= run(\n",
    "    \"Order by (passengerId, flightId)\",\n",
    "    queryFlightsTogether,\n",
    "    3\n",
    ")\n",
    "spark.catalog.dropTempView(\"df\")\n",
    "\n",
    "df.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by (\"passengerId\", \"date\")Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId, date) took 6103 ms.\n",
      "\n",
      "Average time 6103 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId, date) took 6103 ms.\n",
      "Processing Order by (passengerId, date) took 5043 ms.\n",
      "\n",
      "Average time 5573 ms\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|            37|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            38|                         4|2017-01-01|2017-12-31|\n",
      "|             1|            76|                         4|2017-01-01|2017-12-31|\n",
      "|             1|           120|                         4|2017-01-01|2017-12-31|\n",
      "|             1|          1694|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing Order by (passengerId, date) took 6103 ms.\n",
      "Processing Order by (passengerId, date) took 5043 ms.\n",
      "Processing Order by (passengerId, date) took 5243 ms.\n",
      "\n",
      "Average time 5463 ms\n",
      "(12) MapPartitionsRDD[635] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[634] at rdd at <console>:68 []\n",
      " |   MapPartitionsRDD[633] at rdd at <console>:68 []\n",
      " |   ShuffledRowRDD[632] at rdd at <console>:68 []\n",
      " +-(12) MapPartitionsRDD[631] at rdd at <console>:68 []\n",
      "    |   MapPartitionsRDD[627] at rdd at <console>:68 []\n",
      "    |   ShuffledRowRDD[626] at rdd at <console>:68 []\n",
      "    +-(12) MapPartitionsRDD[625] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[624] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[623] at rdd at <console>:68 []\n",
      "       |   MapPartitionsRDD[622] at rdd at <console>:68 []\n",
      "       |   *(2) Sort [passengerId#10 ASC NULLS FIRST, date#14 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(passengerId#10 ASC NULLS FIRST, date#14 ASC NULLS FIRST, 12)\n",
      "   +- *(1) FileScan csv [passengerId#10,flightId#11,date#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/masa/home/repository/git/oonisim/spark-programs/Flight/src/main/reso..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<passengerId:int,flightId:int,date:timestamp>\n",
      " MapPartitionsRDD[517] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |       CachedPartitions: 12; MemorySize: 513.9 KB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n",
      "       |   MapPartitionsRDD[516] at run at ThreadPoolExecutor.java:1149 []\n",
      "       |   ShuffledRowRDD[515] at run at ThreadPoolExecutor.java:1149 []\n",
      "       +-(1) MapPartitionsRDD[514] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  MapPartitionsRDD[510] at run at ThreadPoolExecutor.java:1149 []\n",
      "          |  FileScanRDD[509] at run at ThreadPoolExecutor.java:1149 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [passengerId: int, flightId: int ... 1 more field]\n",
       "timeOrderPassengerDate = 5463\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[passengerId: int, flightId: int ... 1 more field]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = flightData\n",
    "    .orderBy(\"passengerId\", \"date\")\n",
    "    .persist\n",
    "\n",
    "df.createOrReplaceTempView(\"flightData\")\n",
    "val timeOrderPassengerDate = run(\n",
    "    \"Order by (passengerId, date)\",\n",
    "    queryFlightsTogether,\n",
    "    3\n",
    ")\n",
    "spark.catalog.dropTempView(\"df\")\n",
    "\n",
    "df.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elepased Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Order by:\n",
      "Non                     is 10312 ms\n",
      "(passengerId)           is 3582 ms\n",
      "(flightId)              is 6280 ms\n",
      "(passengerId, date)     is 5463 ms\n",
      "(passengerId, flightId) is 5458 ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "report = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "Order by:\n",
       "Non                     is 10312 ms\n",
       "(passengerId)           is 3582 ms\n",
       "(flightId)              is 6280 ms\n",
       "(passengerId, date)     is 5463 ms\n",
       "(passengerId, flightId) is 5458 ms\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val report = s\"\"\"\n",
    "Order by:\n",
    "Non                     is $timeOrderNone ms\n",
    "(passengerId)           is $timeOrderPassenger ms\n",
    "(flightId)              is $timeOrderFlight ms\n",
    "(passengerId, date)     is $timeOrderPassengerDate ms\n",
    "(passengerId, flightId) is $timeOrderPassengerFlight ms\n",
    "\"\"\"\n",
    "println(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|passengerId|count|\n",
      "+-----------+-----+\n",
      "|          1|    5|\n",
      "|         37|    4|\n",
      "|         38|    4|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+--------+\n",
      "|passengerId|flightId|\n",
      "+-----------+--------+\n",
      "|          1|       0|\n",
      "|          1|     901|\n",
      "|          1|     940|\n",
      "|          1|     972|\n",
      "|          1|     993|\n",
      "|         37|       0|\n",
      "|         37|     901|\n",
      "|         37|     940|\n",
      "|         37|     972|\n",
      "|         38|       0|\n",
      "|         38|     901|\n",
      "|         38|     940|\n",
      "|         38|     972|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query = \n",
       "query = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "SELECT passengerId, flightId\n",
       "FROM flightData\n",
       "WHERE passengerId in (1, 37, 38)\n",
       "ORDER BY passengerId\n",
       "\"\n",
       "SELECT passengerId, flightId\n",
       "FROM flightData\n",
       "WHERE passengerId in (1, 37, 38)\n",
       "ORDER BY passengerId\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var query = \n",
    "s\"\"\"\n",
    "SELECT passengerId, count(flightId) as count\n",
    "FROM flightData\n",
    "WHERE passengerId in (1, 37, 38)\n",
    "GROUP BY passengerId\n",
    "HAVING count(flightId) > $NUM_FLIGHT_TOGETHER\n",
    "ORDER BY passengerId\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    ".show()\n",
    "\n",
    "query = \n",
    "s\"\"\"\n",
    "SELECT passengerId, flightId\n",
    "FROM flightData\n",
    "WHERE passengerId in (1, 37, 38)\n",
    "ORDER BY passengerId\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|             1|          2658|                         4|2017-01-01|2017-12-31|\n",
      "|            18|          7544|                         5|2017-01-01|2017-12-31|\n",
      "|            18|         13862|                         4|2017-01-01|2017-12-31|\n",
      "|            53|          6769|                         5|2017-01-01|2017-12-31|\n",
      "|            56|            22|                         4|2017-01-01|2017-12-31|\n",
      "|            58|          1328|                         5|2017-01-01|2017-12-31|\n",
      "|            64|          3087|                         6|2017-01-01|2017-12-31|\n",
      "|            92|          9466|                         5|2017-01-01|2017-12-31|\n",
      "|           117|          7551|                         6|2017-01-01|2017-12-31|\n",
      "|           119|          3615|                         4|2017-01-01|2017-12-31|\n",
      "|           119|          9620|                         4|2017-01-01|2017-12-31|\n",
      "|           121|          2026|                         5|2017-01-01|2017-12-31|\n",
      "|           126|          2924|                         4|2017-01-01|2017-12-31|\n",
      "|           130|          3639|                         5|2017-01-01|2017-12-31|\n",
      "|           135|           869|                         4|2017-01-01|2017-12-31|\n",
      "|           139|          5958|                         5|2017-01-01|2017-12-31|\n",
      "|           139|         11834|                         5|2017-01-01|2017-12-31|\n",
      "|           141|         10310|                         4|2017-01-01|2017-12-31|\n",
      "|           147|          5042|                         4|2017-01-01|2017-12-31|\n",
      "|           157|          2100|                         6|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "seed = 42\n",
       "withReplacement = false\n",
       "fraction = 0.01\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 42\n",
    "val withReplacement = false\n",
    "val fraction = 0.01\n",
    "spark.sql(queryFlightsTogether)\n",
    "    .sample(withReplacement, fraction, seed)\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|Passenger 1 ID|Passenger 2 ID|Number of flights together|      From|        To|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "|            58|          1381|                         7|2017-01-01|2017-12-31|\n",
      "|            58|          2942|                         4|2017-01-01|2017-12-31|\n",
      "+--------------+--------------+--------------------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filter01 = (Passenger 2 ID = 2942)\n",
       "filter02 = (Passenger 2 ID = 1381)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<console>:44: warning: a pure expression does nothing in statement position; you may be omitting necessary parentheses\n",
       "\"\"\"\n",
       "^\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Passenger 2 ID = 1381)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "comm -12 \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^1381,/{print $2}' | sort ) \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^58,/{print $2}' | sort)\n",
    "\n",
    "131\n",
    "189\n",
    "217\n",
    "247\n",
    "272\n",
    "283\n",
    "331\n",
    "\n",
    "comm -12 \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^58,/{print $2}' | sort ) \\\n",
    "<(cat ../../../main/resources/flightData.csv | awk '{FS=\",\"} /^2942,/{print $2}' | sort)\n",
    "\n",
    "131\n",
    "189\n",
    "217\n",
    "247\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "val filter01 = col(\"Passenger 2 ID\") === 2942\n",
    "val filter02 = col(\"Passenger 2 ID\") === 1381\n",
    "spark.sql(queryFlightsTogether)\n",
    "    .where(col(\"Passenger 1 ID\") === 58)\n",
    "    .where(filter01.or(filter02))\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
